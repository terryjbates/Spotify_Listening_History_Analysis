---
title: SQL-Driven Analysis of Spotify Streaming Activity
author: Terry Bates
date: '2025-07-23'
format:
  html:
    toc: true
    code-fold: true
    code-summary: Show the code
    number-sections: true
    theme: cosmos
    fig-cap-location: bottom
execute:
  echo: true
jupyter: python3
---


# üéß SQL-Driven Analysis of Spotify Streaming Activity

## üìùExecutive Summary

This project uses a Python and PostgreSQL-based pipeline to analyze 10+ years of Spotify listening history, combining local JSON data with external metadata to explore trends in genre discovery, artist loyalty, and podcast habits.

We integrate data from the [Spotify Web API](https://developer.spotify.com/documentation/web-api/reference/get-an-artist) to include musical genre information to analyze shifts in musical preference over time. Using PostgreSQL, SQLAlchemy, Plotly, and Seaborn, the project surfaces insights about listening habits mirroring and adapting the "Spotify Wrapped" question list.

This analysis quantifies patterns such as the most-played artists and tracks per year, the recurrence of top genres, daily and seasonal listening habits, and the timeline of genre discovery. We also experiment with "genre-mapping," seeking to leverage data from [Every Noise at Once](https://everynoise.com/ ) to plot genres on a coordinate system to examine visually how listening preferences shifted through this "genre-space" over time.

## üìå Project Objectives

- Analyze and derive insights from Spotify listening, starting February **2014** and ending January **2025**.
- Replicate "Spotify Wrapped" functionality as fully as possible using minimal tooling.
- Attempt to profile my music listening persona using ENAO genre-space.

## üìú Insights Summary

Across our entire Spotify listening data, several meaningful patterns emerge:

* Most years of the entire Spotify listening dataset are characterized by consistent, high-volume engagement with a "core" set of genres. These genres include `ambient`, `minimalism`, `drone` and `space music`.  The listening behavior surrounding tracks within these genres is associated with  evening and morning hours.  

* **2020** stood out as a dramatic exception, with the most significant drop in overall listening hours combined with an unusually diverse array of genres and formats. Podcast listening, in spite of an overarching aversion to the format at large, rose to its highest level in this year, reflecting the social disruption and psychological impacts of pandemic-era lockdowns and work-from-home initiatives.

* After **2020**, listening hours rose back to prior levels, in conjunction with several newly observed genres, significantly adding to diversity and overall hours played. `Lo-fi`, `phonk`, `vaporwave`, and `synthwave`, bit-part players at the beginning of the listening history, took up more listening volume alongside the "core" genres. This signals both a reversion to familiar tastes, coupled with emergent interest and openness to new genres and artists.

* Genre choices and hours played fluctuate not just by year, but across time of day and month.

    * With the exception of **2020**, Morning consistently sees the most engagement, with afternoons and evenings battling for second spot.
    * Summer and Spring feature greater listening intensity than other seasons generally, though Fall has grown more dominant in latter years. Winter months show consistent dips in activity, with an outlier year associated with a "crunch-time" project.

## üì¶ Library Imports

```{python}
# Standard Library
import os
import sys
import json
import time
import base64
import socket
import logging
import datetime
import re
import traceback
import calendar
from collections import defaultdict
from pathlib import Path, PurePath
from time import sleep
from datetime import datetime, timezone
from typing import List, Optional, Dict, Any

# Configure logging
logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s [%(levelname)s] %(message)s")

# Suppress warnings
logging.getLogger('matplotlib').setLevel(logging.WARNING)

# Data libraries
import pandas as pd
import numpy as np

# SQL and Database
import duckdb
import psycopg2
from sqlalchemy import create_engine, text, insert, inspect, update, select, and_, or_, MetaData, Table
from sqlalchemy.dialects.postgresql import insert as pgsql_insert
from sqlalchemy.orm import sessionmaker
from sqlalchemy.engine import Engine

# API Access
import requests
from dotenv import load_dotenv

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.patches as mpatches

# Notebook Utilities
from IPython.display import display, Image, JSON
from tqdm import tqdm
```

## üîê Environmental and Credential Setup

We set up our environment variables for the Spotify Web API as well as database credentials. We use the [dotenv](https://pypi.org/project/python-dotenv/) Python library to avoid stashing credentials within notebooks in clear-text. It can be used for other environmental variables as well. 

```{python}
# Load environment variables
load_dotenv()

CLIENT_ID = os.getenv("SPOTIFY_CLIENT_ID")
CLIENT_SECRET = os.getenv("SPOTIFY_CLIENT_SECRET")

# Define global Spotify API base URL
SPOTIFY_API_BASE_URL = "https://api.spotify.com/v1/"

# Configure Local Database config and credentials
db_user = os.getenv("DB_USER")
db_host = os.getenv("DB_HOST")
db_name = os.getenv("DB_NAME")
db_password = os.getenv("DB_PASSWORD")
db_port = os.getenv("DB_PORT")
```

## üåç Global Constants and File Path Configuration

```{python}
# By default, the current working directory will be the same directory that the notebook resides
# Ex: <gh_repo>/notebooks
# We need to change this so that the current working directory will be the base directory of the 
# Github repo. We can traverse to the top level directory relatively using the ".." shorthand

# Set env variable to 'None' value. O
# DATA_PATH = None

# Manually set dir if things go awry
os.chdir('C:\\Users\\lover\\Documents\\GitHub\\Spotify_Listening_History_Analysis')

# Use explicit DATA_PATH, otherwise use relative dir to infer location
DATA_PATH = PurePath('C:\\Users\\lover\\Documents\\GitHub\\Spotify_Listening_History_Analysis\\data')

# Get current path
cwd = PurePath(Path.cwd())

# Use conditional to ensure we do not traverse to higher level dir.
if not DATA_PATH:
    # Change current working directory from `notebooks` directory to one level above.
    os.chdir('..')    
    
    # Construct path to archive location
    DATA_PATH = cwd / 'data'
    
# Construct query output path
QUERY_OUTPUT_PATH = cwd / 'data'/ 'sql' / 'query_results'

# Confirm location where data files are stored.
logging.info(str(DATA_PATH))
```

Sometimes you want to see the _full_ contents of dataframes, but the notebook will only display truncated versions. Uncomment and execute the cell below to enable full row and column display.

```{python}
# Manually set dir if things go awry
os.chdir('C:\\Users\\lover\\Documents\\GitHub\\Spotify_Listening_History_Analysis')
```

```{python}
# Modify display options to show full column or row contents
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
```

### Constants Mapping

Placeholder section to be used for static items we may need. (Seasons, genres, lists).

## üõ†Ô∏è Function Definitions

### File I/O and JSON Processing

```{python}
def get_json_files(directory: str) -> List[PurePath]:
    """
    Return list of PurePath objects for all JSON files in a given directory.
    """
    return [
        PurePath(directory, f)
        for f in os.listdir(directory)
        if PurePath(directory, f).is_file() and f.endswith(".json")
    ]


def list_files_pathlib(path_str: str) -> List[str]:
    """
    Return a list of filenames from a valid directory path using pathlib.
    """
    path = Path(PurePath(path_str))
    if not path.is_dir():
        return []
    return [entry.name for entry in path.iterdir() if entry.is_file()]


def list_files_os(path_str: str) -> List[str]:
    """
    Return list of filenames from a valid directory path using os and PurePath.
    """
    path = PurePath(path_str)
    if not os.path.isdir(path):
        return []
    return [
        f for f in os.listdir(path)
        if os.path.isfile(os.path.join(path, f))
    ]


def load_json_from_file(json_file: str, engine: Engine, db_table: str) -> None:
    """
    Load JSON file and insert records into a PostgreSQL table using SQLAlchemy.
    """
    with open(json_file, 'r', encoding='utf-8', errors='ignore') as input_file:
        json_data = json.load(input_file)

    with engine.connect() as conn:
        for record in json_data:
            if record.get("offline_timestamp"):
                ts = int(record["offline_timestamp"] / 1000)
                record["offline_timestamp"] = datetime.datetime.fromtimestamp(ts, datetime.UTC)

            insert_data = {
                "timestamp_column": record.get("ts"),
                "platform": record.get("platform"),
                "ms_played": record.get("ms_played"),
                "conn_country": record.get("conn_country"),
                "track_name": record.get("master_metadata_track_name"),
                "artist_name": record.get("master_metadata_album_artist_name"),
                "album_name": record.get("master_metadata_album_album_name"),
                "spotify_track_uri": record.get("spotify_track_uri"),
                "episode_name": record.get("episode_name"),
                "episode_show_name": record.get("episode_show_name"),
                "spotify_episode_uri": record.get("spotify_episode_uri"),
                "audiobook_title": record.get("audiobook_title"),
                "audiobook_uri": record.get("audiobook_uri"),
                "audiobook_chapter_uri": record.get("audiobook_chapter_uri"),
                "audiobook_chapter_title": record.get("audiobook_chapter_title"),
                "reason_start": record.get("reason_start"),
                "reason_end": record.get("reason_end"),
                "shuffle": record.get("shuffle"),
                "skipped": record.get("skipped"),
                "offline": record.get("offline"),
                "offline_timestamp": record.get("offline_timestamp"),
                "incognito_mode": record.get("incognito_mode"),
                "ip_addr": record.get("ip_addr"),
            }

            conn.execute(insert(db_table), [insert_data])
            conn.commit()


def get_audio_json_files(directory: Path) -> List[str]:
    """
    Retrieves all files in the directory that match the 'Audio*.json' pattern.
    """
    all_files = list_files_os(directory)
    audio_files = [f for f in all_files if re.match(r'.*Audio.*\.json$', f)]
    return sorted(audio_files)


def preview_sample_json(file_path: Path, sample_idx: int = 0):
    """
    Preview specific record from a JSON file.
    """
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        data = json.load(f)
    display(JSON(data[sample_idx]))
```

### Spotify Web API Functions

```{python}
# Web API: Token Retrieval Function
def get_token(client_id: str=None, client_secret: str=None) -> str:
    """
    Retrieves Spotify API access token using the Client Credentials flow.
    
    Args:
        CLIENT_ID (str): Spotify API Client ID.
        CLIENT_SECRET (str): Spotify API Client Secret.
    
    Returns:
        str: Bearer token for authorization.
    
    Raises:
        Exception: If token request fails or response is invalid.
    # """
    if not client_id:
        client_id = CLIENT_ID
    if not client_secret:
        client_secret = CLIENT_SECRET

    auth_url = "https://accounts.spotify.com/api/token"
    auth_header = base64.b64encode(f"{CLIENT_ID}:{CLIENT_SECRET}".encode()).decode()

    headers = {
        "Authorization": f"Basic {auth_header}",
        "Content-Type": "application/x-www-form-urlencoded"
    }

    data = {
        "grant_type": "client_credentials"
    }

    response = requests.post(auth_url, headers=headers, data=data)
    
    if response.status_code == 200:
        return response.json().get("access_token")
    else:
        raise Exception(f"Failed to retrieve token: {response.status_code} | {response.text}")


# Web API: Auth Header Builder
def get_auth_header(token: str) -> dict:
    """
    Construct the auth-z header for Spotify API requests.

    Args:
        token (str): Bearer token from `get_token()`.

    Returns:
        dict: Dictionary containing Authorization header.
    """
    return {"Authorization": f"Bearer {token}"}


def search_for_artist(token: str, artist_name: str) -> Optional[Dict[str, Any]]:
    """
    Search the Spotify API for an artist by name and return metadata if found.
    """
    url = f"{SPOTIFY_API_BASE_URL}search?q={artist_name}&type=artist&limit=1"
    headers = get_auth_header(token)
    response = requests.get(url, headers=headers)
    try:
        return response.json()["artists"]["items"][0]
    except (KeyError, IndexError):
        logging.info(f"No artist found for '{artist_name}'")
        return None


def search_for_artist_by_id(token: str, artist_id: str) -> Optional[Dict[str, Any]]:
    """
    Retrieve Spotify artist metadata using the artist's Spotify ID.
    """
    url = f"{SPOTIFY_API_BASE_URL}artists/{artist_id}"
    headers = get_auth_header(token)
    response = requests.get(url, headers=headers)

    if response.status_code != 200:
        msg = response.json().get('error', {}).get('message', 'No message')
        logging.info(f"Error {response.status_code}: {msg}")
        return None

    try:
        return response.json()
    except json.JSONDecodeError:
        logging.info("Error decoding JSON response")
        return None


def get_songs_by_artist(token: str, artist_id: str) -> List[Dict[str, Any]]:
    """
    Retrieve top tracks for an artist using their Spotify ID.
    """
    url = f"{SPOTIFY_API_BASE_URL}artists/{artist_id}/top-tracks?country=US"
    headers = get_auth_header(token)
    return requests.get(url, headers=headers).json()["tracks"]
```

### Database Insert/Update

```{python}
def insert_artist_genres(
    starting_id: int,
    ending_id: int,
    engine: Engine,
    artists_table,
    CLIENT_ID: str,
    CLIENT_SECRET: str,
    artists_lacking_genres: Optional[Dict[int, str]] = None
) -> int:
    """
    Use the Spotify API to populate missing genre fields in an artist table.
    """
    artists_lacking_genres = artists_lacking_genres or {}
    token = get_token(CLIENT_ID, CLIENT_SECRET)
    records_processed = 0
    last_seen_id = starting_id

    stmt = select(artists_table.c.id, artists_table.c.artist_name).where(
        and_(
            or_(
                artists_table.c.genres == '{}',
                artists_table.c.genres.is_(None)
            ),
            artists_table.c.id >= starting_id,
            artists_table.c.id <= ending_id
        )
    ).order_by(artists_table.c.id)

    with engine.connect() as conn:
        for row in conn.execute(stmt):
            last_seen_id = row.id

            if records_processed and records_processed % 450 == 0:
                token = get_token(CLIENT_ID, CLIENT_SECRET)
                records_processed = 0
                time.sleep(900)

            logging.info(f"Artist: {row.artist_name}\nID: {row.id}")

            try:
                api_result = search_for_artist(token, row.artist_name)
                genre_list = api_result.get("genres", []) if api_result else []

                conn.execute(
                    update(artists_table)
                    .where(artists_table.c.id == row.id)
                    .values(genres=genre_list)
                )
                conn.commit()

            except (KeyError, TypeError, IndexError, json.JSONDecodeError) as e:
                logging.info(f"Error: {e} for artist {row.artist_name}")
                artists_lacking_genres[row.id] = row.artist_name
                return last_seen_id

            except requests.exceptions.RequestException as e:
                logging.info(f"Request failed: {e}")
                return last_seen_id

            time.sleep(7)
            records_processed += 1

    return last_seen_id


def retry_with_backoff(
    func,
    start_id: int,
    ending_id: int,
    engine: Engine,
    artists_table,
    CLIENT_ID: str,
    CLIENT_SECRET: str,
    max_retries: int = 2,
    base_wait: int = 60
) -> None:
    """
    Retry function with exponential backoff in case of failure.
    """
    attempt = 0
    current_id = start_id

    while attempt <= max_retries:
        logging.info(f"\n=== Attempt {attempt + 1} starting at ID {current_id} ===")
        start_time = time.time()

        try:
            last_seen_id = func(
                current_id,
                ending_id,
                engine,
                artists_table,
                CLIENT_ID,
                CLIENT_SECRET
            )
            elapsed = (time.time() - start_time) / 3600
            logging.info(f"Function completed in {elapsed:.2f} hours")
            break

        except Exception as e:
            traceback.print_exc()
            wait_time = base_wait * (2 ** attempt)
            logging.info(f"Exception: {e}\nRetrying in {wait_time} seconds...")
            time.sleep(wait_time)
            current_id = max(0, locals().get("last_seen_id", current_id) - 1)
            attempt += 1

    else:
        logging.info("Max retries exceeded.")


def ingest_audio_jsons_to_db(
    file_list: List[str],
    data_dir: Path,
    engine,
    table_name: str,
    loader_func,  # Injected function that does the loading
    dry_run: bool = False,
    max_files: int = None
) -> Dict[str, int]:
    """
    Generalized loader for JSON audio files into a specified DB table.

    Parameters:
    - file_list: List of filenames to process.
    - data_dir: Directory containing files.
    - engine: SQLAlchemy engine or connection.
    - table_name: Name of the database table to insert into.
    - loader_func: Function that takes (file_path, engine, table_name).
    - dry_run: Preview only, don't insert.
    - max_files: Limit number of files to process.

    Returns:
    - Dictionary mapping filename -> record count.
    """
    results = {}
    files_to_process = file_list if max_files is None else file_list[:max_files]

    if dry_run:
        logging.info("üîç Dry run mode: previewing file paths only.\n")
        for file_name in files_to_process:
            logging.info(f"[DRY RUN] Would process: {data_dir / file_name}")
        logging.info(f"\nüõë Dry run complete. {len(files_to_process)} files previewed.")
        return {}

    start_time = time.time()
    for file_name in tqdm(files_to_process, desc="Inserting audio logs"):
        file_path = data_dir / file_name
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                file_data = json.load(f)
            record_count = len(file_data)
            loader_func(file_path, engine, table_name)
            results[file_name] = record_count
        except Exception as e:
            logging.error(f"‚ùå Failed: {file_name} | {str(e)}")
            results[file_name] = 0
    end_time = time.time()

    logging.info(f"\n‚úÖ Loaded {len(results)} files in {(end_time - start_time)/60:.2f} minutes.")
    logging.info(f"üìä Total records inserted: {sum(results.values()):,}")

    return results
```

### Analysis Helpers

```{python}
def last_day_of_year(timestamp: str | pd.Timestamp) -> float:
    """
    Return number of days remaining between the timestamp and end of its year.
    """
    ts = pd.to_datetime(timestamp)
    end = datetime(ts.year, 12, 31, 23, 59, 59, tzinfo=timezone.utc)
    ts_utc = ts.tz_localize('UTC') if ts.tzinfo is None else ts.astimezone(timezone.utc)
    return (end - ts_utc).total_seconds() / 86400

def get_surrounding_genres(
    df: pd.DataFrame,
    center_x: float,
    center_y: float,
    radius_pct: float = 5,
    year: Optional[int] = None
) -> pd.DataFrame:
    """
    Return genres within the radius of a center point in 2D genre space.
    """
    required = {"x", "y", "genre", "year"}
    if not required.issubset(df.columns):
        raise ValueError(f"Missing columns: {required - set(df.columns)}")

    if year is not None:
        df = df[df["year"] == year].copy()
        if df.empty:
            raise ValueError(f"No data found for year {year}")

    x_range = df["x"].max() - df["x"].min()
    radius = (radius_pct / 100) * x_range
    df["distance"] = np.sqrt((df["x"] - center_x) ** 2 + (df["y"] - center_y) ** 2)

    nearby = df[df["distance"] <= radius].copy().sort_values("distance")
    return nearby[["genre", "x", "y", "distance", "ms_played", "year"]].reset_index(drop=True)


def fetch_query_results(query: str, engine = None) -> pd.DataFrame:
    """
    Execute SQL query using a given SQLAlchemy engine and return a pandas DataFrame.
    Automatically downcasts any column containing 'year' in its name to integer.

    Parameters:
    - query (str): SQL query to execute.
    - engine (sqlalchemy.engine.Engine): SQLAlchemy engine object for DB connection.

    Returns:
    - pd.DataFrame: Query results with '*year*'-named columns downcasted to integer.
    """
    # Use the global engine if none is provided
    if engine is None:
        engine = globals().get("engine")
        if engine is None:
            raise ValueError("No SQLAlchemy engine provided or found in global scope.")

    with engine.connect() as conn:
        df = pd.read_sql(text(query), conn)

    year_cols = [col for col in df.columns if 'year' in col.lower()]
    for col in year_cols:
        df[col] = pd.to_numeric(df[col], downcast='integer', errors='coerce')

    return df


def execute_query(query: str, engine: Optional[Engine] = None) -> None:
    """
    Executes a SQL query (e.g. INSERT, UPDATE, DELETE) using a SQLAlchemy engine.

    This function is intended for queries that do not return results.

    Parameters:
    - query (str): The SQL query to execute.
    - engine (Optional[Engine]): SQLAlchemy engine to use for the database connection.
      If not provided, the function will look for a global `engine`.

    Raises:
    - ValueError: If no SQLAlchemy engine is provided or found in the global scope.
    - sqlalchemy.exc.SQLAlchemyError: If execution fails due to SQL errors.
    """
    # Use the global engine if none is provided
    if engine is None:
        engine = globals().get("engine")
        if engine is None:
            raise ValueError("No SQLAlchemy engine provided or found in global scope.")

    with engine.connect() as conn:
        conn.execute(text(query))
        conn.commit()
```

## üóÑÔ∏è Database Setup

### Engine and Connection String

Establish connection with database.

```{python}
# Override the db_name if you happen to be using test instance
# by editing/uncommenting one of the lines below.
db_name = 'spotify_streaming'
# db_name = 'testing_db'

# Create engine object
engine = create_engine(f"postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}") 
```

Confirm the database connection. A connection string echoing database version will be returned.

```{python}
#| scrolled: true
# Confirm database connection
with engine.connect() as conn:
    result = conn.execute(text("SELECT version() "))
    logging.info(result.all())
```

```{python}
# Confirm selected database
with engine.connect() as conn:
    result = conn.execute(text("SELECT current_database() "))
    logging.info(result.all())
```

If you need to dispose of the DB connection, run this command.

```{python}
# If you need to dispose of engine
engine.dispose()
```

After confirming our database instance is accessible via our connection string, it is possible to copy/paste the SQL detailed below into an appropriate database administration tool, such as pgadmin4, or execute the contents of `<local_github_repo_dir>/setup/postgresql_setup.sql`. Prior to executing code from strangers on the internet, it is advisable to read and understand each section to be aware of what will be executed and why it is necessary to conduct this analysis.

### Create Main Tables

Create the core table storing imported Spotify JSON data.

```{python}
#| lang: sql
-- Table: public.spotify_data

CREATE TABLE IF NOT EXISTS public.spotify_data (
    id INTEGER NOT NULL GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
    timestamp_column TIMESTAMPTZ,
    platform VARCHAR(100),
    ms_played INTEGER,
    conn_country VARCHAR(2),
    ip_addr VARCHAR(50),
    track_name VARCHAR(300),
    artist_name VARCHAR(300),
    album_name VARCHAR(300),
    spotify_track_uri VARCHAR(50),
    episode_name VARCHAR(150),
    episode_show_name VARCHAR(100),
    spotify_episode_uri VARCHAR(50),
    audiobook_title VARCHAR(100),
    audiobook_uri VARCHAR(50),
    audiobook_chapter_uri VARCHAR(50),
    audiobook_chapter_title VARCHAR(100),
    reason_start VARCHAR(30),
    reason_end VARCHAR(30),
    shuffle BOOLEAN DEFAULT FALSE,
    skipped BOOLEAN DEFAULT FALSE,
    offline BOOLEAN DEFAULT FALSE,
    offline_timestamp TIMESTAMPTZ,
    incognito_mode BOOLEAN DEFAULT FALSE,
    time_of_day TEXT,
    season TEXT,
    year_played INTEGER,
    CONSTRAINT fk_ip_reference FOREIGN KEY (ip_addr)
        REFERENCES public.ip_metadata (ip_addr)
);
```

Create the `artists` tables, as this is needed to capture artists metadata.

```{python}
#| lang: sql
-- Table: public.artists

CREATE TABLE IF NOT EXISTS public.artists (
    id INTEGER NOT NULL GENERATED ALWAYS AS IDENTITY PRIMARY KEY,
    artist_name VARCHAR(300),
    genres TEXT[],
    last_updated TIMESTAMPTZ DEFAULT now()
);
```

Create a JOIN table linking `artists` to `spotify_data` .

```{python}
#| lang: sql
CREATE TABLE IF NOT EXISTS public.sd_artists_join (
    artist_name TEXT COLLATE pg_catalog."default",
    sd_id INTEGER NOT NULL,
    artist_id INTEGER NOT NULL,
    CONSTRAINT sd_artist_key PRIMARY KEY (sd_id, artist_id),
    CONSTRAINT sd_artists_join_artist_id_fkey FOREIGN KEY (artist_id)
        REFERENCES public.artists (id) MATCH SIMPLE
        ON UPDATE NO ACTION
        ON DELETE NO ACTION,
    CONSTRAINT sd_artists_join_sd_id_fkey FOREIGN KEY (sd_id)
        REFERENCES public.spotify_data (id) MATCH SIMPLE
        ON UPDATE NO ACTION
        ON DELETE CASCADE
);
```

### Index Setup

```{python}
#| lang: sql
-- Indexes for spotify_data
CREATE INDEX IF NOT EXISTS album_name_idx
    ON public.spotify_data USING btree
    (album_name COLLATE pg_catalog."default" ASC NULLS LAST);

CREATE INDEX IF NOT EXISTS artist_name_idx
    ON public.spotify_data USING btree
    (artist_name COLLATE pg_catalog."default" ASC NULLS LAST);

CREATE INDEX IF NOT EXISTS spotify_data_ts_idx
    ON public.spotify_data USING btree
    (timestamp_column ASC NULLS LAST);

CREATE INDEX IF NOT EXISTS spotify_data_year_idx
    ON public.spotify_data USING btree
    (year_played ASC NULLS LAST);

CREATE INDEX IF NOT EXISTS track_name_idx
    ON public.spotify_data USING btree
    (track_name COLLATE pg_catalog."default" ASC NULLS LAST);


-- Indexes for artists
CREATE INDEX IF NOT EXISTS artist_idx
    ON public.artists USING btree
    (id ASC NULLS LAST);


-- Indexes for sd_artists_join
CREATE INDEX IF NOT EXISTS sd_artists_join_artist_idx
    ON public.sd_artists_join USING btree
    (artist_id ASC NULLS LAST);

CREATE INDEX IF NOT EXISTS sd_artists_join_name_idx
    ON public.sd_artists_join USING btree
    (artist_name COLLATE pg_catalog."default" ASC NULLS LAST);

CREATE INDEX IF NOT EXISTS sd_artists_join_sd_idx
    ON public.sd_artists_join USING btree
    (sd_id ASC NULLS LAST);
```

### Define Triggers

```{python}
#| lang: sql
-- Trigger for spotify_data: Automatically populate year_played from timestamp_column
CREATE OR REPLACE FUNCTION update_year_played()
RETURNS TRIGGER AS $$
BEGIN
  NEW.year_played := EXTRACT(YEAR FROM NEW.timestamp_column)::INTEGER;
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE OR REPLACE TRIGGER set_year_played
    BEFORE INSERT OR UPDATE 
    ON public.spotify_data
    FOR EACH ROW
    EXECUTE FUNCTION public.update_year_played();


-- Trigger for artists: Automatically update last_updated timestamp
CREATE OR REPLACE FUNCTION update_last_updated_column()
RETURNS TRIGGER AS $$
BEGIN
  NEW.last_updated := now();
  RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE OR REPLACE TRIGGER set_last_updated_timestamp
    BEFORE UPDATE 
    ON public.artists
    FOR EACH ROW
    EXECUTE FUNCTION public.update_last_updated_column();
```

### Apply Permissions & Ownership

The `spotify_postgres_user` (or the preferred local user) must be created prior to executing queries that alter table ownership enabling access for them.

```{python}
#| lang: sql
-- Create role for local DB user. Will fail if user already exists.
CREATE ROLE spotify_postgres_user WITH
  LOGIN
  PASSWORD '<db_password>';


-- Set ownership of tables to `postgres` user. 
ALTER TABLE IF EXISTS public.spotify_data
    OWNER TO postgres;

ALTER TABLE IF EXISTS public.artists
    OWNER TO postgres;

ALTER TABLE IF EXISTS public.sd_artists_join
    OWNER TO postgres;


-- Grant full access to postgres on all tables
GRANT ALL ON TABLE public.spotify_data TO postgres;
GRANT ALL ON TABLE public.artists TO postgres;
GRANT ALL ON TABLE public.sd_artists_join TO postgres;


-- Grant appropriate user-level privileges to spotify_postgres_user.
-- The rights granted can be altered as needed.
GRANT ALL ON TABLE public.spotify_data TO spotify_postgres_user;
GRANT ALL ON TABLE public.artists TO spotify_postgres_user;
GRANT ALL ON TABLE public.sd_artists_join TO spotify_postgres_user;
```

### Table Validation

#### Validate Tables via SQL

Create a list of database tables that should exist after execution of set up. This may also be useful to sanity check after machine migration or disaster recovery to ensure all tables expected are present.

```{python}
# Create an inspector to examine the current DB
inspector = inspect(engine)

expected_tables = {"spotify_data", "artists", "sd_artists_join"}
existing_tables = set(inspector.get_table_names())

missing_tables = expected_tables - existing_tables
assert not missing_tables, f"Missing expected tables: {missing_tables}"

logging.info("‚úÖ All expected tables are present.")
```

#### Validate Expected Columns

Validate the presence of expected columns in the database table. This snippet is illustrative, listing a subset of tables and associated columns. If schema alterations are made, table columns removed or added, revisiting this validation and populating the code with any new or changed expected columns is suggested.

```{python}
expected_columns = {
    "spotify_data": {"id", "timestamp_column", "ip_addr", "track_name", "artist_name", "year_played"},
    "artists": {"id", "artist_name", "genres", "last_updated"},
    "sd_artists_join": {"artist_name", "sd_id", "artist_id"},
}

for table, expected_cols in expected_columns.items():
    actual_cols = {col["name"] for col in inspector.get_columns(table)}
    missing = expected_cols - actual_cols
    extra = actual_cols - expected_cols
    assert not missing, f"‚ùå {table} is missing columns: {missing}"
    if extra:
        logging.info(f"‚ö†Ô∏è  {table} has extra columns: {extra}")
    else:
        logging.info(f"‚úÖ {table} has expected columns.")
```

#### Confirm Foreign Key Constraints

```{python}
expected_fks = {
    "sd_artists_join": [
        ("sd_id", "spotify_data", "id"),
        ("artist_id", "artists", "id")
    ],
}

for table, fks in expected_fks.items():
    actual_fks = inspector.get_foreign_keys(table)
    actual_set = {(fk["constrained_columns"][0], fk["referred_table"], fk["referred_columns"][0])
                  for fk in actual_fks}
    expected_set = set(fks)
    missing = expected_set - actual_set
    assert not missing, f"‚ùå {table} missing FK constraints: {missing}"
    logging.info(f"‚úÖ {table} foreign keys are valid.")
```

#### Confirm Table Presence

```{python}
def test_table_exists(engine, table_name):
    inspector = inspect(engine)
    try:
        assert table_name in inspector.get_table_names()
        logging.info(f"Table {table_name} exists.")
    except Exception as e:
        traceback.print_exc()
        logging.info(f"Table {table_name} does *not* exist.")
        logging.info(f"Exception: {e}")

postgres_table_names = ['spotify_data', 'artists', 'sd_artists_join']

for table_name in postgres_table_names:
    test_table_exists(engine, table_name)
```

## üìº Loading Spotify Data

In this section, we will discuss two methods to load Spotify listening history data. The first method uses data bundled within the repository, while the second method assumes the reader will load Spotify listening history from their own extracted JSON files.

### Import Bundled CSV 

Execute the following code to generate the query needed to load the Spotify listening history located within the `<local_github_repo_dir>/data/extract` directory.  

**Query**

```{python}
# Set path for our bundled dataset
bundled_spotify_data_csv = DATA_PATH / 'extract'/ 'spotify_data.csv'

# Create query to import CSV with Spotify listening data.
data_import_query = f"""
COPY spotify_data (
        id 
        ,timestamp_column
        ,ms_played
        ,conn_country
        ,track_name
        ,artist_name
        ,album_name
        ,spotify_track_uri
        ,episode_name
        ,episode_show_name
        ,spotify_episode_uri
        ,audiobook_title
        ,audiobook_uri
        ,audiobook_chapter_uri
        ,audiobook_chapter_title
        ,reason_start
        ,reason_end
        ,shuffle
        ,skipped
        ,offline
        ,offline_timestamp
        ,incognito_mode
        ,time_of_day
        ,season
        ,year_played
)
FROM '{bundled_spotify_data_csv}'
WITH(FORMAT CSV, HEADER);
"""
```

As the postgres user leveraged in this notebook may lack permissions to use the `COPY` function, it will be necessary to paste output from command below into either `pgadmin4` or `psql` command prompt to execute data import.

```{python}
print(data_import_query)
```

After executing this query, skip forward to the `Confirm Record Ingestion` section and validate records are present in the `spotify_data` table.

### üì• Loading Raw Data from JSON

This notebook section assumes the reader has extracted JSON files comprising their Spotify listening history into the  `<local_github_repo_dir>/data/raw` directory. The full list of JSON files should be accessible here on a per-file basis,  _not_ nested beneath any subdirectories in the `raw` directory. 

#### Preview Audio Files

The `get_audio_json_files` function will return a list of all JSON files in our specified directory. In this project, we pass a path object of `DATA_PATH / 'raw'`.

```{python}
# Get matching audio JSONs from our data path. This will return a list of all
# JSON files in our DATA_PATH.
audio_files = get_audio_json_files(DATA_PATH / 'raw')
```

Echo the `audio_files` variable and confirm the presence of all associated `JSON` files.

```{python}
audio_files
```

Preview one of the JSON files in the `DATA_PATH / 'raw'` directory.

```{python}
# Take a peek at one of the records. This should return a JSON record for a single track.
sample_file_path = DATA_PATH / 'raw' / audio_files[0]
preview_sample_json(sample_file_path)
```

#### Create Metadata and Table Objects

Create a metadata and Table object representing the `spotify_data` table. The Table object will be one of the parameters passed to our ingestion function to load data from our JSON files into the database.

```{python}
# Create a variable capturing the Postgres table that will store our loaded Spotify data.
metadata_obj = MetaData()
# Create a table object to use for inserting our JSON records.
spotify_table = Table('spotify_data', metadata_obj, autoload_with=engine)

# Visually confirm table object
spotify_table
```

#### "Dry Run" DB Ingestion Function

Execute the "dry run" feature of the ingestion code to sanity check the ingestion function prior to records ingestion.

```{python}
# Preview 3 files only, no DB writes
audio_ingest_path = DATA_PATH / 'raw'
ingest_audio_jsons_to_db(audio_files,
                         audio_ingest_path,
                         engine,
                         spotify_table,
                         load_json_from_file,
                         dry_run=True,
                         max_files=3)
```

#### Ingest All Raw JSON

Read in the JSON files with Spotify listening data into the database. 

```{python}
# Set ingestion directory
audio_ingest_path = DATA_PATH / 'raw'
# Full ingestion of all JSON files
results = ingest_audio_jsons_to_db(
    file_list=audio_files,
    data_dir=audio_ingest_path,
    engine=engine,
    table_name=spotify_table,
    loader_func=load_json_from_file
)
```

### Confirm Record Ingestion

After executing the JSON ingestion code, or loading in the bundled dataset, we should have a non-zero amount of records present in the `spotify_data` table. We execute a shallow `COUNT()` function and check the result.

```{python}
# Confirm we have records in the `spotify_data` table. A simple COUNT() does the trick.
query = """
    SELECT COUNT(*)
    FROM spotify_data
    """
query_results = fetch_query_results(query, engine)

# Validate non-zero number of results
query_results
```

A simple query for well-known artist `Stevie Wonder`.

**Query**

```{python}
# A test search for Stevie Wonder.
query = """
    SELECT 
        artist_name
        ,album_name
        ,track_name
        ,timestamp_column
    FROM spotify_data
    WHERE artist_name = 'Stevie Wonder'
    """
```

**Results**

```{python}
query_results = fetch_query_results(query)
query_results
```

### Populating the `artists` Table

In order to cleanly surface genres data for our tracks, we need to look up artist information. We have an `artists` table laid out from our setup script, but it lacks data. 

Use the following query to seed `artists` data from the `spotify_data` table. (This query can also be found inside of the `sql/queries.sql` file.)

**Query**

```{python}
query = """
-- Insert artists data into `artists` table
WITH distinct_artists (artist_name) AS (
	SELECT DISTINCT artist_name 
	FROM spotify_data 
)
INSERT INTO artists (artist_name)
SELECT da.artist_name
FROM distinct_artists AS da
LEFT JOIN artists AS a 
    ON da.artist_name = a.artist_name
WHERE a.artist_name IS NULL;
"""
```

```{python}
#| echo: false
# Execute query to insert data into the `artists` table.
# No results are returned.
execute_query(query)
```

After insertion of data into `artists`, we can query the DB to see if the `artists` table has records. 

**Query**

```{python}
query ="""
-- Execute query to shallow confirm data insertion into artists table
SELECT * 
FROM artists 
LIMIT 5;
"""
```

**Results**

```{python}
# Execute shallow query for `artists` records
artists_results = fetch_query_results(query)
artists_results
```

### Create `sd_artists_join` JOIN Table

With the `artists` table populated, we create a JOIN table to link the `artists` table to the `spotify_data` table. Since the only IDs we want must be based on these two existing tables, we set constraints on both `artists` and `spotify_data` tables first. This can be done out-of-the-box within our `setup/postgresql.sql` file, but we detail the process for clarity.

```{python}
#| lang: sql
-- Alter the spotify_data and artists table and add a UNIQUE constraint to its `id` column
ALTER TABLE spotify_data ADD CONSTRAINT sd_id_unique PRIMARY KEY (id);
ALTER TABLE artists ADD CONSTRAINT artists_id_unique PRIMARY KEY (id);
```

After this, we create our join table, reference both the `id` columns in `artists` and `spotify_data` table, as well as setting up a composite primary key.

```{python}
#| lang: sql
-- After adding a table constraint to `spotify_data` we create a JOIN table between spotify_data and artists.
-- Automatically delete related rows based on foreign key in spotify_data table
-- Create a composite primary key as a natural key

CREATE TABLE sd_artists_join (
	artist_name text,
	sd_id integer REFERENCES spotify_data (id) ON DELETE CASCADE,
	artist_id integer REFERENCES artists (id),
	CONSTRAINT sd_artist_key PRIMARY KEY (sd_id, artist_id)
)

-- Create indexes for high search volume columns (or those we imagine *will be*)
-- Create indexes on sd_artists_join
CREATE INDEX sd_artists_join_name_idx ON sd_artists_join (artist_name);
CREATE INDEX sd_artists_join_sd_idx ON sd_artists_join (sd_id);
CREATE INDEX sd_artists_join_artist_idx ON sd_artists_join (artist_id)
```

We still must populate the `sd_artists_join` table with data from `artists` and `spotify_data` table. Since there may be NULL values in our `artists` table, we add an appropriate `WHERE` clause for that scenario.

```{python}
#| lang: sql
-- Populate the sd_artists_join table.
INSERT INTO sd_artists_join (artist_name, sd_id, artist_id)
SELECT
    sd.artist_name
    ,sd.id AS spotify_data_id
    ,a.id AS artist_id
FROM spotify_data as SD
LEFT JOIN artists as A
    ON sd.artist_name = a.artist_name
WHERE a.id IS NOT NULL;
```

## üåê Spotify API Enrichment

Now that we have our JOIN table linking `artists` and `spotify_data` tables, we need to populate the artists table with genre data. We extract the artist's name from the database, make a request against the [Spotify Web API](https://developer.spotify.com/documentation/web-api), retrieve the result, and insert genre data for the respective artist into the `genres` column of the `artists` table. Genres data is _not_ directly stored within any column of the `spotify_data` table.

### Test Spotify Web API Interactions

We use the environmental variables of `CLIENT_ID` and `CLIENT_SECRET` to enable our access to the Spotify Web API.

```{python}
#| echo: true
# Obtain a token based on our Client ID and Client Secret. These can be sent as params
# but we have opted to set them as default in the function. 
token = get_token()
# logging.info(token)  # DO NOT persist this in your notebook.

# Search for one of my favorite artists.
result = search_for_artist(token, "Green Piccolo")

# Extract the artist ID from the JSON result.
artist_id = result["id"]

# Use our function to extract songs based on the artist's id value.
songs = get_songs_by_artist(token, artist_id)
```

View artist-related data returned from the Spotify Web API.

```{python}
# View full JSON result data.
result
```

Peruse the artist's genres.

The JSON result is _rich_ with artist-relevant information, along with nice-to-have-elements. This includes links to Artist Spotify URL, their number of followers, artist images hosted on content delivery networks (CDNs), and numerical rankings for artists popularity.

```{python}
#| scrolled: true
# Peruse a list of genres from the result.
result['genres']
```

View songs related to the artist.

```{python}
# Do a clean notebook presentation of JSON.
# display(JSON(songs, expanded=False))
logging.info(json.dumps(songs, indent=2))
```

### Preparing the `artists` Table for Genres

We create a variable representing the `artists` table inside of a [SQLAlchemy](https://www.sqlalchemy.org/docs/stable/tutorial/index.html) `Table` object. Our intent is to iterate over every artist present, issue a Spotify API request based on the artist's name, extract the genres from the returned JSON, then insert this data into the `genres` ARRAY column in the `artists` table.  

Create the Metadata and Table object for `artists` table.

```{python}
# Create table object for `artists`
metadata_obj = MetaData()
artists_table = Table('artists', metadata_obj, autoload_with=engine)
```

Create a query that will select artists from the `artists` table in an ordered fashion. If the `.order_by` function is not used, the query will still return all data meeting query criteria, but the order in which the data can be processed will be random-access, similar to dictionaries in Python. This method will ensure we retain the ability to mark some windows of records as "seen" or "unseen" with future processing.  

```{python}
# Select all artists, in an ordered fashion.
select_all_artists_stmt = select(
    artists_table.c.id,
    artists_table.c.artist_name
).where(
        artists_table.c.id <= 18880                    # Apply to all records by setting this 18880
).order_by(artists_table.c.id)
```

```{python}
logging.info(select_all_artists_stmt)
```

#### Exponential Backoff for Genre Acquisition

We are now set to process every artist present in the `artists` table via `select_all_artists_stmt`, accessing the API, then inserting their genre into the `genres` array column appropriately. 

The Spotify Web API is free, but has rate-limiting restrictions. Exceeding their API rate limit will cause subsequent API requests to fail, and delay the time before requests can be made again. To avoid this situation, we tried to be conservative with the timing between individual API requests, so as not to exhaust our daily quota limits. In an effort to avoid spamming the endpoint unnecessarily, prevent manual intervention, and automate data collection, we use an [exponential backoff](https://bit.ly/4jZi9tN) method to retry API requests if any kind of failure, network disconnection occurs, and spacing out the time gap between each of our data-gathering "sessions." 

The example below demonstrating genre data insertion echoes an API timeout failure even with a small amount of records, likely meriting elongating the time gap between requests.

```{python}
# Execute insertions with a small set of records. Even here we see API timeout.
retry_with_backoff(insert_artist_genres,1,10, engine, artists_table, CLIENT_ID, CLIENT_SECRET)
```

If our code becomes disconnected or the processing is halted by API quota limit, we will restart insertion by invoking our functions with parameters to filter the last processed `artists.id` value. By selecting records in an "ordered" fashion, we can use the last seen record ids to know what boundaries to set for reprocessing. If swaths of records failed to be processed correctly, we can simply record the starting and ending `id` values and re-initiate the processing. This will ensure we only process the amount of requests needed and avoid recurring data for already-seen artists.

#### ‚ö†Ô∏è Use Caution Surfacing the Spotify Web API

Programmatically acquiring genre data via API can be extremely time-consuming and rife with failure. For this project, we sought the _entirety_ of our listening history. If you attempt to mimic this project, we suggest limiting the scope to several hundred records at a time, requiring the ingestion of JSON files encompassing smaller time periods. Another suggestion, if problems are encountered at this stage, is to use the `retry_with_backoff` function with smaller starting and ending `id` values.

## üî¨ Post-Ingestion Overview

After sourcing Spotify data, we initiate data cleaning and validation  processes. 

### üçö Grain

* **spotify_data**: Each row in this table represents a single Spotify playback event.
* **artists**: Each row uniquely identifies an artist and may contain a `genres` array.
* **sd_artists_join**: A JOIN table linking `spotify_data` to `artists` handling one-to-many mapping between an artist and multiple entries in `spotify_data`. 

### üìè Measures

* **ms_played**: Actual playback time in milliseconds. This is not necessarily equivalent to the full length of a Spotify track.

### üìê Dimensions

Every other column in the `spotify_data` table can be considered a dimension. Some examples of these include:

* `timestamp_column`
* `platform`
* `track_name`
* `artist_name`
* `album_name`
* `ip_addr`
* `conn_country`
* `reason_start`
* `reason_end`

## üß© Locate Solvable Issues

First, we want to see how many tracks have NULLs column entries. If there are a sizable amount of issues or errors, the validity of any insights or conclusions drawn may be in doubt.

### Null Records

**Query**

```{python}
# Get total records in `spotify_data` table.
query = """
SELECT COUNT(*)
FROM spotify_data
"""
```

**Results**

```{python}
logging.info(fetch_query_results(query))
```

**Query**

```{python}
query = """
-- Check for NULLs in important columns in `spotify_data`
SELECT
    COUNT(*) FILTER (WHERE timestamp_column IS NULL) AS null_timestamps,
    COUNT(*) FILTER (WHERE ms_played IS NULL) AS null_ms_played,
    COUNT(*) FILTER (WHERE track_name IS NULL) AS null_track_names,
    COUNT(*) FILTER (WHERE artist_name IS NULL) AS null_artist_names,
    COUNT(*) FILTER (WHERE platform IS NULL) AS null_platforms,
    COUNT(*) FILTER (WHERE ip_addr IS NULL) AS null_ips
FROM spotify_data;
"""
```

**Results**

```{python}
logging.info(fetch_query_results(query))
```

Running the above query, we only encountered **177** records that had a combination of empty track names and empty artist names. On further inspection, **173** of these entries were podcast episodes, having a non-null `episode_name`, while the remaining **4** entries were audiobooks.

### Check for Empty Strings

**Query**

```{python}
# We prepend the string with 'r' to process as a raw string.
query = r"""
-- Check for empty strings
SELECT COUNT(*) FROM spotify_data WHERE track_name = '' OR track_name ~ '^\s+$';
SELECT COUNT(*) FROM spotify_data WHERE artist_name = '' OR artist_name ~ '^\s+$';
"""
```

**Results**

```{python}
logging.info(fetch_query_results(query))
```

No records with empty strings were found.

### Detecting Duplicate Records

We make no assumptions about the cleanliness of our Spotify listening data.

We define a "duplicate" as a record sharing the same track name, same artist, same `spotify_track_uri`, IP address, and timestamp with another record. 

First, we count the number of all duplicates using a subquery and output the count of duplicate records present. We use `HAVING` to filter the results of `GROUP BY` after aggregation has been applied; in this case, that is done via the `COUNT` function.

**Query**

```{python}
dupe_check_query = """
-- Obtain count of duplicates
SELECT SUM(dup_count - 1) AS total_duplicates
FROM (
  SELECT COUNT(*) AS dup_count
  FROM spotify_data
  GROUP BY
    timestamp_column,
    artist_name,
    track_name,
    ms_played,
	spotify_track_uri,
	ip_addr
  HAVING COUNT(*) > 1
) sub;
"""
```

**Results**

```{python}
logging.info(fetch_query_results(dupe_check_query))
```

After using  `GROUP BY` with multiple column values, if the counts are greater than one, we have located duplicate records. 

A streaming service _may_ theoretically enable listening to the same track, at the same time, in different locations. 

At the time of writing, I as a user did not take advantage of any simultaneous, multi-session features enabling concurrent playback from different locations, if available. It is very implausible for the same track to be played at the _exact_ same time from different locations.

For samples of our duplicate output:

```{python}
#| lang: sql

"2022-01-16 05:55:35-08"	"not applicable"	178909	"Montania"	"Calma Interiore"	"spotify:track:6t3e2UfHXGVdxXJhPYj6Lg"
"2022-01-16 05:55:35-08"	"not applicable"	178909	"Montania"	"Calma Interiore"	"spotify:track:6t3e2UfHXGVdxXJhPYj6Lg"
"2022-01-16 05:59:48-08"	"not applicable"	253047	"Amoralis"	"Sheila's Disciples"	"spotify:track:6hVsOvinmJAGJJpXxpPhDy"
"2022-01-16 05:59:48-08"	"not applicable"	253047	"Amoralis"	"Sheila's Disciples"	"spotify:track:6hVsOvinmJAGJJpXxpPhDy"
"2022-01-16 06:01:32-08"	"not applicable"	103729	"Objective"	"Olivier Lupin"	"spotify:track:5uxhwtFzJw9Uf2k3uBNXqT"
"2022-01-16 06:01:32-08"	"not applicable"	103729	"Objective"	"Olivier Lupin"	"spotify:track:5uxhwtFzJw9Uf2k3uBNXqT"
```

The timestamp, IP address, platform, and so on, were _exact_ matches. This caught an error I made in data ingestion, loading in the JSON Spotify records; most likely reading the same input file multiple times. There may also be some duplication occurring within the source data, if the JSON files happened to have some temporal overlap. In any case, we can clean these up. 

We create a query with `spotify_data` to find and remove duplicates, relying upon our serial `id` column to assist. 

**Query**

```{python}
dup_delete_query = """
-- DELETE duplicate records in the spotify_data table.
--USE WITH CAUTION! Be sure to backup tables prior to destructive actions.
DELETE FROM spotify_data a
USING spotify_data b
WHERE
  a.ctid < b.ctid AND  -- Keep the "first" row
  a.timestamp_column = b.timestamp_column AND
  a.artist_name = b.artist_name AND
  a.track_name = b.track_name AND
  a.spotify_track_uri = b.spotify_track_uri AND
  a.ip_addr = b.ip_addr AND
  a.ms_played = b.ms_played;
"""
```

```{python}
# Execute the query to remove duplicate record from `spotify_data` table
execute_query(dup_delete_query)
```

We can rerun our `dupe_check_query` and see the count for duplicate records.

```{python}
# Rerun our duplicate check query
logging.info(fetch_query_results(dupe_check_query))
```

After this query, no duplicates were found in the `spotify_data` table.

### Check for Suspicious Playtime Values

Given that tracks have playtimes in milliseconds, it would stand to reason that extraordinarily large or negative playtime values should be detected.

**Query**

```{python}
neg_play_query = """
-- Check for negative playtimes
SELECT * FROM spotify_data WHERE ms_played < 0;
"""

long_play_query = """
-- Check for elongated playtime
SELECT * FROM spotify_data WHERE ms_played > 36000000; -- 10 hours
"""

future_play_query = """
-- Check for playtimes in the future
SELECT * FROM spotify_data WHERE timestamp_column > now();
"""
```

**Results**

```{python}
# Check for negative play time
logging.info(fetch_query_results(neg_play_query))

# Check for elongated play time
logging.info(fetch_query_results(long_play_query))

# Check for playtimes in the Jetson era
logging.info(fetch_query_results(future_play_query))
```

All the queries above returned no entries.

## ‚ûï Augment the Data

Since we anticipate gleaning information on listening patterns in highly granular fashion, we augment our data with a `time_of_day` column. We share the SQL to add this column to an existing `spotify_data` table lacking `time_of_day`. If this column already exists, the command will fail.

```{python}
#| lang: sql
-- First add `time_of_day` column to our tracks in the `spotify_data` table
ALTER TABLE spotify_data
ADD COLUMN time_of_day TEXT;
```

After updating the table (if necessary) we can then update the values for `time_of_day` using the following query.

**Query**

```{python}
update_tod_query = """
UPDATE spotify_data
SET time_of_day = CASE
  WHEN EXTRACT(HOUR FROM timestamp_column) BETWEEN 5 AND 11 THEN 'morning'
  WHEN EXTRACT(HOUR FROM timestamp_column) BETWEEN 12 AND 17 THEN 'afternoon'
  WHEN EXTRACT(HOUR FROM timestamp_column) BETWEEN 18 AND 22 THEN 'evening'
  ELSE 'night'
END;
"""
```

```{python}
# Update `spotify_data` table with time of day column data based on the timestamp column.
# This will take several seconds based upon the size of your dataset.
execute_query(update_tod_query)
```

We similarly add seasonal information to our data.

**Query**

```{python}
seasons_update_query = """
-- Add seasonal data to records
UPDATE spotify_data
SET season = CASE
    WHEN (EXTRACT(MONTH FROM timestamp_column) = 12 AND EXTRACT(DAY FROM timestamp_column) >= 21)
      OR (EXTRACT(MONTH FROM timestamp_column) IN (1, 2))
      OR (EXTRACT(MONTH FROM timestamp_column) = 3 AND EXTRACT(DAY FROM timestamp_column) <= 19)
        THEN 'Winter'
    WHEN (EXTRACT(MONTH FROM timestamp_column) = 3 AND EXTRACT(DAY FROM timestamp_column) >= 20)
      OR (EXTRACT(MONTH FROM timestamp_column) IN (4, 5))
      OR (EXTRACT(MONTH FROM timestamp_column) = 6 AND EXTRACT(DAY FROM timestamp_column) <= 20)
        THEN 'Spring'
    WHEN (EXTRACT(MONTH FROM timestamp_column) = 6 AND EXTRACT(DAY FROM timestamp_column) >= 21)
      OR (EXTRACT(MONTH FROM timestamp_column) IN (7, 8))
      OR (EXTRACT(MONTH FROM timestamp_column) = 9 AND EXTRACT(DAY FROM timestamp_column) <= 22)
        THEN 'Summer'
    WHEN (EXTRACT(MONTH FROM timestamp_column) = 9 AND EXTRACT(DAY FROM timestamp_column) >= 23)
      OR (EXTRACT(MONTH FROM timestamp_column) IN (10, 11))
      OR (EXTRACT(MONTH FROM timestamp_column) = 12 AND EXTRACT(DAY FROM timestamp_column) <= 20)
        THEN 'Fall'
    ELSE 'Unknown'
END;
"""
```

```{python}
# Update `spotify_data` table with seasonal data based on the timestamp column.
# This will take several seconds based upon the size of your dataset.
execute_query(seasons_update_query)
```

## ‚ùì Answering Spotify Wrapped Questions

Our analysis focuses on answering the Spotify Wrapped questions detailed in the ["Understanding My Data"](https://support.spotify.com/us/article/understanding-my-data/) document _and_ seek to apply the answer across the entire listening history. 

1. Number of unique artists listened to for the year.
2. Top artists for the year.
3. Milliseconds spent listening to the number 1 artists.
4. Top percentage fan for the top artist.
5. Number of genres listened to for the year.
6. Top genres for the year.
7. Top podcasts for the year.
8. Milliseconds spent listening to the top podcast.
9. Top percentage fan for the top podcast.
10. Total milliseconds spent listening to podcasts for the year.
11. Top tracks for the year.
12. Amount of plays for the top track of the year.
13. First date the top track was played for the year.
14. Total distinct tracks played for the year.
15. Total milliseconds listened on Spotify for the year.
16. Day with the most time spent listening for the year.
17. Minutes of content listened on the top listening day for the year.
18. Top percentage of worldwide listeners for the year.
19. One-off Wrapped stories for that year.

Some questions cannot be answered from our available data, such as the "Top percentage fan for the top artist" or "Top percentage of worldwide listeners for the year," but the other questions can be explored.

To ensure we avoid including songs that were played mistakenly, or that were skipped immediately, we only consider songs with a play time greater than 5000 ms (5 seconds.)

### Unique Artists Listened To For a Year

This should be simple enough to obtain from our database by first extracting the unique artist and the year the stream was listened to, and then grouping the tuples by year.

**Query**

```{python}
query = """
--- Find unique artists streamed per year. Use songs playing longer than 5 seconds.
WITH distinct_artist_year (artist_name, year_streamed, ts_count) 
AS (
	SELECT DISTINCT 
        artist_name
    	,date_part('year', timestamp_column)::INT  AS year_streamed
    	,COUNT(timestamp_column) AS ts_count 
	FROM spotify_data
	WHERE ms_played > 5000
	GROUP BY 
        artist_name
        ,year_streamed
)
SELECT year_streamed, COUNT(*) as unique_artist_count
FROM distinct_artist_year
GROUP BY year_streamed 
ORDER BY year_streamed ASC
"""
```

**Results**

```{python}
unique_artists_per_year_df = fetch_query_results(query)
unique_artists_per_year_df.head()
```

We use a common table expression to first query for tuples of distinct artists and extract the year portion from the stream timestamp when their streams were observed. The subsequent query aggregates this by year.

```{python}
# Ensure "year_streamed" is the index temporarily.
unique_artists_per_year_df.set_index("year_streamed", inplace=True)

# Reindex directly, filling missing years with 0. This to deal with no streams in 2016.
unique_artists_per_year_df = unique_artists_per_year_df.reindex(
    range(unique_artists_per_year_df.index.min(), unique_artists_per_year_df.index.max() + 1), 
    fill_value=0
)

# Restore "year_streamed" as a regular column.
unique_artists_per_year_df.reset_index(inplace=True)

# Ensure column names are correct.
unique_artists_per_year_df.rename(columns={"index": "year_streamed"}, inplace=True)
```

```{python}
unique_artists_per_year_df
```

```{python}
# Downcast from `int64` to `integer`
unique_artists_per_year_df['year_streamed'] = pd.to_numeric(unique_artists_per_year_df['year_streamed'], downcast='integer')
unique_artists_per_year_df['unique_artist_count'] = pd.to_numeric(unique_artists_per_year_df['unique_artist_count'], downcast='integer')


# Plot the barplot without error bars.
ax = sns.barplot(
    x="year_streamed", 
    y="unique_artist_count", 
    data=unique_artists_per_year_df, 
    errorbar=None,  # Disable error bars
    # order=weekdays_order  # Set the order of the days
)

# Set a bar label to show amounts.
ax.bar_label(ax.containers[0], fontsize=10);

# Add title and labels for clarity
plt.suptitle("Explosive Growth in Artist Discovery by 2024", fontsize=10, fontweight='bold')
plt.title("From One artist in 2014 to 7,657 in 2024", fontsize=9)

plt.xlabel("Year")
plt.ylabel("Artists")
plt.show()
```

The first three years on the platform had negligible listening activity. **2017** is the breakout year, with ever increasing amounts of artists streamed year over year. There is clear trend of accelerated growth in unique artists streamed with every subsequent year on the platform rom **2017** onwards.

### Top Artists for Every Year

We will focus on obtaining the top 5 artists by stream count per year.

**Query**

```{python}
query = """
-- Find the top artists per year by stream count. We use a lateral JOIN to avoid unnecessary row scans.
WITH yearly_counts AS (
    SELECT 
        artist_name
        ,date_part('year', timestamp_column)::INT AS year_streamed 
        ,COUNT(*) AS ts_count
    FROM spotify_data
    WHERE ms_played > 5000
    GROUP BY 
        artist_name
        ,year_streamed
)
SELECT 
    y.year_streamed
    ,top_artist.artist_name
    ,top_artist.ts_count
FROM (
    SELECT DISTINCT date_part('year', timestamp_column)::INT AS year_streamed
    FROM spotify_data
) y
LEFT JOIN LATERAL (
    SELECT 
        artist_name
        ,ts_count
    FROM yearly_counts yc
    WHERE yc.year_streamed = y.year_streamed
    ORDER BY ts_count DESC
    LIMIT 5
) top_artist ON true
ORDER BY y.year_streamed ASC;
"""
```

The `LATERAL` join is used to act as a "for loop," processing each distinct year individually. The query then orders the results by the year ascending and the stream count descending.

**Results**

```{python}
# Find the top artists per year by stream count.
top_5_artists_per_year_df = fetch_query_results(query)

# Sample the results
top_5_artists_per_year_df.head()
```

::: {.panel-tabset}
## üìä View Multiple Top 5 Artists Plots

<details>
<summary>Click to expand plots</summary>

```{python}
#| output: asis
#| scrolled: true

# Ensure the 'year_streamed' column exists and strip whitespace.
top_5_artists_per_year_df.columns = top_5_artists_per_year_df.columns.str.strip()

# Convert 'year_streamed' to integer type.
top_5_artists_per_year_df["year_streamed"] = top_5_artists_per_year_df["year_streamed"].astype(int)

# Confirm the column exists.
# logging.info(top_5_artists_per_year_df.columns)  # Should output Index(['year_streamed', 'artist_name', 'ts_count'], dtype='object')

# Set theme.
sns.set_theme(style="whitegrid")


# Group data by year and plot separately.
for year, group in top_5_artists_per_year_df.groupby("year_streamed"):

    # Assign a dummy hue to avoid annoying deprecation warnings.
    group["dummy_hue"] = group["artist_name"]
    
    plt.figure(figsize=(6, 4))  # Create a new figure for each plot.
    ax = sns.barplot(data=group, x="artist_name", y="ts_count", palette="tab10", hue="dummy_hue", legend=False)

    # Add labels and title
    plt.xlabel("Artist")
    plt.ylabel("Stream Count")
    plt.title(f"Top Artists Streamed in {year}")

    # Rotate x-axis labels for readability.
    plt.xticks(rotation=45, ha="right")

    # Show each plot separately.
    plt.show()
```

</details>
:::

* **2014** is when our data starts, minimally with [Ron Basejam](https://open.spotify.com/artist/3LijUg92GG1XomQ1OkuvfD?si=et47ZoJoT3ib6ROFzy3THg). Never heard of this person before. I wonder why he was one of the first artists discovered.
* **2015** with the exception of [Lars Behrenroth](https://open.spotify.com/artist/7mvD9STpAXPlADKNQAYxFP?si=__UqS2svQVOwPGxTPb86nw) and [Tef Poe](https://open.spotify.com/artist/0Wqbw2ZKno6MrvDcLpc0ax?si=hoLtP_zbTQS9KJPQ1_SK8A), most artists here are unrecognizable. I have acted as an executive producer for one of Lars' shows many moons ago.
* We have "lost" year of absolutely no listening data in **2016**. 
* In **2017** we dive into the platform fully. From **2017** - **2018** we see beginnings of the [Brian Eno](https://open.spotify.com/artist/7MSUfLeTdDEoZiJPDSBXgi?si=h36SrxlkSWuAjIPANd0B6w) era. Other artists I have found memories of being [denitia and sene](https://open.spotify.com/playlist/5vznv7lzMcvEAwYIKjIb3Q?si=6864ffc0347b43bc) and [Kilo Kish](https://open.spotify.com/artist/7lsnwlX6puQ7lcpSEpJbZE?si=dcVYR6xHTUilpbs50A14WA).
* In **2019**, we see the atmospheric, sleep, and nature sounds strewn about. Oddly enough I may never have known I listened to [The Sleep Helpers](https://open.spotify.com/artist/5JTuitdZNeqTBPMa7QU4g4?si=fdTVQC0hSBWLO9LiZBDtpQ) if not for this project.
* **2020** dominated by my binging of reggae with Dexta Daps and Burna Boy. Melancholic R&B championed by Orion Sun and Frank Ocean. MHD being present here echoes the Francophone influence we uncover later on in our analysis.
* **2021** - **2022** indicates the beginning of my content creation and streaming career, with Harris Heller being my DMCA-safe background music. With the exception of [Pop Smoke](https://open.spotify.com/artist/0eDvMgVFoNV3TpwtrVCoTj?si=wKkfPiayQ5O1J7zKffZfPQ) and [NxxxxxS](https://open.spotify.com/artist/36r4ltZmLqtiDBdAs9XSqn?si=di0aqfr2SXiuQ8bafhjbyA), I really do not recognize any other artist in this time period.
* My fixation on the `phonk` genre takes off in **2023** onwards, with [Green Piccolo](https://open.spotify.com/artist/0Of8ndqAY23l2wV3sS6Zez?si=WL0_qdc5Rsq1pOF9OG0bXg), [DJ Acura](https://open.spotify.com/artist/5b8G38VIrYrOdzKocAOH4n?si=v3joabOmQfiNvUiKA_gZJw), [Ryan Celsius Sounds](https://open.spotify.com/artist/2AtYJoC6VmUtkxonmVnbVR?si=-EbJYW_UQX2EtAaeotxbKw), and [Baxdew](https://open.spotify.com/artist/2SQGn8IQmPyUTO6oJtejAl?si=K0IjyLzqSjuIzwyAszGpJw) taking up more slots in the top 5 plots drawing closer to present day. What intrigues me the most is that these artists are "underground," more likely to have cultivated audiences on alternative platforms like [SoundCloud](https://soundcloud.com/discover) or by other means.

### Total Milliseconds Spent Listening to Top Artists

For this question we scope our answer to yearly time periods. Our method to isolate this:

1. Isolate the number one artist by streams, per year, for all years.
2. Aggregate all songs, solo or collaborative, including this artist.
3. Calculate the total streaming time in milliseconds and in a "human-readable" format.
4. Compare the total milliseconds streamed by the top artist versus the total amount of milliseconds stream for that year.

As we have calculated the top 5 artists by streams already, we need only select the most streamed artist per year from this data.

**Query**

```{python}
query = """
-- Reuse previous query but limit results to one artist per year
WITH yearly_counts AS (
    SELECT 
        artist_name
        ,date_part('year', timestamp_column)::INT AS year_streamed 
        ,COUNT(*) AS ts_count
    FROM spotify_data
    WHERE ms_played > 5000
    GROUP BY 
        artist_name
        ,year_streamed
)
SELECT 
    y.year_streamed
    ,top_artist.artist_name
    ,top_artist.ts_count
FROM (
    SELECT DISTINCT date_part('year', timestamp_column)::INT AS year_streamed
    FROM spotify_data
) y
LEFT JOIN LATERAL (
    SELECT 
        artist_name
        ,ts_count
    FROM yearly_counts yc
    WHERE yc.year_streamed = y.year_streamed
    ORDER BY ts_count DESC
    LIMIT 1
) top_artist ON true
ORDER BY y.year_streamed ASC;
"""
```

**Results**

```{python}
# Find time spent listening to top artists.
top_artists_per_year_df = fetch_query_results(query)
top_artists_per_year_df
```

We generate SQL snippets with code iterating over the top artists for each year parameterizing SQL queries that we will eventually combine into a singular dataframe capturing how much time the top artists were played per year. 

```{python}
# Create a list to store rows from our queries
combined_rows = []

# Iterate over artist + year combinations
for index, row in top_artists_per_year_df.iterrows():
    artist = row["artist_name"]
    year = int(row["year_streamed"])

    # Query for artist playtime.
    artist_query = text("""
        SELECT
            date_part('year', timestamp_column)::int AS year_streamed,
            SUM(ms_played) AS total_artist_ms_played
        FROM spotify_data
        WHERE artist_name ILIKE :artist_pattern
          AND date_part('year', timestamp_column)::int = :year
        GROUP BY year_streamed
    """)

    # Query for total playtime that year.
    total_query = text("""
        SELECT
            date_part('year', timestamp_column)::int AS year,
            SUM(ms_played) AS total_ms_played
        FROM spotify_data
        WHERE date_part('year', timestamp_column)::int = :year
        GROUP BY year
    """)

    # Execute queries
    with engine.connect() as conn:
        artist_result = conn.execute(
            artist_query, {"artist_pattern": f"%{artist}%", "year": year}
        ).fetchone()

        total_result = conn.execute(
            total_query, {"year": year}
        ).fetchone()

    # Skip if either query yields nothing
    if artist_result is None or total_result is None:
        continue

    # Unpack the results
    year_streamed, total_artist_ms_played = artist_result
    _, total_ms_played = total_result

    # Append combined results
    combined_rows.append({
        "year_streamed": year_streamed,
        "artist_name": artist,
        "total_artist_ms_played": total_artist_ms_played,
        "total_ms_played": total_ms_played,
        "artist_share": total_artist_ms_played / total_ms_played
    })

# Build final DataFrame
time_listened_top_artists_df = pd.DataFrame(combined_rows)
```

Armed with a dataframe with time listening to top artists, we can convert milliseconds into hours to more easily measure listening activity. We calculate the `artist_share`, the amount of listening activity an artist has scaled from **0** to **1**, but also communicate a normal percentage value. In latter years of the listening history, the top artists take up a consistent, yet reducing amount of `artist_share` due the volume of listening activity.

```{python}
# Convert milliseconds into hours 
time_listened_top_artists_df['total_artist_hours_played'] = time_listened_top_artists_df['total_artist_ms_played'] / 3600000

# Convert artist share (x/1) to percentage
time_listened_top_artists_df['percentage_artist_share'] = time_listened_top_artists_df['artist_share'] * 100

time_listened_top_artists_df
```

This information mostly converges with our "Top Artists" data. "Eras" of listening can be delineated by the composition of top artists. [Ron Basejam](https://open.spotify.com/artist/3LijUg92GG1XomQ1OkuvfD?si=b-IE8_BrQGKzmQVKYI3BFQ) and [Lars Behrenroth](https://open.spotify.com/artist/7mvD9STpAXPlADKNQAYxFP?si=6zXmlYYTTdykdEvwxCZaww) being within the Dance/Electronic genres. The presence of Brian Eno and [Darius](https://open.spotify.com/artist/5vfEaoOBcK0Lzr07WN8KaK?si=8i-1vizvRN2fKucoToG5aA) seems a bit more atmospheric; both artists producing ambient, downtempo electronic music. Much less "danceable."  A hard pivot to Dancehall Reggae with [Dexta Daps](https://open.spotify.com/artist/28UDeKu2FPrU0T7dpUiSGY?si=jlaoTooRQie4rHh4BGQ31A); I suspect I binge-listened to "Breaking News" due to the provocative lyrics and the story they told. 
[Harris Heller](https://open.spotify.com/artist/6GTRLqqiBPUqaOgyxOraHp?si=f7LpJKWmSfuNMkubDkEncg)'s three years of heavy rotation was derived from my streaming career and needing to have music free from DMCA restrictions to use to add background to my streams; the genres produced by Harris Heller can vary, but I would generally use Lo-Fi or House Music. [Green Piccolo](https://open.spotify.com/artist/0Of8ndqAY23l2wV3sS6Zez?si=XAxVLIfHR--WLumst1PykA) is a Phonk artist from Europe, with that genre being an eclectic amalgam of African American music across several different decades.
.

We can peruse descriptive statistics from the dataframe capturing the top artist data.

```{python}
logging.info("Descriptive stats for total top artist hours.")
time_listened_top_artists_df['total_artist_hours_played'].describe()
```

```{python}
logging.info("Descriptive stats for percentage artist share for top artists.")
time_listened_top_artists_df['percentage_artist_share'].describe()
```

Quite surprised that the average time listening to a favorite artists is around **71.8** hours per year. The percentage of my total streaming time for a favorite artists falls in the range of **3.53%** and **4.18%** for the past four years. 

### Number of Genres Listened to for the Year

Spotify does not bundle genre information with individual tracks, which was an extremely surprising fact. Genre associations are tied to artists instead of songs. This design decision on Spotify's part seems efficient. You save tons of storage space that would be "wasted" storing this information within tracks.  It is _possible_ to infer what genre a song is strictly by the artist, assuming the artist has consistency within their catalog. 

What this fails to account for is the possibility of artists potentially shifting or changing genres on a per-track basis. It may also be a challenge if songs have more than one collaborator, from a diverse genre, which may render deciding a song's genre even more challenging. Which artist's genre should take precedence in this scenario? Despite these corner cases, I suspect that Spotify felt the edge cases did not override the generalized ability to identify a song's genre based on the artist.

With genres present in the `artists` table from our previous work, we can determine how many unique genres were listened to, partitioning by year.

Using a CTE, we "unnest" the genres from the `genres` column in the `artists` table, while joining with the `sd_artists_join` and `spotify_data` table, while also ensuring we do not count records that are null, empty, nor tracks that have not been played longer than 5 seconds (5000 ms).

After this, we use another CTE to generate distinct genres per year, which we subsequently aggregate via the `COUNT` function. We are not concerned with how often a genre appeared in a year, just that it appeared at all. We can then output each year and how many genres are present in our streaming history per year.

#### Plotting Unique Genre Counts Per Year

**Query**

```{python}
query = """
-- Find how many unique genres were played per year 
WITH genre_year_cte AS (
    SELECT
        DISTINCT UNNEST(a.genres) AS genre
        ,DATE_PART('year', sd.timestamp_column)::INT AS stream_year
    FROM artists a
    JOIN sd_artists_join sdj
        ON a.id = sdj.artist_id
    JOIN spotify_data sd
        ON sdj.sd_id = sd.id
    WHERE a.genres IS NOT NULL
      AND a.genres != '{}'
      AND sd.ms_played > 5000
),
distinct_genres_per_year AS (
    SELECT 
        stream_year
        ,genre
    FROM genre_year_cte
    GROUP BY 
        stream_year
        ,genre
)
SELECT
    stream_year
    ,COUNT(*) AS unique_genre_count
FROM distinct_genres_per_year
GROUP BY stream_year
ORDER BY stream_year ASC;
"""
```

**Results**

```{python}
# Query DB for unique genres per year.
unique_genre_counts_per_year_df = fetch_query_results(query)
unique_genre_counts_per_year_df.head()
```

```{python}
# Set plot size to 8 * 4
plt.figure(figsize=(8, 4))

# Plot the barplot without error bars
ax = sns.barplot(
    x="stream_year", 
    y="unique_genre_count", 
    data=unique_genre_counts_per_year_df, 
    errorbar=None,  # Disable error bars
    # order=weekdays_order  # Set the order of the days
)

# Set a bar label to show amounts
ax.bar_label(ax.containers[0], fontsize=10);


# Add title and labels for clarity
plt.suptitle("Unique Genres Streamed by Year", fontsize=10, fontweight='bold')
plt.title("Steady, Linear Increase in Genre Diversity", fontsize=9)

plt.xlabel("Year")
plt.ylabel("Genres")
plt.show()
```

We can calculate the average number of unique genres we encounter. We will exclude the years at the "edges" of our listening history. 

```{python}
temp_ugc_list = unique_genre_counts_per_year_df['unique_genre_count'].to_list()
# # Remove the edge elements
temp_ugc_list.pop(0)
temp_ugc_list.pop(len(temp_ugc_list) - 1)
temp_ugc_list

ugc_series = pd.Series(temp_ugc_list)

# Compute differences
differences = ugc_series.diff().dropna()

# Average difference
average_diff = differences.mean()

logging.info(f"Differences: {differences.tolist()}")
logging.info(f"Average genre count difference: {average_diff}")
        
```

We see a gradual increase in unique genres played per year. The plot edges are explainable outliers, with the initial meteoric rise of my earnest use Spotify starting in **2017** and the partial data present for the current year of **2025**.

### Top Genres for the Year

To answer this question, we will consider a "Top Genre" to be a genre played heavily during a yearly time period. This will rely on the elapsed time detailed by `ms_played` field in the `spotify_data` table, and will be determined by the number of songs played in any respective genre. We are using the actual time listened, not the number of tracks listened to.

To craft the SQL query for this, we use two CTEs. The first will provide the `ms_played` along with initial artist and `genres` column data. We then "unnest" the `genres` column, sum the elapsed time for each genre, partitioning by the year for our ranking. Since we want the "top" genres, we filter the genres by their ranking, limiting the amount to ranks less than 10. This lacks precision, as ranking "ties" mean that genres may have the same numeric rank.

**Query**

```{python}
query = """
-- Generate Top Genres per Year
WITH artist_playtime AS (
	SELECT 
        a.artist_name
		,a.genres
		,sd.ms_played
		,DATE_PART('year', sd.timestamp_column)::INT AS stream_year
	FROM artists AS a
	JOIN sd_artists_join AS sdj 
		ON a.id = sdj.artist_id
	JOIN spotify_data AS sd 
		ON sdj.sd_id = sd.id
),
genre_ranks AS (
	SELECT
		stream_year,
		UNNEST(genres) AS genre,
		ROUND((CAST(SUM(ms_played) AS numeric)  / 3600000) , 2) AS hours_played,
		RANK() OVER (
			PARTITION BY stream_year
			ORDER BY SUM(ms_played) DESC
		) AS genre_rank
	FROM artist_playtime
	GROUP BY stream_year, genre
)
SELECT *
FROM genre_ranks
WHERE genre_rank <= 10
ORDER BY stream_year, genre_rank
"""
```

**Results**

```{python}
# Execute query to find the top genres per year.
top_genres_per_year_df = fetch_query_results(query)

# Peruse top genres per year.
top_genres_per_year_df
```

#### Pivot Table Exploration 

```{python}
# Create pivot dataframe
top_genre_pivot = top_genres_per_year_df.pivot(index='stream_year', columns='genre', values='genre_rank').fillna('-')

# Modify display options to show all columns
pd.set_option('display.max_columns', None)

top_genre_pivot
```

Minimal activity is seen in **2015** and a small number of genres are present then.

From **2017** through **2019** there were large amounts of genre diversity. The history communicates interest in `art rock`, `ambient`, `drone`, `lo-fi beats`, and `alternative r&b`. These years feel experimental and exploratory.

The years of **2020** through **2025** shows a shift to more stable listening habits. "Lo-Fi" family genres dominate, such as `lo-fi beats`, `lo-fi hip hop`, and the mall-like resonance of `vaporwave.` Previous genres like `drone` and `ambient` still have presence, implying I have a preference for atmospheric, introspective music.

My tastes appear to be changing organically...or are being guided.

#### How Many Times are Genres in the Top 10?

While we can visually inspect the pivot dataframe and glean by eye what genres appeared in the top ranking by elapsed time played, we can introspect on the original dataframe easily with `value_counts` with some method chaining, and see if there are any dominant genres across all years.

```{python}
# Count how many times a genre has appeared in the top 10 ranking by time elapsed.
top_genres = top_genres_per_year_df['genre'].value_counts().head(10)
```

```{python}
# Sort descending
top_genres_sorted = top_genres.sort_values(ascending=True)  # reverse for horizontal bars

# Plot
fig, ax = plt.subplots(figsize=(8, 4))
bars = ax.barh(top_genres_sorted.index, top_genres_sorted.values, color='#467fcf')

# Titles
ax.set_title("Atmospheric Genres Dominate Top 10 Rankings",
             fontsize=10, fontweight='bold', loc='left')


# # Add title and labels for clarity
# plt.suptitle("Atmospheric Genres Dominate Top 10 Rankings", fontsize=10, fontweight='bold', loc='left')
# plt.title("From 1 artist in 2014 to 7,657 in 2024", fontsize=9, loc='left')

# Labels
ax.set_xlabel("Top 10 Appearances")
ax.set_ylabel("Genre")

# Add count labels
for bar in bars:
    width = bar.get_width()
    ax.text(width + 0.1, bar.get_y() + bar.get_height()/2,
            str(int(width)), va='center', fontsize=10)

plt.tight_layout()
plt.show()
```

No single genre seems on top for all years, but `ambient`, `drone`, and `lo-fi` beats have repeated presence in the top genres. 

It was surprising not to not see any `*phonk*` topping the charts, but we must recall that this is based on _elapsed time_, not the _volume_ of individual tracks played. Instrumental style genres may have longer track playing times, to add to the intrigue.

```{python}
# Find out how many times a genre held the number one spot across listening history.
top_rank_genre = top_genres_per_year_df[top_genres_per_year_df['genre_rank'] == 1]

# Count the genre instances.
top_rank_genre_counts = top_rank_genre['genre'].value_counts()
```

```{python}
# Sort descending
top_rank_genre_counts_sorted = top_rank_genre_counts.sort_values(ascending=True)  # reverse for horizontal bars

# Plot
fig, ax = plt.subplots(figsize=(6, 4))
bars = ax.barh(top_rank_genre_counts_sorted.index, top_rank_genre_counts_sorted.values, color='#467fcf')

# Titles
ax.set_title("Ambient is a Serial First-Place Finisher",
             fontsize=10, fontweight='bold', loc='left')


# Labels
ax.set_xlabel("Times Ranked First Among Genres")
ax.set_ylabel("Genre")

# Add count labels
for bar in bars:
    width = bar.get_width()
    ax.text(width + 0.1, bar.get_y() + bar.get_height()/2,
            str(int(width)), va='center', fontsize=10)

# https://stackoverflow.com/questions/12608788/changing-the-tick-frequency-on-the-x-or-y-axis -- Gary Steele
plt.gca().xaxis.set_major_locator(plt.MultipleLocator(1))

plt.tight_layout()
plt.show()
```

`Ambient` rules the yard with **4** top finishes, with `lo-fi beats` only having half the amount. `Minimalism` did not finish first as much as expected, with more danceable, vocalized genres like `afro house`, `deep house`, and `indie soul` rounding out the group. 

#### Total Hours Played Per Genre 

::: {.panel-tabset}
##### üìä View Total Hours Played Per Genre Plot

<details>
<summary>Click to expand plot</summary>

```{python}
# Compute total hours played per genre (top-ranked only).
genre_totals = (
    top_genres_per_year_df
    .groupby('genre')['hours_played']
    .sum()
    .sort_values(ascending=False)
)

# Convert to dataframe
genre_df = genre_totals.reset_index()

# Create horizontal bar chart
fig = px.bar(
    genre_df,
    x='hours_played',
    y='genre',
    orientation='h',
    color='hours_played',
    color_continuous_scale='Blues',
    labels={'hours_played': 'Total Hours Played', 'genre': 'Genre'},
    # title='Total Listening Time by Top-Ranked Genre (All Years)'
)

# Make highest value on top
fig.update_layout(yaxis=dict(categoryorder='total ascending'))
fig.update_layout(height=800)  # Increase height for readability

fig.update_layout(
    title={
        'text': "Say the Quiet Part Out Loud<br><sup>Top Genres by Total Listening Time (All Years Combined)</sup>",
        'y':0.97,
        'x':0.5,
        'xanchor': 'center',
        'yanchor': 'top'
    }
)


fig.update_traces(marker_line_width=0.5)

fig.show()
```

</details>
:::

The `ambient` genre has the top spot, which makes sense for being used as a default background music, mood-setting device. Other genres like `space music` and `drone` are not far behind. Rounding out the set are `minimalism`, `lo-fi`, and `synthwave.`

#### Yearly Breakdown of Hours Played Per Top Genre

::: {.panel-tabset}
##### üìä Yearly Breakdown of Hours Played Per Top Genre

<details>
<summary>Click to expand plot</summary>

While a birds-eye view of our entire history is interesting, we can subdivide this among individual years and surface what genres possessed the highest hours played.

```{python}
# Create dataframe
unique_years = top_genres_per_year_df['stream_year'].unique()

for year in sorted(unique_years):
    df_year = top_genres_per_year_df[top_genres_per_year_df['stream_year'] == year]
    
    # Group by genre and sum hours (in case of duplicates).
    genre_totals = (
        df_year
        .groupby('genre')['hours_played']
        .sum()
        .sort_values(ascending=False)
        .reset_index()
    )
    
    # Skip years with no valid data
    if genre_totals.empty:
        continue

    # Plot horizontal bar chart.
    fig = px.bar(
        genre_totals,
        x='hours_played',
        y='genre',
        orientation='h',
        color='hours_played',
        color_continuous_scale='Blues',
        labels={'hours_played': 'Total Hours Played', 'genre': 'Genre'},
    )

    fig.update_layout(
        yaxis=dict(categoryorder='total ascending'),
        height=700,
        title={
            'text': f"Yearly Breakdown of Hours Played<br><sup>Top Genres by Listening Time ‚Äì {year}</sup>",
            'y': 0.97,
            'x': 0.5,
            'xanchor': 'center',
            'yanchor': 'top'
        }
    )

    fig.update_traces(marker_line_width=0.5)
    fig.show()
```

</details>
:::

#### Top Genres Over Time

We generate an area chart for our top genres to visualize how the composition of the hours played for these genres changed over time.

```{python}
# Copy top_genres_per_df dataframe and coerce cols
df = top_genres_per_year_df.copy()
df['stream_year'] = pd.to_numeric(df['stream_year'], errors='coerce')
df['hours_played'] = pd.to_numeric(df['hours_played'], errors='coerce')
df = df[df['hours_played'] > 0]

# Group data by year and genre
df_grouped = (
    df.groupby(['stream_year', 'genre'], as_index=False)
    .agg({'hours_played': 'sum'})
)

# Total by genre for sorting 
genre_totals = df_grouped.groupby('genre')['hours_played'].sum()
sorted_genres = genre_totals.sort_values(ascending=False).index.tolist()

# Build plot
fig = go.Figure()

for genre in sorted_genres:
    genre_df = df_grouped[df_grouped['genre'] == genre]
    fig.add_trace(go.Scatter(
        x=genre_df['stream_year'],
        y=genre_df['hours_played'],
        mode='lines',
        name=genre,
        stackgroup='one',
        hovertemplate=(
            f"Genre: {genre}<br>" +
            "Year: %{x}<br>" +
            "Hours Played: %{y:.2f}<extra></extra>"
        )
    ))

# Configure plot layout
fig.update_layout(
    title="Top Genres over Time<br><sup>2020 Had Extreme Diversity With Minimal Playtime</sup>",
    xaxis_title='Year',
    yaxis_title='Hours Played',
    height=600,
    xaxis=dict(dtick=1),
    title_x=0.5,
    hovermode='closest',  # <--- KEY CHANGE HERE
    showlegend=True
)

fig.show()
```

**2020** had the lowest number of hours played, yet the largest amount of unique genres involved. The two apexes on either side of this year have heightened hours played with less cumulative genres in play. 

#### Total Historical Listening Time

**Query**

```{python}
query = """
-- What is the total time we have listened to *EVERYTHING* on Spotify
SELECT ROUND((CAST(SUM(ms_played) AS numeric)  / 3600000) , 2) AS total_hours_played
FROM spotify_data
"""
```

**Results**

```{python}
# Execute a query to populate our dataframe to find total listening time in hours.
total_historic_listening_time_df = fetch_query_results(query)

# View total listening in hours for entire Spotify history.
total_historic_listening_time_df
```

```{python}
# How much of our listening history was `ambient`? (This is the first entry.)
(1688.50 / total_historic_listening_time_df.iloc[0]) * 100
```

This ends up being **14129.45** hours, or ~**588.7** days of listening, or ~**1.61** years, with `ambient` eating up **11.95%** of our listening history.

#### What is `Space Music`?

Not going to lie, I have _never_ heard of `space music` and wondered why it ranked so highly.  Since the genres are associated with an artist, let's uncover who these folks might be from our database.

**Query**

```{python}
query = """
-- We must find the `space music` people
SELECT DISTINCT a.artist_name
	--,a.genres
	,ROUND((CAST(SUM(sd.ms_played) AS numeric)  / 3600000) , 2) AS hours_played
	--,SUM(sd.ms_played) AS time_played
	--,DATE_PART('year', sd.timestamp_column) AS stream_year
FROM artists AS a
JOIN sd_artists_join AS sdj 
	ON a.id = sdj.artist_id
JOIN spotify_data AS sd 
	ON sdj.sd_id = sd.id
WHERE 'space music' = ANY (a.genres)
GROUP BY a.artist_name
ORDER BY hours_played DESC
LIMIT 10
"""
```

**Results**

```{python}
# Query DB for `space music` genre tracks.
top_10_space_music_df = fetch_query_results(query)

# View the `space music` artists and the hours played.
top_10_space_music_df.head()
```

```{python}
# Sort descending
# top_10_space_music_df_sorted = top_10_space_music_df.sort_values(by='hours_played',ascending=False)  # reverse for horizontal bars

# Copy our dataframe
df = top_10_space_music_df.copy()

# Sort for better visual
df = df.sort_values(by='hours_played', ascending=True)

# Plotting
fig, ax = plt.subplots(figsize=(8, 4))
bars = ax.barh(df['artist_name'], df['hours_played'])

# Add data labels
for bar in bars:
    width = bar.get_width()
    ax.text(width + 1, bar.get_y() + bar.get_height()/2, f'{width:.1f}', va='center')

# Titles
plt.suptitle("Top 10 'Space Music' Artists", fontsize=10, fontweight='bold', x=0.4)
plt.title("Who Are These People?", fontsize=9, x=0.13)
plt.xlabel("Hours Played")
plt.ylabel("Artist")
plt.tight_layout()
plt.show()
```

The presence of `space music` makes more sense now. I recognize the artist [Brian Eno](https://open.spotify.com/artist/7MSUfLeTdDEoZiJPDSBXgi?si=hpYRD84HSGO9De1pJWYgRA), as there was a time period I was placing his work on _heavy_ rotation, in addition to downloading his multiple [generative-music phone apps](https://apps.apple.com/us/app/brian-eno-reflection/id1180524479). The airy, atmospheric nature of his music I could see falling into a description akin to `space music`, though  I would associate the term with background music played over pulp fiction television shows like "Lost In Space."

This does validate the concern I had regarding the association of genres with artists, instead of individual songs. I may _not_ be listening to `space music`, but the artist I listen to may have work relating to that (figurative) space. The genre ranks highly in my history, but I had no idea such a genre even existed. It means increased classification ambiguity in exchange for data storage efficiency. 

### Top Podcasts

I do _not_ generally listen to podcasts, on any platform. I expect to find minimal amounts of listening activity in the data.

There is no direct column that delineates a track from a podcast, but this can be cautiously inferred from the presence of data in the `episode_show_name` and `episode_name` fields. Another interesting aspect of these records, is that none of the records having data present in`episode_show_name` and `episode_name` have any `artist_name` data.  

We should exercise caution with the inferring a record matching the above criteria definitively _is_ a podcast, as evidenced by the infamous `"EndIess [CDQ],"` the audio-only version of [Frankie Ocean's](https://open.spotify.com/artist/2h93pZq0e7k5yf4dywlkpM?si=WfT2R3fSSlS9qt22MVJ9iQ) visual album. This album appears in our search results, and is definitely not a podcast.

**Query**

```{python}
query = """
-- Find top podcasts per year.
WITH podcast_tracks AS (
	SELECT * FROM spotify_data
	WHERE 
		ms_played > 5000
	AND episode_show_name IS NOT NULL
	AND episode_name IS NOT NULL
	AND episode_show_name NOT ILIKE '%EndIess [CDQ]%'
)
SELECT DISTINCT episode_show_name
	,DATE_PART('year', timestamp_column) AS stream_year
	,ROUND((CAST(SUM(ms_played) AS numeric)  / 3600000) , 2) AS total_hours_played
FROM podcast_tracks
GROUP BY episode_show_name
    ,artist_name
    ,DATE_PART('year', timestamp_column)
ORDER BY
    DATE_PART('year', timestamp_column) ASC
    ,ROUND((CAST(SUM(ms_played) AS numeric)  / 3600000) , 2) DESC
"""
```

This query finds all records that possess non-null show names and episode names. Using a CTE, we then extract only distinctive show names, computing their total play time. We sort first by the year streamed in ascending order, then by the play time in descending order. This will provide a ranking of the podcasts by the amount of hours played, as well as sorting the records chronologically by year.

**Results**

```{python}
# Query DB for top podcasts per year.
total_podcasts_per_year_df = fetch_query_results(query)

# View top genres per year.
total_podcasts_per_year_df
```

```{python}
# Enable threshold for total hours played
threshold = 0.25

# Convert stream_year as a string for cleaner labels.
total_podcasts_per_year_df['stream_year'] = total_podcasts_per_year_df['stream_year'].astype(str)

# Create filtered_df to remove records not matching threshold
filtered_df = total_podcasts_per_year_df[total_podcasts_per_year_df['total_hours_played'] > threshold]

# Sort by year and total hours to make the plot readable.
sorted_df = filtered_df.sort_values(['stream_year', 'total_hours_played'], ascending=[True, False])

fig = px.bar(
    sorted_df,
    x='total_hours_played',
    y='episode_show_name',
    color='stream_year',
    orientation='h',
    title='Top Podcasts by Year (Total Hours Played)<br><sup>Data Showing I Like Comedy and Data</sup>',
    # title="Top Genres over Time<br><sup>2020: Extreme Diversity With Minimal Volume</sup>",
    labels={
        'episode_show_name': 'Podcast',
        'total_hours_played': 'Total Hours Listened',
        'stream_year': 'Year'
    },
    height=900,  # Adjust height to avoid overlapping bars
)

fig.update_layout(
    yaxis={'categoryorder': 'total ascending'},
    barmode='group',
    legend_title_text='Year',
    xaxis_tickformat='.1f',
    margin=dict(l=200, r=40, t=60, b=40),
)


# Shift title
#fig.update_layout(title={'x': 0.5, 'xanchor': 'left'})
fig.update_layout(title={'x': 0.268,})

fig.show()
```

To augment our analysis, we created a CSV to map podcast categories to each podcast from the following list:

* Comedy
* Football
* Data
* Environment
* Music
* Politics
* Other
* Anime
* Sports

```{python}
# Load the manually podcast categorization mapping from the CSV.
category_map_df = pd.read_csv(DATA_PATH / 'extract' / 'podcast_categories.csv')

# Create the dictionary correctly.
category_map_dict = dict(zip(category_map_df["episode_show_name"],
                             category_map_df["category"]))

# Add the new category column to dataframe, using the dict to map. 
total_podcasts_per_year_df['category'] = total_podcasts_per_year_df['episode_show_name'].map(category_map_dict)

# Sample the results.
total_podcasts_per_year_df.head()
```

After augmenting the data with categories, we can then plot out how podcasts are distributed across them.

```{python}
# Count the number of times a podcast category appears.
podcast_category_counts = total_podcasts_per_year_df['category'].value_counts().reset_index()

# Rename columns for clarity.
podcast_category_counts.columns = ['category', 'podcast_count']
```

```{python}
# Create a bar chart showing the number of podcasts per category.
fig = px.bar(
    podcast_category_counts,
    x="category",
    y="podcast_count",
    title="Number of Podcasts Per Category<br><sup>I Want to Hear About Data and Comedy, Not Politics and the Environment</sup>",
    text_auto=True
)
fig.update_layout(yaxis_title="Count")
fig.show()
```

The podcasts I enjoy seem to align with my personal interests over things I can control (data, comedy, football.) Less emphasis is on hot-button topics like Politics and the Environment, that I (personally) cannot control.  

```{python}
# Display categorized podcast data.
fig = px.bar(
    total_podcasts_per_year_df,
    x='total_hours_played',
    y='episode_show_name',
    color='category',
    facet_col='stream_year',
    facet_col_wrap=3,  # Adjust columns per row
    title="Top Podcasts Per Year, Categorized",
    height=800,
    width=1200
)

# Simplify facet labels by removing 'stream_year=' prefix.
fig.for_each_annotation(lambda a: a.update(text=a.text.split('=')[-1]))

# Clean up layout
fig.update_layout(
    showlegend=True,
    xaxis_title="Hours Played",
    yaxis_title="Podcast Show",
    margin=dict(l=80, r=20, t=60, b=60),
)

# Reduce tick clutter.
fig.update_xaxes(showticklabels=False)

# Align title
fig.update_layout(title={'x': 0.265})

fig.show()
```

Given my interest in data careers, the prominence of data-related podcasts does not surprise me. I was not immediately aware of how often I played `Comedy` podcasts, at least to a level equivalent to `Football` shows.

The `Other` podcasts appear to be either variety shows, shows I cannot categorize due to their absence from the current Spotify catalog, or tracks mistakenly categorized as podcasts.

An interesting pattern, confirming my disinterest in podcasts generally, is that no podcast was _ever_ played more than once in any individual year. There are no shows that were ever played more than once in the same calendar year. Given this behavior, I will deem the "top podcasts" by the number of times played over the entire listening history.

I became curious as to how many _different_ podcasts I would play, each over the entire listening history. 

**Query**

```{python}
query = """
	SELECT 
        year_played as year
        ,episode_show_name
        ,COUNT(*) AS podcast_play_count 
        FROM spotify_data
	WHERE 
		ms_played > 5000
	AND episode_show_name IS NOT NULL
	AND episode_name IS NOT NULL
	AND episode_show_name NOT ILIKE '%EndIess [CDQ]%'
    GROUP BY
        year_played
        ,episode_show_name
"""
```

**Results**

```{python}
podcast_play_count = fetch_query_results(query)
```

```{python}
#| echo: false
# Get a sheer number of podcasts played of any kind.
podcast_play_count.groupby('year')['podcast_play_count'].sum()
```

```{python}
#| scrolled: true
# Total podcast plays per year, we don't care if same show
# or multiple different shows.
podcast_plays_per_year = podcast_play_count.groupby('year')['podcast_play_count'].sum().reset_index()

# Line chart for total podcast plays per year.
plt.figure(figsize=(8, 4))
plt.plot(podcast_plays_per_year['year'], podcast_plays_per_year['podcast_play_count'], marker='o')
plt.suptitle("Total Podcast Plays Per Year", fontsize=10, fontweight='bold')
# Add title and labels for clarity
#plt.suptitle("Unique Genres Streamed by Year", fontsize=10, fontweight='bold')
plt.title("Human Voices Were Most Appealing in 2020", fontsize=9, )
plt.xlabel("Year")
plt.ylabel("Play Count")
plt.grid(True)
plt.tight_layout()
plt.show()
```

**2020** gives the impression I am seeking out new content, new voices, new ideas on Spotify. But is this true? Since I know I have binge-style behaviors, we can reveal how many _distinct_ podcasts were listened to.

```{python}
#| scrolled: true
# Distinct podcasts per year
distinct_podcasts_per_year = podcast_play_count.groupby('year')['episode_show_name'].nunique().reset_index()
distinct_podcasts_per_year.columns = ['year', 'distinct_podcast_count']

# Bar chart with number of distinct podcasts played per year
plt.figure(figsize=(8, 4))
plt.bar(distinct_podcasts_per_year['year'], distinct_podcasts_per_year['distinct_podcast_count'], color='mediumseagreen')
plt.title("Distinct Podcasts Listened to Per Year")
plt.xlabel("Year")
plt.ylabel("Unique Podcast Shows")

# Force all year labels to show
plt.xticks(distinct_podcasts_per_year['year'])

plt.tight_layout()
plt.show()
```

An extremely "normal" looking distribution of distinct podcasts over the listening history, reaching the zenith in **2021**.

```{python}
total_podcasts_per_year_df[total_podcasts_per_year_df['category'] == 'Sports']
```

* The top podcast over the entire listening is a `Comedy` podcast called [Another Round](https://open.spotify.com/show/6Fige4EyC2sg8jVVdQgIdN?si=e8c6090ae11c4e41). This show appears in the years **2017**, **2019**, and **2021**. I really enjoy revisiting this show, and hearing happy, joyful women laughing and joking together.
* **2020** onwards, I began to listen increasingly to more data-related content, such as [Linear Digressions](https://open.spotify.com/show/1JdkD0ZoZ52KjwdR0b1WoT?si=8c5f647ebd50492f) and the [Data Engineering Podcast](https://open.spotify.com/show/2iLvljRGVVIGlJshT5vNDS?si=e25a57ca8dc7469c).

* Increasing amounts of football podcasts, starting in **2019**.  I am subscribed to [The Athletic](https://www.nytimes.com/athletic/), am an Arsenal fan, and watch as much football as possible. The leadup to the 2022 World Cup in Qatar also contributed to higher consumption of football-related media. [Project Fu√üball](https://open.spotify.com/show/4uCsr5lakqLtuEwkVnxVUC?si=b9537a1aa5a8439e) and [Handbrake Off](https://open.spotify.com/show/3ANY129Bnkfhq4tSXytqhn?si=2a7ae58e8447477c) and [Tifo Football Podcast](https://open.spotify.com/show/06QIGhqK31Qw1UvfHzRIDA?si=3d63760305044150) being present is unsurprising.

### Time Spent Listening to Top Podcasts

Since our surfacing of podcasts appears to be singular in nature, we will simply group the data by the show name, total the hours played, then sort the resultant dataframe in descending order to see what shows come out on top.

```{python}
#| scrolled: true
# Step 1: Aggregate by show and category.
podcast_total_df = (
    total_podcasts_per_year_df
    .groupby(['episode_show_name', 'category'], as_index=False)['total_hours_played']
    .sum()
    .sort_values('total_hours_played', ascending=True)
    .tail(20)  # Optionally select top 20
)

# Step 2: Plot using Plotly Express, color-coded by 'category.'
fig = px.bar(
    podcast_total_df,
    x='total_hours_played',
    y='episode_show_name',
    color='category',  # <--- Adds color grouping
    orientation='h',
    title='Top Podcasts by Total Hours Played (All Time)<br><sup>I Enjoy Joyful Women, Football, and Data Warehouse Chatter',
    labels={
        'episode_show_name': 'Podcast',
        'total_hours_played': 'Total Hours Listened',
        'category': 'Podcast Category'
    },
    height=600
)

fig.update_layout(
    yaxis=dict(categoryorder='total ascending'),
    margin=dict(l=200, r=40, t=60, b=40),
    legend_title='Category'
)


fig.update_layout(title={'x': 0.273,})

fig.show()
```

Viewing the cumulative playing time of podcasts is fine, but we can break down what was listened to and when in a more granular fashion. We can group this data by both the year played and their show names, then do faceted data display, along with color-categorization based on the podcast type. 

```{python}
# We can limit to top N amount of shows per year, or expand this more fully
# to include all the podcasts in the history if we choose.

# Group by year and show, sum total hours.
grouped_df = (
    total_podcasts_per_year_df
    .groupby(['stream_year', 'episode_show_name', 'category'], as_index=False)['total_hours_played']
    .sum()
    .sort_values(['stream_year', 'total_hours_played'], ascending=[True, False])
)

# For each year, select the top 5 podcasts.
top_per_year = (
    grouped_df
    .groupby('stream_year', group_keys=False)
    .apply(lambda x: x.nlargest(5, 'total_hours_played'))
    .reset_index(drop=True)
)

# Create faceted horizontal bar charts.
fig = px.bar(
    top_per_year,
    x='total_hours_played',
    y='episode_show_name',
    color='category',
    facet_col='stream_year',
    facet_col_wrap=3,
    orientation='h',
    title='Top 5 Podcasts by Year (Hours Played)',
    labels={
        'stream_year': 'Year',
        'episode_show_name': 'Podcast',
        'total_hours_played': 'Hours Played',
        'category': 'Category'
    },
    hover_data=['category'],
    height=800
)

# Include layout tweaks for clarity
fig.update_layout(
    margin=dict(t=80, l=140, r=40, b=60),
    showlegend=True  # Turn on legend by category
)

fig.update_yaxes(matches=None, automargin=True)
fig.for_each_annotation(lambda a: a.update(text=a.text.split("=")[-1]))  # Clean facet titles

# Align title
fig.update_layout(title={'x': 0.185})

# Show the plot
fig.show()

```

Having `Comedy`, `Data`, `Sports` and  `Football` related podcasts as my top listening experiences seems true to form. What solidifies in my mind is the aversion to listening to people speak to me using the Spotify platform. It aligns with an overall trend of avoiding podcasts altogether. I do not listen to podcasts on Spotify, nor do I consume them on other platforms. What _might_ explain this is my tendency to view YouTube videos; this can be a de facto audio and visual experience, with creators segmenting their content for multiple platforms. If I am working and have the video playing in the background, I have a podcast-like experience with the option of visual content and human connection if I face the monitor. 

It is not the case that I no longer enjoy the topics at all, but that I have opted to consume them in an audio-visual format, eschewing the use of audio-only. This may also speak to my personal work-life habits. Many people consume podcasts as part of daily rituals in their workplace commute. Working from home, I am less inclined to consume in this way; I drive and I enjoy the "vibe" of background music more so than human voices.

### Total Podcast Listening Time

We already have the individual time spent listening to each individual podcast and are able to determine how much time was spent listening to podcasts each year. Using hours is much more user-friendly than milliseconds.

```{python}
# Total podcast listening time for our entire Spotify listening history.
logging.info(f"Total time listening to podcasts: {total_podcasts_per_year_df['total_hours_played'].sum().round(2)}")
```

```{python}
#| echo: false
# Podcast listening time for each year.
total_podcasts_per_year_df.groupby('stream_year')['total_hours_played'].sum()
```

```{python}
# Set up our dataframe. 
df = total_podcasts_per_year_df.groupby('stream_year')['total_hours_played'].sum().reset_index()

# Configure plot.
fig, ax = plt.subplots(figsize=(10, 5))

# Plot lines and dots.
ax.hlines(y=df['stream_year'], xmin=0, xmax=df['total_hours_played'], color='skyblue')
ax.plot(df['total_hours_played'], df['stream_year'], "o", color='steelblue')

# Add labels
ax.set_xlabel("Hours Played")
ax.set_ylabel("Year")
ax.set_title("Total Podcast Listening Time by Year")
ax.grid(axis='x', linestyle='--', alpha=0.5)


ax.set_xlim(0, df['total_hours_played'].max() + 1)  # Add buffer on right

# Annotate the hours.
for val, yr in zip(df['total_hours_played'], df['stream_year']):
    ax.text(val + 0.05, yr, f"{val:.2f} hrs", va='center', fontsize=10)

plt.tight_layout()
plt.show()
```

Over the entire span of listening history, we observe _less than a full day_ of podcast listening. I _really_ do not care for podcasts at all. The years of **2020** - **2021** may have been outliers due to the impending  World Cup of **2022**, but after **2024** any activity all but dissipates. This may render me a weak "lead" regarding any products or sponsors podcasters may promote. I may have an aversion to the human voice.

### Top Tracks For the Year

We deem "top tracks" those played the most frequently, regardless of their individual elapsed play time.

We start out with a test query to help formulate our strategy to identify "tracks" uniquely. We are not concerned if the same song title is shared among multiple albums. We will pick an artist I know of, but do not have on heavy rotation of [FKA Twigs](https://open.spotify.com/artist/6nB0iY1cjSY1KyhYyuIIKH?si=w4vL8rRZQHaNq1hE5CzY2A), as the total number of tracks should not be daunting.

**Query**

```{python}
fka_query = """
-- Test query for top tracks. 
SELECT COUNT(timestamp_column), artist_name, track_name, album_name, spotify_track_uri
FROM spotify_data
WHERE artist_name = 'FKA twigs'
AND ms_played > 5000
GROUP BY artist_name
    ,track_name
    ,album_name
    ,spotify_track_uri
"""
```

**Results**

```{python}
logging.info(fetch_query_results(fka_query))
```

An initial concern is that we have not encoded a method to uniquely identify "tracks." Thankfully there is a `spotify_track_uri` that does the job for us. From the above query, we can see that there may be tracks that are _effectively_ the same item, but have different albums and different `spotify_track_url` values.  

We observe multiple instances of the [holy terrain track](https://open.spotify.com/track/5ZmzzBiEAIVCkgsPyh80gt?si=64047a7432af4746), with this track present on `MAGDALENE` and several different `spotify_track_uri` entries. We will not count these as separate tracks, so we will use this query for an initial pass, then aggregate the play counts, removing album and `spotify_track_uri` in the final tally.

We create a test query with the same artist, but use a CTE then filter and aggregate to make the tracks "unique."

**Query**

```{python}
new_fka_query = """
WITH track_count_cte AS 
	(
		SELECT 
			COUNT(timestamp_column) AS track_count
			,artist_name
			,track_name
			,album_name
			,spotify_track_uri
			,DATE_PART('year', timestamp_column) AS stream_year
		FROM spotify_data
		WHERE 
			artist_name =  'FKA twigs'
			AND ms_played > 5000
		GROUP BY 
		artist_name
		,track_name
		,album_name
		,spotify_track_uri
		,stream_year
	)
SELECT 
	SUM(track_count) as times_played
	,artist_name
	,track_name
	,stream_year
FROM track_count_cte
GROUP BY 
	artist_name
	,track_name
	,stream_year
ORDER BY 
	times_played DESC
"""
```

**Results**

```{python}
logging.info(fetch_query_results(new_fka_query))
```

To satisfy my curiosity about overall track plays across the entire listening history, we modify the query removing the `artist_name` filter to see what songs had the highest overall play count. We ignore the _time elapsed_ playing a track, as we are interested in frequency, and limit this to 10 results. 

**Query**

```{python}
query = """
-- Find the most played tracks across entire listening history
WITH track_count_cte AS 
	(
		SELECT 
			COUNT(timestamp_column) AS track_count
			,artist_name
			,track_name
			,album_name
			,spotify_track_uri
			,DATE_PART('year', timestamp_column) AS stream_year
		FROM spotify_data
		WHERE 
			ms_played > 5000
		GROUP BY 
		artist_name
		,track_name
		,album_name
		,spotify_track_uri
		,stream_year
	)
SELECT 
	SUM(track_count) as times_played
	,artist_name
	,track_name
	,stream_year
FROM track_count_cte
GROUP BY 
	artist_name
	,track_name
	,stream_year
ORDER BY 
	times_played DESC
LIMIT 
	10
"""
```

**Results**

```{python}
# Query DB for top 10 tracks played per year.
top_10_tracks_played_df = fetch_query_results(query)

# View top 10 tracks played.
top_10_tracks_played_df.head()
```

```{python}
# Overall Top 10 tracks across entire listening history by play count
# Ensure dataframe contents descending order
top_10_tracks_played_df_sorted = (
    top_10_tracks_played_df
    .sort_values(by='times_played', ascending=False)
)

# Plot
fig = px.bar(
    top_10_tracks_played_df_sorted,
    x='times_played',
    y='track_name',
    orientation='h',
    title='Top 10 Most Played Tracks (All Time)<br><sup>All Are Unknown To Me<sup>',
    labels={
        'track_name': 'Track',
        'times_played': 'Number of Plays'
    },
    hover_data={
        'artist_name': True,
        'stream_year': True,
        'times_played': True,
        'track_name': False  # Already on y-axis
    },
    height=500
)

# Sort bars top to bottom
fig.update_layout(
    yaxis=dict(categoryorder='total ascending'),
    margin=dict(t=60, l=160, r=40, b=40)
)

# Align title
fig.update_layout(title={'x': 0.155,})

fig.show()
```

To my surprise, I only recognize a single artist [jackLNDN](https://open.spotify.com/artist/6ZPZXXnq3PbxZSR9vu9fso?si=MBQvvuWaQsa5QrzMikQvIA) among the ten most-played tracks in my history. I recognize _absolutely none_ of the names, nor the artists listed. Another interesting pattern is that six out of the ten tracks all appear in the year **2021**. I have a suspicion that the proximity to the global pandemic coupled with some observations about my listening habits may underlie this data. We modify the query to include the track genres.

**Query**

```{python}
query = """
WITH track_count_cte AS 
	(
		SELECT 
			COUNT(timestamp_column) AS track_count
			,artist_name
			,track_name
			,album_name
			,spotify_track_uri
			,DATE_PART('year', timestamp_column) AS stream_year
		FROM spotify_data
		WHERE 
			ms_played > 5000
            AND artist_name IS NOT NULL
		GROUP BY 
		artist_name
		,track_name
		,album_name
		,spotify_track_uri
		,stream_year
	)
SELECT 
	SUM(tce.track_count) AS times_played
	,tce.artist_name
	,tce.track_name
	,tce.stream_year
	,a.genres AS genres
FROM 
	track_count_cte AS tce
LEFT JOIN 
	artists AS a 
ON 
	tce.artist_name = a.artist_name	
GROUP BY 
	tce.artist_name
	,tce.track_name
	,tce.stream_year
	,a.genres
ORDER BY 
	times_played DESC
LIMIT 
	10
"""
```

**Results**

```{python}
# Query DB for top 10 tracks played per year with genre metadata.
top_10_tracks_played_w_genres_df = fetch_query_results(query)

# Peruse top 10 tracks coupled with genre data.
top_10_tracks_played_w_genres_df.head()
```

```{python}
# Overall Top 10 tracks across entire listening history by play count, with genres included.
# Create dataframe sorted by times played
top_10_tracks_played_w_genres_df_sorted = (
    top_10_tracks_played_w_genres_df
    .sort_values(by='times_played', ascending=False)
)

# Plot the data
fig = px.bar(
    top_10_tracks_played_w_genres_df_sorted,
    x='times_played',
    y='track_name',
    orientation='h',
    title='Top 10 Most Played Tracks (All Time) with Genres<br><sup>Still Unknown to Me',
    labels={
        'track_name': 'Track',
        'times_played': 'Number of Plays'
    },
    hover_data={
        'artist_name': True,
        'stream_year': True,
        'times_played': True,
        'genres': True, # Include genres in hover-over
        'track_name': False  # Already on y-axis
    },
    height=500
)

# Sort bars top to bottom
fig.update_layout(
    yaxis=dict(categoryorder='total ascending'),
    margin=dict(t=60, l=160, r=40, b=40)
)

# Align title
fig.update_layout(title={'x': 0.152,})

fig.show()
```

The ambiguous nature of the songs becomes a bit apparent with genres included. While most lack any genre, the [18 Carat Affair](https://open.spotify.com/track/65aQpAGncm88mRukQhJkIG?si=370bae7be9274a2b) entry leads me to believe that these were `ambient` tracks played to help me sleep or unwind; tracks of this nature would not have any recognizable vocals or artists I would associate with offhand.  [Chill Bees](https://open.spotify.com/artist/317OsNvmAmolBVu7dLWhhD?si=i9xWwgNuRZWBpyu4vc-ZGw), with their [Light Rain](https://open.spotify.com/track/3BA4y1ENVDOeT7XP82sk09?si=ba1a93507d574640) entry was result of me wanting some natural sounds in the loud apartment complex I previously resided in. California does not have much interesting weather, so having a gentle sense of a softened world, with leaves and moisture, elicited by rain sounds makes sense. Since I would request my home automation to "play rain" or "play ocean waves," I never took account of what artists and tracks were used.

To find top tracks partitioned by year, we use our previous foundation, rank the tracks based on play count, partition by the year, then filter the records down to tracks with a `ranking` below **10**. We also will slightly modify the output table, since we want to quantify the play count, then shift into greater details about the track, such as artist, track name, and genres.

**Query**

```{python}
query = """
-- Find top tracks for entire history and include genres, rank by times played, then filter
-- results down to top 10 by ranking column.
WITH track_count_cte AS 
	(
		SELECT 
			COUNT(timestamp_column) AS track_count
			,artist_name
			,track_name
			,album_name
			,spotify_track_uri
			,DATE_PART('year', timestamp_column) AS stream_year
		FROM spotify_data
		WHERE 
			ms_played > 5000
		GROUP BY 
		artist_name
		,track_name
		,album_name
		,spotify_track_uri
		,stream_year
	),
tracks_and_genres AS
	(
		SELECT
			tce.stream_year
			,SUM(tce.track_count) AS times_played
			,RANK() OVER
			(
				PARTITION BY tce.stream_year
				ORDER BY SUM(tce.track_count) DESC
			) AS ranking
			,tce.artist_name
			,tce.track_name
			,a.genres AS genres

		FROM 
			track_count_cte AS tce
		LEFT JOIN 
			artists AS a 
		ON 
			tce.artist_name = a.artist_name
		GROUP BY 
			tce.artist_name
			,tce.track_name
			,tce.stream_year
			,a.genres
		ORDER BY 
			tce.stream_year
			,RANK() OVER
			(
				PARTITION BY tce.stream_year
				ORDER BY SUM(tce.track_count) DESC
			)
	)
SELECT 
	* 
FROM 
	tracks_and_genres
WHERE
	ranking <= 10
"""
```

**Results**

```{python}
# Query DB for top 10 tracks played by year.
top_10_tracks_played_by_year_df = fetch_query_results(query)

# View top 10 tracks 
top_10_tracks_played_by_year_df.head()
```

```{python}
fig = px.bar(
    top_10_tracks_played_by_year_df,
    x='times_played',
    y='track_name',
    color='artist_name',
    facet_col='stream_year',
    facet_col_wrap=3,
    orientation='h',
    title='Top Tracks Played by Year',
    labels={
        'track_name': 'Track',
        'times_played': 'Play Count',
        'artist_name': 'Artist',
        'stream_year': 'Year'
    },
    hover_data={
        'artist_name': True,
        'times_played': True,
        'genres': True,
        'stream_year': False  # already in facet title
    },
    height=900
)

# Improve readability
fig.update_layout(
    margin=dict(t=80, l=120, r=40, b=60),
    showlegend=False
)

# Avoid automatic category linking.
fig.update_yaxes(matches=None, automargin=True)

# Clean up facet titles.
fig.for_each_annotation(lambda a: a.update(text=a.text.split("=")[-1]))


# Align title
fig.update_layout(title={'x': 0.23,})

fig.show()
```

Visually inspecting datadata, a familiar name in [Brian Eno](https://open.spotify.com/artist/7MSUfLeTdDEoZiJPDSBXgi?si=hpYRD84HSGO9De1pJWYgRA) tops the play count in **2017**, totaling **4** out of **11** of the most played tracks. I recognize no other artists in that year.

The years **2018** - **2023**, with the exceptions of Brian Eno and jackLNDN, I recognize absolutely _none_ of the artists, nor their tracks. Numerous tracks have absolutely no genres data present. Significant amounts of the tracks are dotted with terms indicating ambient nature sounds, such as "rain," "thunder," and "forest." This speaks to me barking out orders to my home automation to play natural sounds, to lull me into a pleasant slumber. I would never know, nor select any of the artists or tracks by any measure. I unintentionally and deliberately have been listening in ignorance.

**2024** is a more comforting and clear view, with my current set of preferred `phonk`, `lofi-*`, and `vaporware` genres. Among the top artists for **2024** I have name recognition of [Green Piccolo](https://open.spotify.com/artist/0Of8ndqAY23l2wV3sS6Zez?si=7GJLm-sTRber2niW9WzM1g), [Ryan Celsius Sounds](https://open.spotify.com/artist/2AtYJoC6VmUtkxonmVnbVR?si=Fj7AGZu-TMO9lnxeooHtBw), [Yvng Tavo](https://open.spotify.com/artist/0cNu3VVuE76EYfID6I9v1H?si=Sc90ywR-TD-NMvldKULElw), and [Baxdew](https://open.spotify.com/artist/2SQGn8IQmPyUTO6oJtejAl?si=DzPqhavGR_SDkvfBrPKNDg), with many of the songs I would play on continuous rotation, and consciously reference by name.  

### Amount of Plays for Top Tracks per Year

This data is implicit in the `top_10_tracks_played_by_year`, but we can get some general statistics from the `times_played` column.

```{python}
top_10_tracks_played_by_year_df['times_played'].describe()
```

The average amount of plays for top played tracks is ~**85.95**. We can filter this dataframe and see if there are any recognizable artists amount tricks played greater than the average.

```{python}
# We can reduce the working set to a smaller number of tracks to visualize.
top_tracks_per_year = (
    top_10_tracks_played_by_year_df
    .sort_values(['stream_year', 'times_played'], ascending=[True, False])
    .groupby('stream_year', group_keys=False)
    .head(5)
    .reset_index(drop=True)
)
```

```{python}
# Plot top_tracks_per_year in faceted fashion.

fig = px.bar(
    top_tracks_per_year,
    x='times_played',
    y='track_name',
    color='artist_name',
    facet_col='stream_year',
    facet_col_wrap=3,
    orientation='h',
    title='Top Tracks Per Year by Number of Plays',
    labels={
        'track_name': 'Track',
        'times_played': 'Play Count',
        'artist_name': 'Artist'
    },
    hover_data={
        'artist_name': True,
        'times_played': True,
        'genres': True
    },
    height=900
)

fig.update_layout(
    margin=dict(t=80, l=140, r=40, b=60),
    showlegend=False
)

fig.update_yaxes(matches=None, automargin=True)
fig.for_each_annotation(lambda a: a.update(text=a.text.split("=")[-1]))


# Align title
fig.update_layout(title={'x': 0.17})

fig.show()
```

Out of curiosity, I wanted to see if I recalled any artists wherein their times played exceeded the mean. 

```{python}
#| scrolled: true
# View tracks played equal to or more times than average.
top_10_tracks_played_by_year_df.query('times_played >= 85.95').head()
```

Again, the only recognizable artist is jackLNDN. Every other entry in the tracks exceeding the average have no artists or tracks I have any familiarity with. 

What is bothering me is that I have no _emotional_ resonance with the data. It is just text and numbers, I have no recollection of what the tracks meant to me experientially. Knowing that many of the "top tracks" are unrecognizable means these were mostly listened to in a manner similar to elevator music. They have function and no form in my mind, lacking emotion, and sterile when I survey them. The recognizable nature of more recent tracks, I can reason about, in terms of how my tastes have solidified consciously and with intent. Basically, knowing the "top tracks" for the bulk of this data, does actually _tell_ me something interesting, outside of their alien nature.

### First Date Top Tracks Were Played in a Year

As mentioned before, "Top Tracks" will be defined as tracks having the highest ranking based on play count within each year. We have a query providing these top tracks, but we need to use a for-loop like structure to search the `spotify_data` table, find instances of the track, then find the track with the oldest date. We create a temp table by using the same basic query, wrapping it inside of a `CREATE TABLE AS(..)` statement and altering the ranking filter for values of `1`. (This query can also be found inside of the `sql/queries.sql` file.)

‚ö†Ô∏è Temporary tables are owned by the `postgres` user, so these will need to be created and then _granted_ to the database user used in this notebook. Create these tables manually with `pgadmin4` or `psql` command prompt to then enable necessary permissions.

```{python}
#| lang: sql
CREATE TABLE temp_most_played_tracks AS (
	WITH track_count_cte AS 
		(
			SELECT 
				COUNT(timestamp_column) AS track_count
				,artist_name
				,track_name
				,album_name
				,spotify_track_uri
				,DATE_PART('year', timestamp_column) AS stream_year
			FROM spotify_data
			WHERE 
				ms_played > 5000
				AND artist_name IS NOT NULL
			GROUP BY 
			artist_name
			,track_name
			,album_name
			,spotify_track_uri
			,stream_year
		),
	tracks_and_genres AS
		(
			SELECT
				tce.stream_year
				,SUM(tce.track_count) AS times_played
				,RANK() OVER
				(
					PARTITION BY tce.stream_year
					ORDER BY SUM(tce.track_count) DESC
				) AS ranking
				,tce.artist_name
				,tce.track_name
				,a.genres AS genres
	
			FROM 
				track_count_cte AS tce
			LEFT JOIN 
				artists AS a 
			ON 
				tce.artist_name = a.artist_name
			GROUP BY 
				tce.artist_name
				,tce.track_name
				,tce.stream_year
				,a.genres
			ORDER BY 
				tce.stream_year
				,RANK() OVER
				(
					PARTITION BY tce.stream_year
					ORDER BY SUM(tce.track_count) DESC
				)
		)
	SELECT * 
	FROM tracks_and_genres
	WHERE ranking = 1
)
```

With the temp table `temp_most_played_tracks` created for our session, we can then use this to search the `spotify_data` table and extract the first date a track was played. We use `LATERAL` with `JOIN` for the for-loop invocation we desire. For each row generated by the query in front of the `LATERAL` join, a subquery or function after the `LATERAL` join will be evaluated once. 

For every entry in the temp table, we will execute the subquery, isolate the oldest record, then move on to the next entry.

**Query**

```{python}
query = """
-- Use the "for loop like" functionality of LATERAL with JOIN
SELECT 
	tmpt.artist_name
	,tmpt.track_name
	,tmpt.times_played
	,sd.timestamp_column
FROM
	temp_most_played_tracks AS tmpt
LEFT JOIN LATERAL (
	SELECT
		timestamp_column
		,DATE_PART('year', timestamp_column) AS sd_stream_year
	FROM
		spotify_data
	WHERE
		artist_name = tmpt.artist_name
		AND
		track_name = tmpt.track_name
		AND
		DATE_PART('year', timestamp_column) = tmpt.stream_year
	ORDER BY 
		timestamp_column ASC
	LIMIT 1
	) AS sd
ON TRUE
"""
```

**Results**

```{python}
# Query DB for when top 10 tracks were first played.
top_10_tracks_played_first_played = fetch_query_results(query)

# Peruse top 10 tracks and when they were played first.
top_10_tracks_played_first_played.head()
```

```{python}
# Ensure the timestamp is parsed properly.
first_played_df = top_10_tracks_played_first_played.copy()

# Add a dummy Y axis just to anchor points to the horizontal baseline.
first_played_df['y'] = 0

# Optionally create a label column
first_played_df['label'] = first_played_df['track_name'] + " ‚Äî " + first_played_df['artist_name']

# Alternate label positions: top for even rows, bottom for odd. Otherwise, the labels
# will run on to each other and be unreadable.
text_positions = ['top center' if i % 2 == 0 else 'bottom center' for i in range(len(first_played_df))]

fig = px.scatter(
    first_played_df,
    x='timestamp_column',
    y='y',
    size='times_played',
    text='label',
    hover_data={
        'timestamp_column': True,
        'track_name': True,
        'artist_name': True,
        'times_played': True,
        'y': False
    },
    title="Timeline of First Plays for Top Tracks",
    height=450
)

# Apply alternating text positions
fig.update_traces(
    marker=dict(color='mediumseagreen', opacity=0.7),
    textposition=text_positions
)

fig.update_layout(
    showlegend=False,
    yaxis=dict(showticklabels=False, title=None),
    xaxis_title='Date First Played',
    margin=dict(t=60, l=40, r=40, b=40)
)

# Align title
fig.update_layout(title={'x': 0.032})

fig.show()
```

**10** out of the **12** listening years, the tracks were listened to before the midpoint of that respective year. This may provide those tracks with a longer time period of opportunity to be played in that year.  Top tracks appearing closer to the end of a year, may indicate "binging" behavior, as there is literally less time to be played. 

Two tracks topped **2020** and happened to have the exact same play count are "[Med slutna √∂gon](https://open.spotify.com/track/6RuFOroO9VO0aMGEzirLHk?si=4199a10926b34cda)" and "[Et √∏yeblikk](https://open.spotify.com/track/4wwtlq6dqi0ztt989n4yiQ?si=b2939931d3c34a72)." The fact they have a proximate initial listening start date, have exactly the same amount of plays, would indicate a level of determinism. Using home-automation to trigger sleeping or nature sounds, the playlist appears to have deterministically played the _exact_ set of tracks, in the same sequence, over the same time period, such that these tracks ranked equally at the end of year.

To detect how much time elapsed between the time first played and end of the year, we create a function to read a timestamp and return this time delta. We can then apply this to our current dataframe, and do some interesting calculations.

```{python}
#| scrolled: true
# Modify the dataframe to have a `time_delta` column, delta from the time of 
# timestamp and the final day of its respective year.
top_10_tracks_played_first_played['time_delta'] =\
    top_10_tracks_played_first_played['timestamp_column'].apply(last_day_of_year)

# Peruse the dataframe after changes.
top_10_tracks_played_first_played.head()
```

```{python}
# Using the `time_delta` and `times_played, we can generate average amount of
# times per day a song was played.  
top_10_tracks_played_first_played['average_plays_per_day'] = \
    top_10_tracks_played_first_played['times_played'] / top_10_tracks_played_first_played['time_delta']

# Check our data.
top_10_tracks_played_first_played.head()
```

The years of **2014** and **2015** lack enough data to merit deep consideration. **2017** and my Brian Eno phase seems interesting, with me listening to this track every ~**3.84** days. This feels like a weekend habit, especially given the **275** day spread. The peculiar nature of the `Chill Bees` track makes me wonder about the nature of the playlist and its frequency. If played during sleeping hours, and more than twice a day on average, I suspect that I must have been using the _exact_ same home automation command to trigger the _exact_ same playlist. There also must be a directive to repeat the playlist, cycling through the same set of songs to enable this track to play multiple times in a single session. There may also be some peculiarity to the playlist composition. The track listing for the `Chill Bees` track may need to be short enough to enable the same songs to replay more than once in a session. Another possibility is that the track length may impact how quickly the songs are cycled. We could foresee a situation where there are minimal songs that play for longer periods before cycling back to the beginning, or a smaller number of tracks that have shorter play times to enable the full rotation of the entries. 

For the sleeps sounds era denoted in **2018** - **2023**, if these songs were the most played for that year and their initial play times were in the `evening` or `morning`  this could clearly signal to Spotify that I am a nocturnal person, or that I use these songs as background music instead of intentional play. 

The [18 Carat Affair](https://open.spotify.com/track/65aQpAGncm88mRukQhJkIG?si=246e6af8823d48da) track seems like a conscious decision on my part, given the time of day initially seen, and the truncated time-span at the tail-end of **2024**. The phrasing that comes to mind is "earworm."

### Total Distinct Tracks Played per Year

This task is pretty straightforward to express in SQL and we can use multiple columns within a `DISTINCT` clause. Searching for "unique tracks," we interpret this as the artist + track name + album name, without further filtering. We want to avoid situations where a track name and artist combo may actually have multiple `spotify_track_url` values, but still be _effectively_ the same track. We also _do_ want to count when tracks have the same title, but reside on different albums; these may be live version studio versions, remixes, mashups, and so on. These tracks are "unique" in that regard, despite sharing other metadata.

**Query**

```{python}
query = """
SELECT 
	DATE_PART('year', timestamp_column)::INT AS year
	,COUNT(DISTINCT (track_name, artist_name, album_name)) AS distinct_song_count
FROM 
	spotify_data
GROUP BY 
	year
ORDER BY
	year;
"""
```

**Results**

```{python}
# Query DB for distinct tracks played per year.
distinct_tracks_played_per_year = fetch_query_results(query)

# View distinct tracks played per year
distinct_tracks_played_per_year
```

```{python}
# Plot the barplot without error bars.
ax = sns.barplot(
    x="year", 
    y="distinct_song_count", 
    data=distinct_tracks_played_per_year, 
    errorbar=None,  # Disable error bars
)

# Set a bar label to show amounts.
ax.bar_label(ax.containers[0], fontsize=10);

# Add title and labels for clarity
plt.title("Looking for Something New in 2024", fontsize=9, x=0.43)
plt.suptitle("Distinct Song Counts Per Year", fontsize=10, fontweight='bold')

plt.xlabel("Year")
plt.ylabel("Count")
plt.tight_layout()
plt.show()
```

The distinct song count follows a steeper trajectory in unique tracks for every subsequent year. Compared to unique genres, the total amount of distinct tracks played, especially in **2024** is striking. The number of unique genres of artists I listen to has gradually increased, while the sheer amount of distinct tracks has risen at an extraordinary rate.  I am likely listening to more unique tracks within an only marginally increasing amount of genres. More unique tracks, implies more distinct artists. We can confirm that with a query to count the number of unique artists, not caring about any track info.

```{python}
query = """
-- Find unique artists per year
SELECT 
	DATE_PART('year', timestamp_column) AS year,
	COUNT(DISTINCT (artist_name)) AS distinct_artist_count
FROM 
	spotify_data
GROUP BY 
	year
ORDER BY
	year;
"""
```

```{python}
# Query DB for distinct artist count.
distinct_artists_by_year = fetch_query_results(query)

# View distinct artists by year.
distinct_artists_by_year
```

```{python}
# Plot the barplot without error bars.
ax = sns.barplot(
    x="year", 
    y="distinct_artist_count", 
    data=distinct_artists_by_year, 
    errorbar=None,  # Disable error bars
)

# Set a bar label to show amounts.
ax.bar_label(ax.containers[0], fontsize=10);

# Add title and labels for clarity
plt.title("Looking for Noobs on My Speakers", fontsize=9, x=0.43)
plt.suptitle("Distinct Artist Counts Per Year", fontsize=10, fontweight='bold')

plt.xlabel("Year")
plt.ylabel("Count")
plt.tight_layout()
plt.show()
```

The distinct artist count similarly rises with every subsequent year as did the distinct song count. I am hearing an increasing amount of novel artists every successive year, a tremendous amount of new songs, while not necessarily increasing the total diversity of genres within my Spotify listening history. 

### Total Time Listened on Spotify per Year

As milliseconds are not an intuitive measure, we will compute total listening time in hours. We only are concerned with tracks with over **5** seconds of time elapsed, to avoid counting situations of mistakenly played tracks or tracks skipped over.

**Query**

```{python}
query = """
-- Total time in hours listened on Spotify by year.
SELECT 
	DATE_PART('year', timestamp_column) AS year
	,SUM(ms_played)/3600000 AS hours_played
FROM spotify_data
WHERE
	ms_played > 5000
GROUP BY year
ORDER BY year ASC
"""
```

**Results**

```{python}
# Query DB for distinct artist count.
total_listening_time_by_year = fetch_query_results(query)

# Show total listening time by year
total_listening_time_by_year
```

```{python}
# Plot the barplot without error bars.
ax = sns.barplot(
    x="year", 
    y="hours_played", 
    data=total_listening_time_by_year, 
    errorbar=None,  # Disable error bars
)

# Set a bar label to show amounts.
ax.bar_label(ax.containers[0], fontsize=10);

# Add title and labels for clarity
plt.title("2020 Signals Routine Disruption", fontsize=9, x=0.43)
plt.suptitle("Total Hours Played Per Year", fontsize=10, fontweight='bold')

plt.xlabel("Year")
plt.ylabel("Hours Played")
plt.tight_layout()
plt.show()
```

**2020** stands out due to the COVID-19 pandemic and subsequent work from home directives disrupting my Spotify listening habits. It seem peculiar that I have a stable amount of stable amount of listening time after **2021**, but ended up increasing the amount of unique artists and tracks I was exposed to. One possibility is that the average amount of time of individual tracks may have decreased as a result of listening to less nature/sleep sounds and consciously listening to genres that may have shorter playing times, like `phonk` and `lofi`.

**Query**

```{python}
query = """
-- Find the average track length in minutes
SELECT 
	DATE_PART('year', timestamp_column) AS year
	,ROUND((AVG(ms_played)/60000),2) AS avg_mins_played
FROM
	spotify_data
WHERE
	ms_played > 5000
GROUP BY
	year
ORDER BY
	year ASC
"""
```

**Results**

```{python}
# Query for average track elapsed time per year
avg_track_elapsed_time_by_year_df = fetch_query_results(query)

# Display average track elapsed time by year
avg_track_elapsed_time_by_year_df
```

```{python}
# Plot the barplot without error bars.
ax = sns.barplot(
    x="year", 
    y="avg_mins_played", 
    data=avg_track_elapsed_time_by_year_df, 
    errorbar=None,  # Disable error bars
)

# Set a bar label to show amounts.
ax.bar_label(ax.containers[0], fontsize=10);

# Add title and labels for clarity
plt.title("We Do Not Talk about 2014", fontsize=9, x=0.43)
plt.suptitle("Average Minutes Played Per Year", fontsize=10, fontweight='bold')

plt.xlabel("Year")
plt.ylabel("Average Minutes Played")
plt.tight_layout()
plt.show()
```

The first year of our listening history has only a few rows of data, so its high magnitude is an outlier. It appears that the overall average time per track has reduced since **2019**. The reduction in average time by **13.6%** in **2024** as compared to **2023** may indicate more intention with my listening habits; I may skip forward, stop, change tracks consciously, versus more passive nature/sleep sound oriented behavior. The genres I choose may simply have shorter general play times for their works. One challenge for this consideration is that the `ms_played` column communicates how much time the track was played; it may not be indicative of the actual song length, start to finish. This makes it challenging to state that particular genres may have shorter play times generally, since the elapsed time may relate to how I use Spotify to listen. 

Since we have dozens of genres, we create a temp table called `temp_genre_track_avg_playtime` in case we need to query for the average play time for songs in a respective genre, for a particular year. (This query can also be found inside of the `sql/queries.sql` file.)

```{python}
#| lang: sql
-- Generate temp_genre_track_avg_playtime
CREATE TABLE temp_genre_track_avg_playtime AS (
	WITH artist_playtime AS (
		SELECT a.artist_name
			,a.genres
			,sd.ms_played
			,DATE_PART('year', sd.timestamp_column) AS stream_year
		FROM artists AS a
		JOIN sd_artists_join AS sdj 
			ON a.id = sdj.artist_id
		JOIN spotify_data AS sd 
			ON sdj.sd_id = sd.id
	),
	genre_avg AS (
		SELECT
			stream_year
			,UNNEST(genres) AS genre
			,ROUND((CAST(AVG(ms_played) AS numeric)  / 60000) , 2) AS avg_mins_played
		FROM artist_playtime
		GROUP BY stream_year, genre
	)
	SELECT *
	FROM 
		genre_avg
	ORDER BY 
		stream_year
		,avg_mins_played DESC
)
```

The `temp_genre_track_avg_playtime` temp table has **3140** entries. Let's see what genres had the highest average minutes for **2020**.

**Query**

```{python}
query = """
--Sample data from temp table
SELECT 
	*
FROM  
	temp_genre_track_avg_playtime
WHERE
	avg_mins_played > 4
	AND stream_year = '2020'
ORDER BY
	stream_year
	,avg_mins_played DESC
LIMIT 10
"""
```

**Results**

```{python}
# Query DB for validating temp table.
temp_table_df = fetch_query_results(query)

# View contents of temp_table_df
temp_table_df.head()
```

While this output is simply representing the average times we associate with a genre, they seem absolutely out of left field. When have I _ever_ played a sea shanty? Why is a `cha cha cha` song even in my listening history? Why are `salsa` and `cha cha cha` genres averaging **36.04** minutes? We can glance at the `artists` table and see a bit more about this.

```{python}
#| '0': l
#| '1': a
#| '2': 'n'
#| '3': g
#| '4': ':'
#| '5': s
#| '6': q
#| '7': l
-- Search the array column `genres` to find artists associated with `cha cha cha`
SELECT * FROM artists 
WHERE 'cha cha cha' = ANY(genres)
```

```{python}
#| lang: sql
-- Results from searching for `cha cha cha`
8075	"Joe Baker"	"{""cha cha cha"",salsa}"	"2025-06-11 12:57:41.964242-07"
1648	"Band Ricos"	"{""cha cha cha""}"	"2025-06-11 13:22:30.321776-07"
```

Only two artist results, one of them being [Joe Baker](https://open.spotify.com/album/26jfzSuXL8FWoTMpUyc8Od?si=jz4GEjctS0-6_sxxGNQqvg). I recognize Joe Baker, as this was the artist mentioned by my home automation software when I would request it to play "ocean waves" at night for natural sounds. We can survey how much time their associated tracks were played.  

**Query**

```{python}
query = """
-- Joe Baker, my "ocean waves" artist, classified as 'cha cha cha', focusing on 2020.
SELECT 
	artist_name
	,track_name
	,ms_played/6000 as min_played
FROM 
	spotify_data
WHERE 
	artist_name = 'Joe Baker'
	AND DATE_PART('year', timestamp_column) = 2020
"""
```

**Results**

```{python}
# Query DB for distinct artist count.
joe_baker_2020_df = fetch_query_results(query)

# View Joe Baker data.
joe_baker_2020_df.head()
```

```{python}
# Our manual calculations align with the SQL query data for ~ 35.7 min average playtime
joe_baker_2020_df['min_played'].describe()
```

It was _precisely_ this scenario I was afraid of related to the design decision to attach genres to artists instead of tracks/songs. I would initially think I may have had a one-off instance of playing a song in the genre on some mystical night in **2020**, only to find it conflicts with my lived experience of dozing off to `Joe Baker`. The ambiguity of associating genres with artists means we should compare findings in the data with our actual, lived, experience of Spotify listening.

### Day with the Most Time Spent Listening

This question of what day each year had the most listening time can be framed in two ways:
* The calendar day that had the most elapsed listening time initiated in that 24-hour period, out of all days in a calendar year.
* The specific weekday (Sunday through Saturday) when we listened to Spotify, compared to other weekdays.

#### Calendar Day

**Query**

```{python}
query = """
WITH daily_playtime AS (
  SELECT
    DATE_TRUNC('day', timestamp_column) AS calendar_day,
    DATE_PART('year', timestamp_column) AS year,
    SUM(ms_played) / 3600000.0 AS hours_played
  FROM spotify_data
  GROUP BY 
	  year
	  ,calendar_day
)
SELECT *
FROM (
  SELECT 
    *
    ,RANK() OVER (PARTITION BY year ORDER BY hours_played DESC) AS rank
  FROM daily_playtime
) ranked
WHERE rank = 1
ORDER BY year;
"""
```

**Result**

```{python}
# Query DB for days with highest listening.
days_of_highest_listening_df = fetch_query_results(query)

# View days of highest listening activity.
days_of_highest_listening_df.head()
```

```{python}
query = """
WITH daily_playtime AS (
  SELECT
    DATE_TRUNC('day', timestamp_column) AS calendar_day
    ,DATE_PART('year', timestamp_column) AS year
    ,SUM(ms_played) / 3600000.0 AS hours_played
  FROM spotify_data
  GROUP BY 
    year
    ,calendar_day
),
ranked AS (
  SELECT 
    *
    ,RANK() OVER (PARTITION BY year ORDER BY hours_played DESC) AS rank
  FROM daily_playtime
)
SELECT *
FROM ranked
WHERE rank <= 5
ORDER BY 
    year
    ,rank;
"""
```

```{python}
days_of_highest_listening_df = fetch_query_results(query)
```

```{python}
#| scrolled: true
# Filter to only top (rank 1) day per year. Avoids duplicative labeling.
top_days_df = days_of_highest_listening_df[days_of_highest_listening_df['rank'] == 1].copy()

# Setup the figure and bars.
fig, ax = plt.subplots(figsize=(8, 4))
bars = ax.barh(
    top_days_df['year'].astype(str), 
    top_days_df['hours_played'], 
    color='steelblue'
)

# Add one label per bar using bar_label.
ax.bar_label(bars, labels=[f'{v:.2f} hrs' for v in top_days_df['hours_played']], 
             label_type='edge', padding=3, fontsize=9, color='black')


ax.set_xlim(0, top_days_df['hours_played'].max() + 4)  # Add some padding

# Add graph elements.
ax.set_xlabel('Hours Played')
ax.set_ylabel('Year')
ax.set_title('Top Spotify Listening Day by Year')
ax.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()
```

Knowing the day with the maximum amount of playtime seems important, but needs to be seated in context. We now will plot a continuous-like line across all days of the listening history, to see how many hours were played daily across all of the days.

**Query**

```{python}
query = """
SELECT
    DATE_TRUNC('day', timestamp_column)::date AS calendar_day
    ,ROUND(SUM(ms_played) / 3600000.0, 2) AS hours_played
FROM spotify_data
--WHERE DATE_PART('year', timestamp_column) = 2021
GROUP BY calendar_day
ORDER BY calendar_day;
"""
```

**Results**

```{python}
hours_played_per_day_df = fetch_query_results(query)

hours_played_per_day_df.head()
```

We reindex our data in preparation for plotting our daily listening time.

```{python}
# Ensure datetime and year
df = hours_played_per_day_df.copy()
df['calendar_day'] = pd.to_datetime(df['calendar_day'])
df['year'] = df['calendar_day'].dt.year
df['month'] = df['calendar_day'].dt.month

# Create a full date range from min to max
full_range = pd.date_range(start=df['calendar_day'].min(), end=df['calendar_day'].max())

# Reindex with full calendar and fill missing days with 0
df_full = df.set_index('calendar_day').reindex(full_range).rename_axis('calendar_day').reset_index()
df_full['hours_played'] = df_full['hours_played'].fillna(0)

# Recompute year column after reindexing
df_full['year'] = df_full['calendar_day'].dt.year
```

We plot every day point of our listening history, coloring each year. Heightened listening periods can be subtly inferred from seeing "airspace" between the x-axis and the plotting line. 

```{python}
plt.figure(figsize=(16, 6))

# Create a color palette with unique colors per year
palette = sns.color_palette('tab20', n_colors=df_full['year'].nunique())

# Plot each year with a distinct color
for i, (year, group) in enumerate(df_full.groupby('year')):
    plt.plot(group['calendar_day'], group['hours_played'], label=str(year), color=palette[i], linewidth=1)

plt.title("Spotify Listening Time Per Day (Colored by Year)")
plt.xlabel("Date")
plt.ylabel("Hours Played")
plt.grid(True, linestyle='--', alpha=0.5)
plt.legend(title="Year", bbox_to_anchor=(1.01, 1), loc='upper left')
plt.tight_layout()
plt.show()
```

Since the daily line plot _can_ be read but is not exactly clear, we can roll up our aggregation to the monthly level for ease of viewing. 

```{python}
# base dataframe with 'calendar_day' and 'hours_played'
df = df_full.copy()
df['month'] = df['calendar_day'].dt.month
df['year_month'] = df['calendar_day'].dt.to_period('M').dt.to_timestamp()

# Aggregate by month.
monthly_df = df.groupby('year_month', as_index=False)['hours_played'].sum()
monthly_df['month'] = monthly_df['year_month'].dt.month  # for color

# Plot.
plt.figure(figsize=(14, 6))
norm = plt.Normalize(1, 12)
sm = plt.cm.ScalarMappable(cmap='tab20', norm=norm)

for i in range(1, len(monthly_df)):
    plt.plot(
        [monthly_df['year_month'][i-1], monthly_df['year_month'][i]],
        [monthly_df['hours_played'][i-1], monthly_df['hours_played'][i]],
        color=plt.cm.tab20(norm(monthly_df['month'][i])),
        linewidth=2
    )

plt.title('Spotify Listening Time Per Month (Chronological, Colored by Month)')
plt.ylabel('Hours Played')
plt.xlabel('Years')
plt.xticks(rotation=45)
plt.grid(alpha=0.3)

# Set major ticks to every year
years = df_full['calendar_day'].dt.year.unique()
year_starts = pd.to_datetime([f"{y}-01-01" for y in years])

plt.xticks(
    ticks=year_starts,
    labels=[str(y) for y in years],
    rotation=45
)


# Legend
# Replace numeric month labels with 3-letter month abbreviations
month_labels = [calendar.month_abbr[i] for i in range(1, 13)]

legend_handles = [
    mpatches.Patch(color=plt.cm.tab20(norm(i)), label=month_labels[i-1])
    for i in range(1, 13)
]

plt.legend(
    handles=legend_handles,
    title='Month',
    bbox_to_anchor=(1.01, 1),
    loc='upper left'
)
plt.tight_layout()
plt.show()
```

The summer of **2019** has the highest amount of hours played. Since our previous data revealed listening times running over three hours over 24 hours in a day, I suspect this must have been a long-running track that started before midnight and did not end until several hours afterward. A listening session that may have been initiated for sleep/nature sounds where I neglected to end my listening would align with this, in terms of running time as well as a track of sufficient individual length being able to run over. We can query for specific days and inspect what tracks were played on the day.

```{python}
#| lang: sql
-- Search for a specific date.
SELECT
	timestamp_column
	,platform
	,ms_played
	,track_name
	,artist_name
	,spotify_track_uri
FROM
	spotify_data
WHERE
	DATE_TRUNC('day', timestamp_column) = '2019-06-30'::timestamp;
```

If we are interested in knowing what specific tracks were played on each of the days with the longest listening times for each year, we can create a temp table to capture that. (This query can also be found inside of the `sql/queries.sql` file.)

```{python}
#| lang: sql
CREATE TABLE temp_longest_listening_days AS (
	WITH daily_playtime AS (
		SELECT
			DATE_TRUNC('day', timestamp_column) AS calendar_day
			,DATE_PART('year', timestamp_column) AS year
			,ROUND(SUM(ms_played) / 3600000.0, 2) AS hours_played
		FROM spotify_data
		GROUP BY 
			year
			,calendar_day
	)
	,highest_days AS (
		SELECT
			calendar_day
			,year
		FROM (
			SELECT 
				calendar_day
				,year
				,hours_played
				,RANK() OVER (PARTITION BY year ORDER BY hours_played DESC) AS rank
			FROM daily_playtime
		) ranked
		WHERE rank = 1
	)
	SELECT
		hd.calendar_day
		,sd.timestamp_column
		,sd.platform
		,sd.ip_addr
		,sd.ms_played
		,sd.track_name
		,sd.artist_name
		,sd.spotify_track_uri
	FROM highest_days AS hd
	JOIN spotify_data AS sd
		ON DATE_TRUNC('day', sd.timestamp_column) = hd.calendar_day
	ORDER BY 
		hd.calendar_day
		,sd.timestamp_column DESC
);
```

Armed with a temp table, we can suss out what genres composed each of the top listening days by genre count. Days where nature/sleep sounds or ambient genres are observed can be assumed to have been driven by home automation activation and left running without intervention. Days with more "danceable" or vocal-based genres imply instances of deliberative control. We will only filter to the top 10 genres ranked by count.

**Query**

```{python}
query = """
-- Generating genre counts for days with 
-- highest listening time.
WITH tracks_and_genres AS (
	SELECT 
		tlda.timestamp_column
		,UNNEST(a.genres) AS genre
	FROM 
		temp_longest_listening_days AS tlda
	LEFT JOIN artists AS a
		ON tlda.artist_name = a.artist_name
),
ranking_table AS (
	SELECT
		DATE_PART('year', timestamp_column) AS year
		,genre
		,COUNT(genre) AS genre_count
		,RANK() OVER (PARTITION BY DATE_PART('year', timestamp_column) ORDER BY COUNT(genre) DESC) AS ranking
	FROM 
		tracks_and_genres
	GROUP BY
		genre
		,year
	ORDER BY
		year ASC
		,ranking
		,genre_count DESC
)
SELECT *
FROM ranking_table
WHERE ranking <= 10;
"""
```

**Results**

```{python}
# Query DB for top 10 genres on days with highest listening.
top_10_genres_most_listened_days_df = fetch_query_results(query)

# View the top 10 genres on the most listened days.
top_10_genres_most_listened_days_df.head()
```

**2019** is what I expected it to be, full of `ambient`, `drone` and `minimalism` tracks, which would match with my perception of this being a sleep session left to run.

```{python}
# 2019 looks like a Brian Eno session that never ends.
top_10_genres_most_listened_days_df.query('year == 2019')
```

**2024** looks to be a binge session I had for French artist [Lala &ace](https://open.spotify.com/artist/1AKP8Tnz8KfOdRM4mqvNtF?si=hwb5ADPySGycq_F65Qrrtw), a purveyor of [cloud rap](https://en.wikipedia.org/wiki/Cloud_rap).

```{python}
# 2024 looks like the Lala &ce binge-listening session.
top_10_genres_most_listened_days_df.query('year == 2024')
```

We have our answer for what days have the longest play time as well as how long each day ran in terms of hours. 

#### Day of the Week

Discovering what weekday had the most tracks played may be straightforward. We need only process the `timestamp_column`, extracting the weekday, and then group our results. We will find out which days have the most tracks played as well as what days had the longest cumulative amount of listening time.

**Query**

```{python}
query = """
-- Find the cumulative amount of track and hours played
-- segmented by weekday. 
SELECT
	TO_CHAR(timestamp_column, 'Day') as day_of_week
	,EXTRACT(DOW FROM timestamp_column) as numeric_day
	,ROUND((SUM(ms_played) / 3600000.0), 2) AS hours_played
	,COUNT(*) as track_count
FROM 
	spotify_data
GROUP BY
	day_of_week
	,numeric_day
ORDER BY
	numeric_day
	,hours_played DESC
"""
```

**Result**

```{python}
# Query DB to see listening activity broken down by weekday.
weekdays_of_listening_df = fetch_query_results(query)

# Peruse the days of highest listening.
weekdays_of_listening_df
```

```{python}
# Plot the barplot without error bars.
ax = sns.barplot(
    x="day_of_week", 
    y="track_count", 
    data=weekdays_of_listening_df, 
    errorbar=None,  # Disable error bars
)

# Set a bar label to show amounts.
ax.bar_label(ax.containers[0], fontsize=10);

# Add title and labels for clarity
plt.title("Sunday Cleaning Means Heavy Rotation", fontsize=9, x=0.43)
plt.suptitle("Cumulative Tracks Played Per Weekday", fontsize=10, fontweight='bold')

plt.xlabel("")
plt.ylabel("Tracks")
plt.tight_layout()
plt.show()
```

```{python}
# Plot the barplot without error bars.
ax = sns.barplot(
    x="day_of_week", 
    y="hours_played", 
    data=weekdays_of_listening_df, 
    errorbar=None,  # Disable error bars
)

# Set a bar label to show amounts.
ax.bar_label(ax.containers[0], fontsize=10);

# Add title and labels for clarity
plt.title("Sunday Cleaning Means Heavy Rotation", fontsize=9, x=0.43)
plt.suptitle("Cumulative Hours Played Per Weekday", fontsize=10, fontweight='bold')

plt.xlabel("")
plt.ylabel("Hours")
plt.tight_layout()
plt.show()
```

* **Sunday** having the highest track count and hours played across our entire listening history makes sense. A day off, unencumbered by work responsibilities combined with my habit of weekly cleaning would enable more opportunity to listen to more tracks.
* **Monday** and **Wednesday** having lower amounts of hours played and track counts can be reasoned by those days usually being full of meetings, distractions, and work-related activities may permit less opportunity to play Spotify, at home or work. "Hump day" is real.
* **Thursday** having a higher number of tracks and playtime  is likely due to **Friday**. As in, no meetings, the periodic day off, and expectations to relax on the weekend.
* **Saturday** may have less tracks, due to weekend activities, being away from work and from home. 

## ‚åö Temporal Listening Patterns

In this section, we will surface our listening history at varying levels of granularity. First, we start with plotting listening activity across every day of our listening history. Then, we aggregate monthly, to get a general sense how listening activity could fluctuate month-to-month. Then, we roll up to seasons and then drill downward to times of day.

### üåû Hours of Listening 

Knowing the day with the maximum amount of playtime seems important, but needs to be seated in context. We now will plot a continuous-like line across all days of the listening history, to see how many individual hours were played daily across all of the days.

**Query**

```{python}
# Extract the number of hours played for *every* day. We can then aggregate by transforming
# the calendar_day value as needed.
query = """
SELECT
    DATE_TRUNC('day', timestamp_column)::date AS calendar_day
    ,ROUND(SUM(ms_played) / 3600000.0, 2) AS hours_played
FROM spotify_data
--WHERE year_played > 2016
GROUP BY calendar_day
ORDER BY calendar_day;
"""
```

**Results**

```{python}
hours_played_per_day_df = fetch_query_results(query)
hours_played_per_day_df.head()
```

To ensure a smoothtime series plot and avoid breaks in the visualizations, we must ensure our Spotify listening history has a row for every single day, even if no music was played. 

- We extract the `year` and `month` to help group and analyze data later.
- We generate a **complete date range** from the earliest to latest listening date.
- Reindex the dataframe using the full calendar, inserting 0 into missing days.
- Recalculate the `year` column so it aligns with the newly created full date range.

```{python}
# Ensure datetime and year
df = hours_played_per_day_df.copy()
df['calendar_day'] = pd.to_datetime(df['calendar_day'])
df['year'] = df['calendar_day'].dt.year
df['month'] = df['calendar_day'].dt.month

# Create a full date range from min to max
full_range = pd.date_range(start=df['calendar_day'].min(), end=df['calendar_day'].max())

# Reindex with full calendar and fill missing days with 0
df_full = df.set_index('calendar_day').reindex(full_range).rename_axis('calendar_day').reset_index()
df_full['hours_played'] = df_full['hours_played'].fillna(0)

# Recompute year column after reindexing
df_full['year'] = df_full['calendar_day'].dt.year
```

```{python}
df_full.head()
```

```{python}
# Ensure calendar_day is in datetime format.
df_full['calendar_day'] = pd.to_datetime(df_full['calendar_day'])

fig = px.line(
    df_full,
    x='calendar_day',
    y='hours_played',
    color='year',
    labels={
        'hours_played': 'Hours Played'
    },
    title="Hours of Spotify Listening Time Per Day"
)

# Simplify customdata and custom hovertemplate.
fig.update_traces(
    customdata=df_full[['calendar_day']],
    hovertemplate=(
        'Hours Played: %{y:.2f}<br>'
    )
)

# Improve layout and legend.
fig.update_layout(
    height=600,
    xaxis_title="Date",
    yaxis_title="Hours Played",
    legend_title_text='Year',
    hovermode='x unified',
    title={'x': 0.5}
)

# Fix yearly tick issues
years = df['calendar_day'].dt.year.unique()
tickvals = [f"{year}-01-01" for year in sorted(years)]

fig.update_layout(
    xaxis=dict(
        tickmode='array',
        tickvals=tickvals,
        tickformat='%Y',
        ticklabelmode='period'  # Optional: show the full year label
    )
)

fig.show()
```

The lack of any listening activity from 2014 until 2017 is stark. Afterwards, Spotify streaming is a consistent portion of my daily lifestyle. Seeing "air" beneath the lines would indicate heightened listening activity, consistently, over a period of days.

### ‚è±Ô∏è Time of Day

We define the "times of day" as follows:

* **Morning**: 5:00 AM to Noon (7 hours)
* **Afternoon**: Noon to 6:00 PM (6 hours)
* **Evening**: 6:00 PM to 11:00 PM (5 hours)
* **Night**: 11:00PM to 5:00 AM (6 hours)

**Query**

```{python}
query = """
	SELECT 
		year_played
        ,time_of_day
        ,COUNT(*) as track_count
        ,ROUND((SUM(ms_played) / 3600000.0), 2) AS hours_played
        ,CASE
			WHEN time_of_day = 'morning' THEN 0
			WHEN time_of_day = 'afternoon' THEN 1
			WHEN time_of_day = 'evening' THEN 2
			WHEN time_of_day = 'night' THEN 3
            ELSE 4
        END AS tod_order
    FROM spotify_data
    WHERE
        ms_played > 5000
    GROUP BY
        year_played
        ,time_of_day
    ORDER BY
        year_played ASC
        ,tod_order ASC
        ,track_count
    """
```

**Results**

```{python}
time_of_day_df = fetch_query_results(query)
time_of_day_df.head()
```

```{python}
time_of_day_df[time_of_day_df['time_of_day'] == 'morning']
```

```{python}
#| echo: false
# One-off calculations for percentages
# 100 - ((244.71/846.39) * 100)
# 100 - ((4387/12016) * 100)

```

```{python}
# Define the correct time of day order
tod_order = [
    'morning', 'afternoon', 'evening', 'night'
]

# Set the 'time_of_day' column as a categorical for sorting in the plot
time_of_day_df['time_of_day'] = pd.Categorical(
    time_of_day_df['time_of_day'],
    categories=tod_order,
    ordered=True
)

# Sort the dataFrame 
time_of_day_df = time_of_day_df.sort_values(['year_played', 'time_of_day'])

# Plot the data
plt.figure(figsize=(16, 6))
sns.barplot(
    data=time_of_day_df,
    x='year_played',
    y='track_count',
    hue='time_of_day',
    hue_order=tod_order
)
plt.title("Track Count by Time of Day and Year")
plt.ylabel("Track Count")
plt.xticks(rotation=0)
plt.legend(title="Time of Day", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()
```

```{python}
# Define the correct time of day order
tod_order = [
    'morning', 'afternoon', 'evening', 'night'
]

# Set the 'time_of_day' column as a categorical for sorting in the plot
time_of_day_df['time_of_day'] = pd.Categorical(
    time_of_day_df['time_of_day'],
    categories=tod_order,
    ordered=True
)

# Sort the dataFrame 
time_of_day_df = time_of_day_df.sort_values(['year_played', 'time_of_day'])

# Plot the data
plt.figure(figsize=(16, 6))
sns.barplot(
    data=time_of_day_df,
    x='year_played',
    y='hours_played',
    hue='time_of_day',
    hue_order=tod_order
)
plt.title("Play Time by Time of Day and Year")
plt.ylabel("Hours Played")
plt.xticks(rotation=0)
plt.legend(title="Time of Day", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()
```

```{python}
#| echo: false
# One off calculations for average morning percentage
# Group by year and time of day to sum hours
tod_summary = time_of_day_df.groupby(['year_played', 'time_of_day'], as_index=False)['hours_played'].sum()

# Pivot to reshape into one row per year
tod_pivot = tod_summary.pivot(index='year_played', columns='time_of_day', values='hours_played').fillna(0)

# Calculate total hours per year and morning percentage
tod_pivot['total_hours'] = tod_pivot.sum(axis=1)
tod_pivot['morning_percent'] = (tod_pivot.get('morning', 0) / tod_pivot['total_hours']) * 100

# Get overall average morning percentage
average_morning_percent = tod_pivot['morning_percent'].mean()
average_morning_percent
```

Every single year, with the exception of **2020** and the incomplete **2025** had `morning` as the time of day with the highest count of tracks played and hours played. The global pandemic of **2020** and its impact on my working and personal life _reduced_ my listening activity in the `morning` segment of the day instead of increasing streaming activity with WFH. This may be related to generalized acclimation to working from home, overzealous utilizing and focus on teleconferencing as a replacement for in-person meetings. Listening patterns corrected in **2021**, but it _feels_ like there is a literal missed year where the hours played _should_ have been between **850** and **1000**.

Another strongly generalizable pattern over each year, track count or listening time, is defined by:

* Peak listening is always in `morning`.
* Significant, but lesser, listening in the `afternoon`.
* The lowest level of listening in the `evening`.
* Resurgent listening, outpacing `afternoon` and `evening` at `night`.

Comparisons between different times of day are not exactly "fair," given the time boundaries between them are not exactly equivalent. Factors impacting my personal streaming activity include:

* My nocturnal nature
* The music selections
* Silicon Valley workstyles


If I sleep less and later than most people conventionally, this will limit potential Spotify usage. If the genres in play at that phase of day are atmospheric/sleep-sounds, they tend to have longer play times, reducing track count while extending play times. One of my employers had a "core hours" practice where in-office attendance usually meant you arrived by 10:00 AM, to adjust for Bay Area traffic and commuting travel times. This would enable ample time for any music I would normally play to occupy the `morning` zone. 

`Afternoon` may have high contributions due to less meetings than `morning`, Spotify would act as a coding session productivity booster, enabling focus and background noise to drown out office distractions. I admit to expecting larger amounts of `evening` time, assuming there would be more time and freedom to play music after work. This time period may have been taken up by exercise, cooking, other outside activities, reducing use of the Spotify platform.

Inspecting **2020** afternoons in greater detail, I was curious as to _what_ exactly I was playing during those periods. I ran a filtered version of the "Top Genres of the Year" query to get a list of genres, then ran the following query to get a list of artists in that year.

**Query**

```{python}
query = """
-- Get list of artists in 2020 played in the afternoon matching 
-- top genres discovered earlier.
WITH artist_playtime AS (
	SELECT 
		a.artist_name AS artist_name
		,a.genres
		,sd.ms_played
		,year_played
	FROM artists AS a
	JOIN sd_artists_join AS sdj 
		ON a.id = sdj.artist_id
	JOIN spotify_data AS sd 
		ON sdj.sd_id = sd.id
	WHERE time_of_day IN ('afternoon')
	AND year_played = 2020
),
genre_ranks AS (
	SELECT
		artist_name
		,year_played
		,UNNEST(genres) AS genre
		,ROUND((CAST(SUM(ms_played) AS numeric)  / 3600000) , 2) AS hours_played
		,RANK() OVER (
			PARTITION BY year_played
			ORDER BY SUM(ms_played) DESC
		) AS genre_rank
	FROM artist_playtime
	GROUP BY artist_name, year_played, genre
)
SELECT DISTINCT 
	artist_name
	,SUM(hours_played) AS hours_played
FROM genre_ranks
WHERE genre_rank <= 20
AND genre IN ('dancehall', 'dancehall','pop urbaine', 'afrobeats', 'afro house', 'french rap',
'afrobeat',	'afropop soca', 'afroswing', 'french r&b', 'riddim' ,'alternative r&b', 'afro soul', 'gqom', 'amapiano'
)
GROUP BY artist_name
ORDER BY 
	hours_played DESC
	,artist_name
"""
```

**Result**

```{python}
interesting_2020_afternoon_df = fetch_query_results(query)
interesting_2020_afternoon_df
```

The `dancehall` prominence is quickly explained by [Dexta Daps](https://open.spotify.com/artist/28UDeKu2FPrU0T7dpUiSGY?si=Vr5J35rgSEG7E0EUavMKgA) and [Burna Boy](https://open.spotify.com/artist/3wcj11K77LjEY1PkEazffa?si=siPwYQrrTKSZ81gZU3cW-Q). The other artists took me a bit of time to discern. [MHD](https://open.spotify.com/artist/4WnAHZz1pgl8hus8hidIRV?si=T1WmFKYtSlSna_IpiBOZZg), [Damso](https://open.spotify.com/artist/2UwqpfQtNuhBwviIC0f2ie?si=kNgnPFOFSDCPD2zyKhAMTg), and [Mister V](https://open.spotify.com/artist/5qisLjDrKoHMI9zOK2hfzs?si=PPvtncuBTna7cF5bCzoXFQ). So, I was likely binge-listening to the `pop urbaine`, `french rap`, and `french r&b` genres _heavily_ fixated on these three artists. I do **not** speak any French in the slightest, I just like how it feels in my ear.

* [Fuego](https://open.spotify.com/track/2BbvRknyKBO1A2wp8nFOtZ?si=d2fe676391a54b79)
* [Apollo13](https://open.spotify.com/track/0P8n6AIBVc88gxbliu0lej?si=37c5f299a2f64311)
* [K.Kin la belle](https://open.spotify.com/track/6YuSiFqJP4MUo88RtUAfAX?si=54f20f0bbcf740fc)


### üà∑Ô∏è Months Of Listening

We now  aggregate hours played on a monthly level.

```{python}
# Use existing dataFrame
df = df_full.copy()
df['year_month'] = df['calendar_day'].dt.to_period('M').dt.to_timestamp()
df = df.groupby('year_month', as_index=False)['hours_played'].sum()
df['month'] = df['year_month'].dt.month
df['month_abbr'] = df['year_month'].dt.strftime('%b')

# Create segmented traces manually.
fig = go.Figure()
norm = plt.Normalize(1, 12)
cmap = plt.colormaps.get_cmap('tab20').resampled(12)

for i in range(1, len(df)):
    x_vals = [df['year_month'][i-1], df['year_month'][i]]
    y_vals = [df['hours_played'][i-1], df['hours_played'][i]]
    month_val = df['month'][i]

    color_rgb = cmap(norm(month_val))  # RGBA
    color_hex = 'rgba({},{},{},{})'.format(
        int(color_rgb[0]*255), int(color_rgb[1]*255), int(color_rgb[2]*255), color_rgb[3]
    )

    fig.add_trace(go.Scatter(
        x=x_vals,
        y=y_vals,
        mode='lines',
        line=dict(color=color_hex, width=2),
        #hovertemplate=f"<b>{df['year_month'][i].strftime('%b %Y')}</b><br>Hours Played: %{y:.2f}<extra></extra>",
        hovertemplate=f"<b>{df['year_month'][i].strftime('%b %Y')}</b><br>Hours Played: {df['hours_played'][i]}<extra></extra>",
        showlegend=False
    ))

# Add custom legend manually.
for m in range(1, 13):
    month_name = calendar.month_abbr[m]
    rgba = cmap(norm(m))
    color_hex = 'rgba({},{},{},{})'.format(
        int(rgba[0]*255), int(rgba[1]*255), int(rgba[2]*255), rgba[3]
    )
    fig.add_trace(go.Scatter(
        x=[None], y=[None],
        mode='lines',
        line=dict(color=color_hex, width=3),
        name=month_name
    ))

fig.update_layout(
    title='Spotify Listening Time Per Month<br><sup>We Came From Nothing to Something</sup>',
    xaxis_title='Date',
    yaxis_title='Hours Played',
    hovermode='x unified',
    legend_title_text='Month',
    height=600,
    xaxis=dict(tickformat='%Y', tickangle=45),
    title_x=0.5,
)

# Show every year avoid annoying year-skipping
# fig.update_layout(
#     xaxis=dict(
#         tickmode='linear',
#         dtick=31536000000,  # 1 year in milliseconds
#         tickformat='%Y'
#     )
# )


# Fix yearly tick issues
years = df['year_month'].dt.year.unique()
tickvals = [f"{year}-01-01" for year in sorted(years)]

fig.update_layout(
    xaxis=dict(
        tickmode='array',
        tickvals=tickvals,
        tickformat='%Y',
        ticklabelmode='period'  # Optional: show the full year label
    )
)

fig.show()
```

#### What happened in 2020?

**Query**

```{python}
query = """
-- Generate Top Genres for 2020
WITH artist_playtime AS (
	SELECT 
		timestamp_column::date AS cal_day
		,a.genres
		,sd.ms_played
	FROM artists AS a
	JOIN sd_artists_join AS sdj 
		ON a.id = sdj.artist_id
	JOIN spotify_data AS sd 
		ON sdj.sd_id = sd.id
	AND year_played = 2020
	GROUP BY 
		cal_day
		,a.genres
		,sd.ms_played
	ORDER BY
		timestamp_column::date ASC
),
genre_ranks AS (
	SELECT
		UNNEST(genres) AS genre,
		ROUND((CAST(SUM(ms_played) AS numeric)  / 3600000) , 2) AS hours_played,
		RANK() OVER (
			PARTITION BY cal_day
			ORDER BY SUM(ms_played) DESC
		) AS genre_rank,
		cal_day
	FROM artist_playtime
	GROUP BY cal_day, genre
)
SELECT *
FROM genre_ranks
WHERE genre_rank <= 10
ORDER BY cal_day, genre_rank;
"""
```

**Result**

```{python}
# Obtain the top 5 genres in 2020, partitioned on a daily level
top_10_genres_2020_df = fetch_query_results(query)
top_10_genres_2020_df.head(5)
```

```{python}
#Convert date
top_10_genres_2020_df['cal_day'] = pd.to_datetime(top_10_genres_2020_df['cal_day'])

# Create 'month' column  with `YYYY-MM`format
top_10_genres_2020_df['month'] = top_10_genres_2020_df['cal_day'].dt.to_period('M').astype(str)

# Parameterize the number of genres displayed. 5 through 10 should be sufficient.
num_genres = 10

# Aggregate by month + genre.
monthly_genre = top_10_genres_2020_df.groupby(['month', 'genre'], as_index=False)['hours_played'].sum()

# Sort genres by total hours played (descending), to control stacking order.
genre_order = (
    monthly.groupby('genre')['hours_played']
    .sum()
    .sort_values(ascending=False)
    .index.tolist()
)

# Keep only top 10 genres overall (or top N)
top_genres = (
    monthly_genre.groupby('genre')['hours_played']
    .sum()
    .nlargest(num_genres)
    .index
)

monthly_top = monthly_genre[monthly_genre['genre'].isin(top_genres)]


# Plot
fig = px.bar(
    monthly_top,
    x='month',
    y='hours_played',
    color='genre',
    title=f'Top {num_genres} Genres by Month in 2020<br><sup>Dance in the Summer, Brood in the Winter',
    labels={'hours_played': 'Hours Played', 'month': 'Month'},
    category_orders={'genre': genre_order}
)

fig.update_layout(
    barmode='stack',
    xaxis_tickangle=-45
)

# Align title
fig.update_layout(title={'x': 0.065})

fig.show()
```

* The `afro house` genre had a significant share of hours played in March. Afterwards, this genre does not have a prominent share in any subsequent month.
* With COVID-19 emergency lockdown and workplace restrictions taking place towards the end of march, the dropoff in hours played in April and May coincide. This is counterintuitive, since one would expert that more time at home, away from the office would mean more listening time. In this case, the disruption of routine explains the plummet in listening time. As detailed in the "Time of Day" section, we see a notable drop in morning time listening behavior as well as less listening volume on a yearly scale.
* June, July, and August aligned with my personal preconception of summer months being more "free," lending themselves to higher listening volume. The winter months pale in comparison with playing time, in spite of there being more reasons to be indoors, where listening may occur, versus outside and enjoying summer air.
* The "Francophone" nature of my listening habits fluctuates this year. `Pop urbaine`, `french rap`, `french r&b` appear in winter months mostly, and in the months immediately after lockdown, before the summer listening explosion. While these genres are still present in summer, their contribution to listening time is overwhelmed by other genres. 
* Qualitatively, "feel" of the genres seems to match the season.`Afropop`, `afrobeats` being danceable, feeling light and free; uplifting music to counter the stresses of the year. The "Francophone" sounds dominate the winter months, more bleak, and sinister; [Sartre](https://en.wikipedia.org/wiki/Nausea_(novel)) and cigarettes.

#### What happened in 2017?

**Query**

```{python}
query = """
-- Generate Top Genres for 2017
WITH artist_playtime AS (
	SELECT 
		timestamp_column::date AS cal_day
		,a.genres
		,sd.ms_played
	FROM artists AS a
	JOIN sd_artists_join AS sdj 
		ON a.id = sdj.artist_id
	JOIN spotify_data AS sd 
		ON sdj.sd_id = sd.id
	AND year_played = 2017
	GROUP BY 
		cal_day
		,a.genres
		,sd.ms_played
	ORDER BY
		timestamp_column::date ASC
),
genre_ranks AS (
	SELECT
		UNNEST(genres) AS genre,
		ROUND((CAST(SUM(ms_played) AS numeric)  / 3600000) , 2) AS hours_played,
		RANK() OVER (
			PARTITION BY cal_day
			ORDER BY SUM(ms_played) DESC
		) AS genre_rank,
		cal_day
	FROM artist_playtime
	GROUP BY cal_day, genre
)
SELECT *
FROM genre_ranks
WHERE genre_rank <= 10
ORDER BY cal_day, genre_rank;
"""
```

**Result**

```{python}
# Obtain the top 5 genres in 2020, partitioned on a daily level
top_10_genres_2017_df = fetch_query_results(query)
top_10_genres_2017_df.head(5)
```

```{python}
#Convert date
top_10_genres_2017_df['cal_day'] = pd.to_datetime(top_10_genres_2017_df['cal_day'])

# Create 'month' column  with `YYYY-MM`format
top_10_genres_2017_df['month'] = top_10_genres_2017_df['cal_day'].dt.to_period('M').astype(str)

# Parameterize the number of genres displayed. 5 through 10 should be sufficient.
num_genres = 10

# Aggregate by month + genre.
monthly_genre = top_10_genres_2017_df.groupby(['month', 'genre'], as_index=False)['hours_played'].sum()

# Sort genres by total hours played (descending), to control stacking order.
genre_order = (
    monthly.groupby('genre')['hours_played']
    .sum()
    .sort_values(ascending=False)
    .index.tolist()
)

# Keep only top 10 genres overall (or top N)
top_genres = (
    monthly_genre.groupby('genre')['hours_played']
    .sum()
    .nlargest(num_genres)
    .index
)

monthly_top = monthly_genre[monthly_genre['genre'].isin(top_genres)]


# Plot
fig = px.bar(
    monthly_top,
    x='month',
    y='hours_played',
    color='genre',
    title=f'Top {num_genres} Genres by Month in 2017<br><sup>June and July Stand High Above the Pack',
    labels={'hours_played': 'Hours Played', 'month': 'Month'},
    category_orders={'genre': genre_order}
)

fig.update_layout(
    barmode='stack',
    xaxis_tickangle=-45
)

# Align title
fig.update_layout(title={'x': 0.065})

fig.show()
```

* April seems to be the first month of significant streaming history. This month was intensely exploratory, `classical`, `alternative r&b`, and `minimalism` dotted throughout. 
* May, June and July profile very similarly in genre composition, `ambient`, `space music`, `minimalism`, `drone` as the usual suspects.
*  June and July are twin towers for this year in terms of sheer hours played. The dropoff in hours played in August almost seems like I was attending college. Both months were defined by _intense_ amounts of Brian Eno and [Steve Reich](https://open.spotify.com/artist/1aVONoJ0EM97BB26etc1vo?si=MzkjB71wSZGsbPvd1mo-GQ) tracks, [Harmonia](https://open.spotify.com/artist/0tHDVpPzMs1JqKTAuGQkQR?si=SmStAaurSLewD26oQa-CPw) usually following behind in hours played. Steve Reich's presence reminded me how often I would use [Music for Musicians](https://open.spotify.com/album/370lBw4JOlF1CeDRKPqfFh?si=RcHO7A39SS-6AfGlHVXbzg) to just focus on work tasks.
* August, I suspect I got plain sick and tired of hearing the same things over and over again, and tired of listening to artists like Brian Eno to such a magnitude, and weary of listening to Spotify heavily.
* Kilo Kish was "the" `alternative r&b` artist of choice in this year, with her songs being played most heavily in April, August, and September. Her lowest playtime, as not too much of a surprise, occurred in July, with the peaks of hours played falling on either side of summer months.

### ‚ùÑÔ∏è Seasons üçÅ

I held a preconception that Spotify listening activity would increase during inclement months, with inhospitable weather, keeping me inside, bored and wanting audio stimulation. Being at home for longer periods _smay_ lend credence to provoking more listening activity.

As winter resides at the cusp of year's end and year's start. We added separate categorizations of this season to clearly differentiate when a track was played.

**Query**

```{python}
query = """
-- Query to find track counts during different seasons.
WITH with_seasons AS (
	SELECT 
		timestamp_column,
		DATE_PART('year', timestamp_column) AS year,
        ms_played,
		CASE
			WHEN (EXTRACT(MONTH FROM timestamp_column) = 12 AND EXTRACT(DAY FROM timestamp_column) >= 21) THEN 'Winter End'
			WHEN (EXTRACT(MONTH FROM timestamp_column) IN (1, 2)) THEN 'Winter Start'
			WHEN (EXTRACT(MONTH FROM timestamp_column) = 3 AND EXTRACT(DAY FROM timestamp_column) <= 19) THEN 'Winter Start'
			WHEN (EXTRACT(MONTH FROM timestamp_column) = 3 AND EXTRACT(DAY FROM timestamp_column) >= 20) THEN 'Spring'
			WHEN (EXTRACT(MONTH FROM timestamp_column) IN (4, 5)) THEN 'Spring'
			WHEN (EXTRACT(MONTH FROM timestamp_column) = 6 AND EXTRACT(DAY FROM timestamp_column) <= 20) THEN 'Spring'
			WHEN (EXTRACT(MONTH FROM timestamp_column) = 6 AND EXTRACT(DAY FROM timestamp_column) >= 21) THEN 'Summer'
			WHEN (EXTRACT(MONTH FROM timestamp_column) IN (7, 8)) THEN 'Summer'
			WHEN (EXTRACT(MONTH FROM timestamp_column) = 9 AND EXTRACT(DAY FROM timestamp_column) <= 22) THEN 'Summer'
			WHEN (EXTRACT(MONTH FROM timestamp_column) = 9 AND EXTRACT(DAY FROM timestamp_column) >= 23) THEN 'Fall'
			WHEN (EXTRACT(MONTH FROM timestamp_column) IN (10, 11)) THEN 'Fall'
			WHEN (EXTRACT(MONTH FROM timestamp_column) = 12 AND EXTRACT(DAY FROM timestamp_column) <= 20) THEN 'Fall'
			ELSE 'Unknown'
		END AS season,
		CASE
			WHEN (EXTRACT(MONTH FROM timestamp_column) = 12 AND EXTRACT(DAY FROM timestamp_column) >= 21) THEN 4
			WHEN (EXTRACT(MONTH FROM timestamp_column) IN (1, 2)) THEN 0
			WHEN (EXTRACT(MONTH FROM timestamp_column) = 3 AND EXTRACT(DAY FROM timestamp_column) <= 19) THEN 0
			WHEN (EXTRACT(MONTH FROM timestamp_column) = 3 AND EXTRACT(DAY FROM timestamp_column) >= 20) THEN 1
			WHEN (EXTRACT(MONTH FROM timestamp_column) IN (4, 5)) THEN 1
			WHEN (EXTRACT(MONTH FROM timestamp_column) = 6 AND EXTRACT(DAY FROM timestamp_column) <= 20) THEN 1
			WHEN (EXTRACT(MONTH FROM timestamp_column) = 6 AND EXTRACT(DAY FROM timestamp_column) >= 21) THEN 2
			WHEN (EXTRACT(MONTH FROM timestamp_column) IN (7, 8)) THEN 2
			WHEN (EXTRACT(MONTH FROM timestamp_column) = 9 AND EXTRACT(DAY FROM timestamp_column) <= 22) THEN 2
			WHEN (EXTRACT(MONTH FROM timestamp_column) = 9 AND EXTRACT(DAY FROM timestamp_column) >= 23) THEN 3
			WHEN (EXTRACT(MONTH FROM timestamp_column) IN (10, 11)) THEN 3
			WHEN (EXTRACT(MONTH FROM timestamp_column) = 12 AND EXTRACT(DAY FROM timestamp_column) <= 20) THEN 3
			ELSE 99
		END AS season_order
	FROM spotify_data
)
SELECT 
	year,
	season,
	season_order,
    ROUND((CAST(SUM(ms_played) AS numeric)  / 3600000) , 2) AS hours_played,
	COUNT(*) AS track_count
FROM with_seasons
GROUP BY year, season, season_order
ORDER BY year, season_order;
"""
```

**Result**

```{python}
# Query the DB for tracks and the season they occupy.
seasonal_track_count_df = fetch_query_results(query)

seasonal_track_count_df.head()
```

```{python}
# Define the correct season order.
season_order = [
    'Winter Start', 'Spring', 'Summer', 'Fall', 'Winter End'
]

# Set the 'season' column as a categorical for sorting in the plot.
seasonal_track_count_df['season'] = pd.Categorical(
    seasonal_track_count_df['season'],
    categories=season_order,
    ordered=True
)

# Sort the DataFrame so plotting is predictable.
seasonal_track_count_df = seasonal_track_count_df.sort_values(['year', 'season'])

# Plot using Seaborn.
plt.figure(figsize=(16, 6))
sns.barplot(
    data=seasonal_track_count_df,
    x='year',
    y='track_count',
    hue='season',
    hue_order=season_order
)
plt.title("Track Count by Season and Year")
plt.ylabel("Track Count")
plt.xlabel("Year")
plt.xticks(rotation=0)
plt.legend(title="Season", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()
```

```{python}
# Define the correct season order.
season_order = [
    'Winter Start', 'Spring', 'Summer', 'Fall', 'Winter End'
]

# Set the 'season' column as a categorical for sorting in the plot.
seasonal_track_count_df['season'] = pd.Categorical(
    seasonal_track_count_df['season'],
    categories=season_order,
    ordered=True
)

# Sort the DataFrame so plotting is predictable.
seasonal_track_count_df = seasonal_track_count_df.sort_values(['year', 'season'])

# Plot using Seaborn.
plt.figure(figsize=(16, 6))
sns.barplot(
    data=seasonal_track_count_df,
    x='year',
    y='hours_played',
    hue='season',
    hue_order=season_order
)
plt.title("Hours Played by Season and Year")
plt.ylabel("Hours Played")
plt.xlabel("Year")
plt.xticks(rotation=0)
plt.legend(title="Season", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()
```

```{python}
seasonal_track_count_df.head()
```

```{python}
# Calculate summer percentage.

# Group by year and season, then sum hours played
seasonal_sums = seasonal_track_count_df.groupby(['year', 'season'], as_index=False)['hours_played'].sum()

# Pivot to get one row per year and one column per season
seasonal_pivot = seasonal_sums.pivot(index='year', columns='season', values='hours_played').fillna(0)

# Calculate total hours per year and percentage of summer
seasonal_pivot['total_hours'] = seasonal_pivot.sum(axis=1)
seasonal_pivot['summer_percent'] = (seasonal_pivot.get('Summer', 0) / seasonal_pivot['total_hours']) * 100

# Get average summer percentage across all years (optional)
average_summer_percent = seasonal_pivot['summer_percent'].mean()
```

```{python}
average_summer_percent
```

Summer is the season with the highest track count in five years out of the ten full listening years in our history. Spring clocks in with the highest amount within two of the years, and Fall only has only a single year with the maximum track count. Our splitting of winter into "start" and "end" segments, even when combined, never rises to a level where this season has the highest play count. Summer months may permit more leisure and vacation time, with the listening activity observed reflecting this. 

## üóìÔ∏è When Did I Find New Music?

I was curious  _when_ I first played a genre during my listening history. It may not be the first time I have heard the song in life, but just when I played it on Spotify. We focus our attention on the top 10 genres instead of applying this across the board.

```{python}
# Find when a genre first appeared in the top 10.
genre_first_appearance = top_genres_per_year_df.groupby('genre')['stream_year'].min().sort_values()

# Find recent genres
new_genres = top_genres_per_year_df[top_genres_per_year_df['stream_year'] >= 2020]['genre'].value_counts().head(10)
```

```{python}
# Visually inspect when we first encountered genres.
genre_first_appearance.head()
```

```{python}
# Convert to dataFrame
genre_first_df = genre_first_appearance.reset_index()
genre_first_df.columns = ['genre', 'first_year']

# Count number of new genres per year
new_genres_by_year = genre_first_df.groupby('first_year').size().reset_index(name='genre_count')

# Plot as scatter plot
fig = px.scatter(
    new_genres_by_year,
    x='first_year',
    y='genre_count',
    text='genre_count',
    title='New Top-10 Genres Encountered Per Year',
    labels={'first_year': 'Year', 'genre_count': 'New Genres'},
    hover_data={'first_year': True, 'genre_count': True}
)

fig.update_traces(marker=dict(size=8, color="mediumvioletred"), textposition='bottom center')
fig.update_layout(showlegend=False)

fig.show()
```

At the tail end of the listening history, we observe the introduction larger amounts of new genres (>**6**), as well as the zenith in **2020** of **10** new genres. **2025** and my initial surfacing of the platform would lend itself to new music, but the **2020** explosion is intriguing. I wonder if work-from-home initiatives, coupled with the generalized reduction in availability of activities outside of the home unintentionally provide more opportunity and motivation to find new music.

```{python}
# List the genres seen after the year 2020. More recently discovered music may share characteristics.
new_genres
```

```{python}
top_genres_per_year_df.describe()
```

::: {.panel-tabset}
#### üìä View Genres Exceeding Mean Hours Played Table

<details>
<summary>Click to expand plot</summary>

```{python}
#| scrolled: true
# Let's find genres played for time larger than the mean of 89.046
top_genres_per_year_df.query('hours_played >=89.045729')
```

</details>
:::

The data here indicates extreme playtime for `ambient`, `drone` and `minimalism`, reflecting utilization of Spotify as a "tool." A tool to provide a productivity boon, for working relaxation, or a tool to help me fall asleep more easily. There is nothing about this that seems danceable, melodic, or strongly emotionally evocative at the top of the pile. Less is more, and I wanted to do more, with Spotify in the background.

## ü•£ Where Are My Tastes Centered?

Since we seek insights, I am curious _how_ my tastes have changed over time, along with my increased Spotify listening activity.

I stumbled upon the [Every Noise At Once](https://everynoise.com/) website made by [Glenn McDonald](http://furia.com/)  as I was exploring how to access the Spotify Web API. There are thousands of musical genres displayed on the site seemingly mapped in two-dimensional space.

If we view every genre shown at ENAO as a fixed 2-d point, each point could become a **vector**; with our listening history in mind, we can assign each vector a magnitude based on play count or time elapsed associated with tracks in our listening history, normalizing as needed. If we compute a weighted average for "genre-vectors" for a given year, based on `ms_played`, we could compute a crude "musical center of gravity" per year. We could then observe, year by year, how this musical center of gravity changes over time. 
 

```{python}
# Set image path
enao_image_path = os.getcwd() + "\\exports\\images\\ENAO_screenshot.png"

# Path to image file (relative or absolute)
display(Image(filename=enao_image_path, width=600, height=600))
```

_Credit: [Every Noise at Once](https://everynoise.com)_

### Visual Explainer

To illustrate our idea of a genre vector weighted by listening time, consider the following:

- Each genre from the ENAO dataset has a fixed (x, y) coordinate in a 2D space.

- Listen to an artist, we associate the observed listening time with _all_ genres that artist is tagged with.

- To capture the center of musical gravity in a given year, we calculate the weighted average of genre coordinates, with the "weight" being how much time you spent listening.

This diagram below shows how that works on a sample dataset of five genres, with the red star the resulting center computed:

```{python}
example_data = pd.DataFrame({
    "genre": ["math rock", "ambient", "space music", "rap", "classical"],
    "x": [500, 200, 1000, 800, 300],
    "y": [8000, 12000, 5000, 9000, 15000],
    "ms_played": [10000, 20000, 5000, 15000, 30000]
})

example_data["weighted_x"] = example_data["x"] * example_data["ms_played"]
example_data["weighted_y"] = example_data["y"] * example_data["ms_played"]

total_ms = example_data["ms_played"].sum()
x_center = example_data["weighted_x"].sum() / total_ms
y_center = example_data["weighted_y"].sum() / total_ms

fig = go.Figure()
fig.add_trace(go.Scatter(
    x=example_data["x"],
    y=example_data["y"],
    mode='markers+text',
    marker=dict(size=example_data["ms_played"] / 1000, color='lightblue', line=dict(width=1, color='black')),
    text=example_data["genre"],
    textposition="top center",
    name="Genres"
))

fig.add_trace(go.Scatter(
    x=[x_center],
    y=[y_center],
    mode='markers+text',
    marker=dict(size=25, color='red', symbol='star', line=dict(width=2, color='black')),
    text=["Weighted Center"],
    textposition="bottom center",
    name="Center"
))

# Add arrows for clarity

# Add arrows from each point to the weighted center
for i, row in example_data.iterrows():
    fig.add_annotation(
        ax=row["x"],
        ay=row["y"],
        x=x_center,
        y=y_center,
        xref="x",
        yref="y",
        axref="x",
        ayref="y",
        showarrow=True,
        arrowhead=2,
        arrowsize=1,
        arrowwidth=1,
        opacity=0.4,
        arrowcolor="gray"
    )
    
    # Add annotation with ms_played value
    fig.add_annotation(
        x=row["x"],
        y=row["y"],
        text=f"{row['ms_played']//1000} sec",
        showarrow=False,
        font=dict(size=10, color="black"),
        xanchor="right",
        yanchor="bottom",
        yshift=-30
    )


fig.update_layout(
    title="Visual Explanation of Genre Vector Averaging",
    xaxis_title="X Coordinate",
    yaxis_title="Y Coordinate",
    width=800,
    height=600,
    showlegend=False
)

fig.show()
```

### Caveats

* Defining "genre points" in a coordinate system appropriated for the purpose of finding similarity, is "fuzzy" by nature. We have seen our listening activity shift from minimal, to non-existent, to explosive in volume. If the listening history is defined by unpredictable behavior, subject to rapid changes, and possessing intense fixations on specific artists or genres, the information obtained from analyzing the "musical centers" may not communicate anything conclusive or meaningful.
* We lack familiarity with the methodology underlying [ENAO](https://everynoise.com/), so the mechanisms and relationships of this musical genre-space are nebulous. For our purposes, we treat the coordinate system as an accurate representation of genre similarity. Genres with smaller distances between them in the coordinate system sharing characteristics, genres further apart in the coordinate system sharing less characteristics, however those aspects are calculated and measured internally.
* The Spotify design decision to associate genres with artists, affects the implied semantics governing vector calculations. Recall that `space music` was a mysterious genre, but derived from a familiar artist in heavy rotation within the listening history. Our vector calculations may be pulled across the genre-space in mysterious ways due to broad genre associations artists may possess.

### Implementation

We must first obtain data related to the [ENAO](https://everynoise.com/) genre coordinates. There was no CSV on their website that contained this information but a GitHub gist rendered this data accessible.

```{python}
# Read the EveryNoise Data from GitHub into a dataframe
genre_coords = pd.read_csv(
    'https://raw.githubusercontent.com/AyrtonB/EveryNoise-Watch/main/data/genre_attrs.csv'
)

# It typically has columns: 'genre', 'x', 'y', and RGB if desired
logging.info(genre_coords.head())
```

We must extract the relevant genre information from our database. We execute the following query, again ensuring we only obtain tracks with relevant amounts of listening time.

**Query**

```{python}
# Execute a query to grab unnested genre data
query = """
-- Use CROSS JOIN UNNEST to expand artist genres into separate rows for each listening record.
SELECT 
    DATE_PART('year', sd.timestamp_column)::int AS year,
    g AS genre,
    sd.ms_played
FROM spotify_data sd
JOIN sd_artists_join sdj ON sd.id = sdj.sd_id
JOIN artists a ON sdj.artist_id = a.id
CROSS JOIN UNNEST(a.genres) AS g
WHERE a.genres IS NOT NULL
AND sd.ms_played > 5000
"""
```

**Results**

```{python}
genres_per_track_df = fetch_query_results(query)
genres_per_track_df.head()
```

We retrieve listening data at the level of individual genres for each track. We use the `spotify_data`, `sd_artists_join`, and `artists` tables, with the `artists` table holding the genre metadata. Since the genres are stored in an array, we must `UNNEST` to obtain them individually.

The use of `CROSS JOIN UNNEST...` expands each artist's genre array into individual rows. If an artist has three genres, for each matching track, there will be three separate rows in the result, one for each genre.

We also extract the year from the listening timestamp and include the total milliseconds played for each listening event. With the final result stored in a dataframe containing one row per track-genre-year combination, we can move on to merging this data with the genre coordinates information.

We define a "musical center of gravity" as 2-D weighted centroid of all genre coordinates for a given year, weighted by listening duration.

To calculate these centers, we need to combine two sources of data:

- The listening history, with tracks and genre information.
- The genre "coordinate space," placing each genre on a 2D map.

To do this, we will:

1. Ensure all required columns exist in both datasets
2.  Remove any existing `x` or `y` coordinate columns from the `genres_per_track_df` DataFrame to prevent duplicate columns during the merge
3. Merge the listening data with the genre coordinates on the genre name
4. Drop rows with missing data or zero listening time

The resulting dataframe should contain valid, coordinate-tagged listening events, ready for future calculations.

```{python}
# Safely merge genre coordinates.

# Sanity check to ensure required columns are present.
assert all(col in genre_coords.columns for col in ["genre", "x", "y"]), "Missing coordinate columns in genre_coords"
assert "genre" in genres_per_track_df.columns, "'genre' column missing in genres_per_track_df"

# Drop existing x/y columns from main dataframe to avoid collision during merge.
genres_per_track_df = genres_per_track_df.drop(columns=[col for col in ["x", "y"] if col in genres_per_track_df.columns])

# Merge listening data with genre coordinates using genre as the key.
genres_with_coords = genres_per_track_df.merge(genre_coords, on="genre", how="inner")

# Drop any rows missing coordinates or listening time.
genres_with_coords.dropna(subset=["x", "y", "ms_played"], inplace=True)

# Filter out rows where minimal listening occurred.
valid_df = genres_with_coords[genres_with_coords["ms_played"] > 5000].copy()

# Final check: output column names to confirm structure
logging.info(f"‚úÖ Clean merge complete. valid_df columns:{valid_df.columns.tolist()}")
```

We carefully merge our Spotify listening data and the genre coordinate space. As this particular step was iterated numerous times, we encountered issues like duplicate columns or missing keys, causing problems further along in our process.

To prevent problems downstream, we:
- Explicitly check that both datasets contain the necessary columns (`genre`, `x`, `y`)
- Drop any `x` or `y` columns from the listening data to avoid duplication during the merge
- Merge on the shared `genre` key to add 2D coordinates to each listening event
- Drop rows with missing or minimal `ms_played` values to ensure we are only working with valid, meaningful data

These checks may not be required in a finalized pipeline, but are useful during development to ensure that we are operating on a consistent and clean dataset.

We now compute the "musical center of gravity" for each year. Each genre has coordinates, each track has listening time. We use a weighted average to reflect listening intensity.

```{python}
# Sanity check for duplicate columns
dupes = valid_df.columns[valid_df.columns.duplicated()]
assert len(dupes) == 0, f"Duplicate columns found: {dupes.tolist()}"

# Do a safe copy
grouped = valid_df[["year", "x", "y", "ms_played"]].copy()

# Weighted coordinate math.
grouped["weighted_x"] = grouped["x"] * grouped["ms_played"]
grouped["weighted_y"] = grouped["y"] * grouped["ms_played"]

# Aggregate totals for each year
weighted_centers = (
    grouped.groupby("year")
    .agg(
        total_ms=("ms_played", "sum"),
        total_weighted_x=("weighted_x", "sum"),
        total_weighted_y=("weighted_y", "sum")
    )
    .reset_index()
)

# Compute final weighted centers
weighted_centers["x_center"] = weighted_centers["total_weighted_x"] / weighted_centers["total_ms"]
weighted_centers["y_center"] = weighted_centers["total_weighted_y"] / weighted_centers["total_ms"]

# Optional cleanup
weighted_centers.drop(columns=["total_ms", "total_weighted_x", "total_weighted_y"], inplace=True)

# ‚úÖ Sanity check
logging.info("‚úÖ Weighted centers calculated:")
display(weighted_centers)
```

We verify there are no duplicated columns before calculations, weighting each genre‚Äôs coordinates by listening time and compute the center for each year. 

We now generate plots using plotly, with each genre dot now supporting tooltips on hover, clear labeling for the "musical center of gravity," looping through the years to render an interactive plot for each.

::: {.panel-tabset}
#### üìä View Yearly Listening Center Pots

<details>
<summary>Click to expand plots</summary>

```{python}
#| scrolled: true
# Make sure we have hex colours.
if "hex_colour" not in valid_df.columns:
    valid_df = valid_df.merge(
        genre_coords[["genre", "hex_colour"]],
        on="genre",
        how="left"
    )

# Unique years to loop over.
years = sorted(valid_df["year"].unique())

# One plot per year.
for year in years:
    year_df = valid_df[valid_df["year"] == year]
    center = weighted_centers[weighted_centers["year"] == year].iloc[0]

    fig = go.Figure()

    # Plot genre dots.
    fig.add_trace(go.Scatter(
        x=year_df["x"],
        y=year_df["y"],
        mode='markers',
        marker=dict(
            size=10,
            color=year_df["hex_colour"],
            line=dict(width=0.5, color='black'),
            opacity=0.7
        ),
        text=year_df["genre"],
        hoverinfo='text',
        name='Genres'
    ))

    # Plot the listening "center."
    fig.add_trace(go.Scatter(
        x=[center["x_center"]],
        y=[center["y_center"]],
        mode='markers',
        marker=dict(
            size=30,
            color='red',
            symbol='star',
            line=dict(width=2, color='black')
        ),
        text=[f"Listening Center: {year}"],
        hoverinfo='text',
        name='Listening Center'
    ))

    fig.update_layout(
        title=f"Yearly Genre Listening Map ‚Äî {year}",
        xaxis_title="X",
        yaxis_title="Y",
        height=600,
        width=800,
        showlegend=True
    )

    fig.show()
```

</details>
:::

Hovering over the plot points clearly reveals genre names, giving clearer insight into what clusters form around each of the listening centers. 
In spite of any plots, we want the ability to determine genres that are proximate to our "center" via function without having to view plots. The `get_surrounding_genres` function allows us to zoom into the ‚Äúneighborhood‚Äù around any (x, y) coordinate and retrieve the genres located nearby. The search radius is defined as a percentage of the total horizontal (x-axis) spread, and with optional filtering by year.

```{python}
# Get surrounding genres in 2024 within 30% x-range of the center
center_row = weighted_centers[weighted_centers["year"] == 2020].iloc[0]

# Obtain surrounding genres
surrounding_genres = get_surrounding_genres(valid_df, center_row["x_center"], center_row["y_center"], radius_pct=60, year=2020)

# Preview results, limiting range to 200
surrounding_genres.query('distance <200').head()
```

### Recommended Genres by Proximity

Seeing how close the musical "centers of gravity" are to other genres provides avenues of future genre exploration, providing fodder for future song and artist recommendations.

To explore genres residing near our computed musical centers, we iterate over the range of years in our listening history.

```{python}
for year in range(2013,2026):
    logging.info(year)
    try:
        center_row = weighted_centers[weighted_centers["year"] == year].iloc[0]
        
        # Find surrounding genres
        surrounding_genres = get_surrounding_genres(valid_df, center_row["x_center"], center_row["y_center"], radius_pct=30, year=year)

        # Extract the unique genres and select 10 of them
        logging.info(surrounding_genres['genre'].unique()[:9])
    except Exception as e:
        logging.info(f"An error occurred: {e}")
        logging.info(f"Unable to process year: {year}")
```

We collect the three most-recent years and list out the genres near to our musical centers of gravity. Surprising results, as these seem to contrast with my musical self-image at times. Still, exciting to see there may be aspects to these novels genres I may find appealing.  

* **2022**
    * glam rock
    * soft rock
    * indie rock
    * yacht rock
    * philly soul
    * bedroom pop
    * jangle pop
    * riot grrrl
    * v-pop
* **2023**
    * art pop
    * opm
    * celtic rock
    * space rock
    * v-pop
    * lo-fi
    * rock
    * indie r&b
    * stoner rock
* **2024**
    * chinese r&b
    * taiwanese indie
    * sholawat
    * taiwanese pop
    * mandopop
    * gospel r&b
    * grupera
    * pinoy indie
    * retro soul 

## üÜöComparing Centers vs. Top Genres

### Hours Played and Genre Rank

As we have already analyzed our top genres by playtime, we can understand how closely top genres reflect the musical centers of gravity computed earlier.

The proximity of top genres to our musical centers can be interpreted different ways:

| Scenario                                            | What It Tells Us                                                                               |
| --------------------------------------------------- | ---------------------------------------------------------------------------------------------- |
| Top genre is near center                            | Listening habits were consistent and focused                                                    |
| Top genre is far from center                        | Explored diverse genres, diluting the center                                            |
| Center near niche genres with minimal listening activity | Primary genres live in a crowded genre space, sharing coordinates with adjacent styles |


To understand whether my top genres truly reflect the musical centers of gravity computed earlier, we need to:

* Merge the top genres data with the genre coordinates (x, y).
* Bring in the yearly musical "centers."
* Compute the [Euclidean Distance](https://en.wikipedia.org/wiki/Euclidean_distance) between each top genre and that year's "center."
* Normalize distances from the "center" based on the range of the x-axis for easy comparison.
* Rank genres per year by proximity to their "center."

```{python}
# Copy previously generated top genres by hours_played.
top_genres = top_genres_per_year_df.copy()

# Drop index column if present, possible if data is loaded from file.
if "Unnamed: 0" in weighted_centers.columns:
    weighted_centers.drop(columns=["Unnamed: 0"], inplace=True)

# Merge genre coordinates.
top_genres = top_genres.merge(genre_coords, on="genre", how="left")

# Merge with musical centers.
top_genres = top_genres.merge(weighted_centers, left_on="stream_year", right_on="year", how="left")

# Compute Euclidean distance.
top_genres["distance"] = np.sqrt((top_genres["x"] - top_genres["x_center"])**2 +
                                 (top_genres["y"] - top_genres["y_center"])**2)

# Normalize distance by x-axis range.
x_range = genre_coords["x"].max() - genre_coords["x"].min()
top_genres["normalized_distance"] = top_genres["distance"] / x_range

# Rank genres per year by closeness to center.
top_genres["rank_by_proximity"] = top_genres.groupby("stream_year")["normalized_distance"].rank()

# Preview results
top_genres.head()
```

```{python}
top_genres.query('rank_by_proximity < 3').head()
```

```{python}
# Sort and map colors to years.
years = sorted(top_genres["year"].unique())
color_map = {
    year: px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)]
    for i, year in enumerate(years)
}

# Genre Rank vs Normalized Distance.
fig1 = go.Figure()
for year in years:
    year_df = top_genres[top_genres["year"] == year]
    fig1.add_trace(go.Scatter(
        x=year_df["genre_rank"],
        y=year_df["normalized_distance"],
        mode='markers+text',
        text=year_df["genre"],
        marker=dict(
            size=10,
            color=color_map[year],
            line=dict(width=0.5, color="black"),
            opacity=0.8
        ),
        name=str(year),
        legendgroup=str(year),
        customdata=year_df[["hours_played"]].values,
        hovertemplate=(
            "<b>Genre:</b> %{text}<br>" +
            "<b>Rank:</b> %{x}<br>" +
            "<b>Normalized Distance:</b> %{y:.2f}<br>" +
            "<b>Hours Played:</b> %{customdata[0]:.2f}<extra></extra>"
        )
    ))

fig1.update_layout(
    title="Genre Rank vs. Normalized Distance from Yearly Listening Center",
    xaxis_title="Genre Rank (1 = Most Played)",
    yaxis_title="Normalized Distance",
    legend_title="Year",
    height=700,
    width=1000
)
fig1.show()

# 2. Hours Played vs Normalized Distance
fig2 = go.Figure()
for year in years:
    year_df = top_genres[top_genres["year"] == year]
    fig2.add_trace(go.Scatter(
        x=year_df["normalized_distance"],
        y=year_df["hours_played"],
        mode='markers+text',
        text=year_df["genre"],
        marker=dict(
            size=10,
            color=color_map[year],
            line=dict(width=0.5, color="black"),
            opacity=0.8
        ),
        name=str(year),
        legendgroup=str(year),
        hovertemplate=(
            "<b>Genre:</b> %{text}<br>" +
            "<b>Normalized Distance:</b> %{x:.2f}<br>" +
            "<b>Hours Played:</b> %{y:.2f}<extra></extra>"
        )
    ))

fig2.update_layout(
    title="Hours Played vs. Normalized Distance from Yearly Listening Center",
    xaxis_title="Normalized Distance",
    yaxis_title="Hours Played",
    legend_title="Year",
    height=700,
    width=1000
)
fig2.show()
```

* Isolating the outlier year of **2015**, `deep house` has both the highest ranking in playtime and the smallest normalized distance from the computed center. Other genres have much greater normalized distances, and all share the same genre ranking of **2**.
* In **2024**, where `lo-fi beats` had high genre rank and was closest to the center. At the same time during this year, `synthwave` had a high genre ranking of **4** while residing some distance from the center; this implies some exploratory behavior on my part.
* **2021** was unique, with numerous genres being physically, close to the musical center, but having low genre ranks. This maps onto the scenario of genres living in a crowded genre space. 

### Musical Center Movement Through Time

We can further enrich our plotting the position of musical centers yearly, layering in added genre context from the coordinate space. In the following plot, the red path shows how our listening "centers" shifted over time.

```{python}
# Group by year to get unique centers.
center_df = top_genres.groupby("year")[["x_center", "y_center"]].first().reset_index()

# Add line connecting centers.
fig = go.Figure()
fig.add_trace(go.Scatter(
    x=center_df["x_center"],
    y=center_df["y_center"],
    mode="lines+markers+text",
    text=center_df["year"],
    marker=dict(size=10, color="red"),
    line=dict(width=2),
    name="Center Movement",
    hovertemplate="<b>Year:</b> %{text}<br><b>X:</b> %{x}<br><b>Y:</b> %{y}<extra></extra>",
    textposition="top center"
))

fig.update_layout(
    title="Movement of Listening Center Over the Years",
    xaxis_title="X Coordinate",
    yaxis_title="Y Coordinate",
    height=700,
    width=900,
    showlegend=False
)

# Adding genres to flesh out plot
genre_labels = ["hip hop", "ambient", "math rock", "experimental", "indie pop", "drone", "new wave", "funk", "trap"]

label_df = genre_coords[genre_coords["genre"].isin(genre_labels)].copy()

fig.add_trace(go.Scatter(
    x=label_df["x"],
    y=label_df["y"],
    mode='markers+text',
    marker=dict(
        color="gray",
        size=8,
        opacity=0.5
    ),
    text=label_df["genre"],
    textposition="top center",
    hoverinfo="text",
    name="Reference Genres",
    showlegend=False
))


fig.show()
```

```{python}
#| scrolled: true
# Clean column names
genre_coords.columns = [col.strip() for col in genre_coords.columns]
weighted_centers.columns = [col.strip() for col in weighted_centers.columns]

# Select genres near the center region.
manual_genre_labels = genre_coords[
    (genre_coords["x"] >= 500) & (genre_coords["x"] <= 12000) &
    (genre_coords["y"] >= 8000) & (genre_coords["y"] <= 18000) 
].copy()

manual_genre_labels = manual_genre_labels.sample(n=15, random_state=42)  # Select a sample to avoid clutter

# Identify the genre closest to each yearly center.
closest_genres = []
for _, row in weighted_centers.iterrows():
    year = row["year"]
    center_x, center_y = row["x_center"], row["y_center"]
    genre_coords["distance"] = ((genre_coords["x"] - center_x) ** 2 + (genre_coords["y"] - center_y) ** 2) ** 0.5
    closest = genre_coords.loc[genre_coords["distance"].idxmin()]
    closest_genres.append({
        "genre": closest["genre"],
        "x": closest["x"],
        "y": closest["y"],
        "year": year
    })

closest_genre_df = pd.DataFrame(closest_genres)

# Plot the movement, nearby genres, and closest genres.
fig = go.Figure()

# Listening center path
fig.add_trace(go.Scatter(
    x=weighted_centers["x_center"],
    y=weighted_centers["y_center"],
    mode="lines+markers+text",
    text=weighted_centers["year"].astype(str),
    line=dict(color="red", width=2),
    marker=dict(size=10, color="red"),
    textposition="top center",
    name="Listening Center"
))

# Manually selected nearby genres
fig.add_trace(go.Scatter(
    x=manual_genre_labels["x"],
    y=manual_genre_labels["y"],
    mode="markers+text",
    marker=dict(color="gray", size=6, opacity=0.5),
    text=manual_genre_labels["genre"],
    textposition="top center",
    name="Nearby Genres",
    showlegend=False
))

# Closest genre per year
fig.add_trace(go.Scatter(
    x=closest_genre_df["x"],
    y=closest_genre_df["y"],
    mode="markers+text",
    marker=dict(color="blue", size=8),
    text=closest_genre_df["genre"],
    textposition="bottom center",
    name="Closest Genre to Center"
))

# Added layout tweaks
fig.update_layout(
    title="Listening Center Movement with Nearby and Closest Genres",
    xaxis_title="X Coordinate",
    yaxis_title="Y Coordinate",
    height=800,
    width=1000,
    showlegend=True
)

fig.show()
```

This visualization provides more context for our listening trajectory, with the inclusion of nearby genres along with the nearest genre to a yearly "center." The gray-labeled genres help ground the movement path within familiar genre clusters and give us a clearer sense of the genre space. The blue-labeled genres pinpoint the genre most associated with each year's listening center. These provide "anchor points" in our musical taste.

Inspecting **2020**, apparently my tastes were centered near `bhangra` and `hip hop`. While I would normally not explicitly play a `bhangra` track off the top of my head, this genre having proximity to `romanian rap` and `afroswing` is easier to reason about given our remembrance of the "Francophone" binge-listening period (`pop urbaine`, `french rap`) uncovered earlier in discussions relating to time of day.

**2015** being an "outlier" is likely due to the extremely small amount of data collected in that year of listening. 

```{python}
top_genres_per_year_df.query('stream_year == 2024')
```

### Year-by-Year Comparison: Listening Centers vs. Top Genres

We visualized each year's listening center mapped onto the genre coordinate system, and labeled the nearest genre to each center. We can also compare each center's location alongside the actual top genres in our listening history. This can determine whether our listening was highly focused (center aligned with top genres) or more exploratory (center lies in a niche away from top genres).

---

#### 2015
- **Top Genres**: *deep house*
- **Closest Genres**: *deep house*
- **Interpretation**: An "outlier" year with only the `deep house` genre was played. Only **11** minutes of music in this genre was played for the entire year. It may give indications of what sorts of music I wanted to hear with my initial foray into the platform, but will be eclipsed by later activity.

---

#### 2017‚Äì2018
- **Top Genres**: *ambient*, *drone*, *minimalism*
- **Closest Genres**: *acoustic pop*, *math rock*, *kayokyoku*
- **Interpretation**: These years were the [Brian Eno](https://open.spotify.com/artist/7MSUfLeTdDEoZiJPDSBXgi?si=Hk8x3dnbTOe6ZOCLf3_4VA)/[Steve Reich](https://open.spotify.com/artist/1aVONoJ0EM97BB26etc1vo?si=-5rOKMNnRKqGRDE5GYq3xw) era. These years had strong thematic coherence, with the centers deeply embedded in the regions where the top genres also lay. Tracks of this ilk could bleed into some form of `acoustic pop` but the proximity to `math rock` is curious; We associate `rock*` genres with aggression and higher BPM, more vocalized, and less atmospheric. It could be the case of a more dispersed set of genres, or a nascent relation to the `lo-fi` genre one would associate with more melodic rock. The `kayokyoku` is of intense personal interest, sharing attributes with the favorite `city pop` genre, but this genre refers to pop in the early 20th century. 

---

#### 2019
- **Top Genres**: *indie soul*, *nu disco*, *ambient*, *french house*, *drone*, *minimalism*
- **Closest Genres**: *retro soul*, *taiwanese indie*, *smooth jazz*
- **Interpretation**: Initially, the top genre was marked as `country`. This genre is  _not_ listened generally, prompting research into the  tracks played in this year. The literal artist name used in search was  [`Darius`](https://open.spotify.com/artist/5vfEaoOBcK0Lzr07WN8KaK?si=3KBQ78CdRhO_gIkVv2Xzyw), but the artist our data returned via the Spotify API was for the artist [`Darius Rucker`] of [Hootie and the Blowfish](https://open.spotify.com/artist/08ct2eZF5lUPdJpHwNKWof?si=RWJJxmn6SMyFV2fEU3TPLQ) fame. After using the [Get Artist](https://developer.spotify.com/documentation/web-api/reference/get-an-artist) endpoint, we repaired the `artists` table and re-ran methods to recalculate top genres. The API requests were configured to only return singular result, versus collecting all possible results, with code or inspection to find the most accurate selection. Searching for `Darius` should have meant the  literal `Darius` search query and should never have been conflated to be `Darius Rucker`. With the error corrected and plots redrawn, we found `retro soul` being the closest genre to the "center" in this year, which is easy to reason about. The `taiwanese indie` genre was suprising, but these are the sorts of songs we find interesting on alternative services like [SoundCloud](https://soundcloud.com) and [MixCloud](https://www.mixcloud.com).

---

#### 2020
- **Top Genres**: *afro house*, *afrobeat*, *afrobeat*
- **Closest Genre**: *kompa*, *indie soul*, *neo soul*
- **Interpretation**: This year was more exploratory, given the center's distance was far from previous years. Its journey to the "northeast" of the ENAO genre-space feels like a return to my first encounters with the platform, centering on more danceable fare. *kompa* sounds reminiscent of *soukous* and aligns well with *pop urbaine*, *french rap*, and *french r&b* tracks observed in this year of reduced listening activity. The *soul* entries seem shared from previous years, in spite of the drift.

---

#### 2021‚Äì2023
- **Top Genres**: *ambient*, *space music*, *minimalism*
- **Closest Genres**: *indie pop*, *glam rock*, *art pop*
- **Interpretation**: These years the centers retreat toward the cluster established in 2017‚Äì2019 in the southwest of the genre-space. This suggests I had enough experimentation and reverted to genres in my comfort zones after the experimentation of 2020. This area seems rife with *pop*, *rock*, and *indie* adjacent genres. This time period may speak to the reassertion of routine and familarity after the disruption of **2020**. 

---

#### 2024‚Äì2025
- **Top Genres**: *lo-fi beats*, *ambient*, *lofi*, *synthwave*
- **Closest Genres**: *chinese r&b*, *taiwanese indie*, *sholawat*, *taiwanese pop* 
- **Interpretation**: Drifting upwards again, near **2019**. Newer sounds with at eastern flourish, as seen by *chinese r&b* and *taiwanese indie*. This could reflect platform recommendations or personal curiosity, suggesting exploration within bounds rather than radical departure.

---

### Summary

- Listening centers clustered tightly in **2017**‚Äì**2019** and again in **2021**‚Äì**2023**, showing strong stylistic consolidation. After collapsing on a set of genres, those choices remain for extended periods of time.
- **2020** was an outlier year, the listening center suggesting active searching for new music genres outside of prior experiences. There were less hours played and more variety than previously seen. The genres were extremely "danceable," like `afrobeat` and `afroswing`. 
- Recent years (**2024**‚Äì**2025**) show consistency with some mild expansion. This may indicate platform recommendations or ambient discovery of new artists and genres stumbling upon new sounds. Despite exploration, atmospheric genres remain "foundational" to our listening behavior.

## üìù Appendix

### Data Import Challenges

* In spite of the `README` included with the Spotify streaming history data indicating that `offline_timestamp` was an _actual_ timestamp, this JSON field appears to be an epoch time value. Ex:
    ```
    "offline_timestamp": 1521173773147,"
    ```
    This epoch time value also includes milliseconds in the last three digits. To process this, we divide by **1000** in our `load_json_from_file` function and cast into `int` type prior to database table insertion.

* Various columns, such as  `episode_name` require use of `varchar` columns which are vastly oversized compared to the majority of the data lengths seen in this field. 
    ```
    "episode_name": "On The Continent: Tensions rise at Barcelona, Roberto Soldado's back among the goals, and Gennaro Gattuso works on his night moves",    ```
    We need `VARCHAR` columns with 100-150 character lengths just in case any one or more records has an extraordinarily long value.

* IP addresses can either be in `IPV4` or `IPV6` format. `IPV6` address types required columns of `VARCHAR(38)` in the `ip_addr` field to accommodate the long addresses in this namespace.

* Query objects are random-access, even though the expectations is that the order of records

### IP Address and Platform Data

The Spotify listening history contains extensive and detailed data on the platform and IP address that streaming activity takes place on. While this provides interesting exploratory data analysis possibilities, we suggest you do _not_ do conduct or share any such public-facing research or analysis exposing this information. Networking data will allow malicious actors to map and exploit private home networks, profile how users navigate internet and VPN providers, obtain information on work and personal activities, and gather awareness of home and workplace devices. 

